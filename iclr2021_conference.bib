
@inproceedings{du2018gradient,
  title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{allen2019convergence,
  title={A Convergence Theory for Deep Learning via Over-Parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019}
}
@inproceedings{neyshabur2015norm,
  title={Norm-based capacity control in neural networks},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={1376--1401},
  year={2015}
}
@inproceedings{bartlett2017spectrally,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6240--6249},
  year={2017}
}
@article{neyshabur2017pac,
  title={A {P}{A}{C}-{B}ayesian approach to spectrally-normalized margin bounds for neural networks},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and Srebro, Nathan},
  journal={arXiv preprint arXiv:1707.09564},
  year={2017}
}
@inproceedings{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in neural information processing systems},
  pages={6155--6166},
  year={2019}
}
@article{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  journal={arXiv preprint arXiv:1703.11008},
  year={2017}
}
@inproceedings{arora2018stronger,
  title={Stronger generalization bounds for deep nets via a compression approach},
  author={Arora, Sanjeev and Ge, R and Neyshabur, B and Zhang, Y},
  booktitle={35th International Conference on Machine Learning, ICML 2018},
  year={2018}
}
@inproceedings{wei2019regularization,
  title={Regularization matters: Generalization and optimization of neural nets vs their induced kernel},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9709--9721},
  year={2019}
}


@inproceedings{neyshabur2019towards,
  title={Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks},
  author={Neyshabur, Behnam and Li, Zhiyuan},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}


@inproceedings{brutzkus2018sgd,
  title={SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data},
  author={Brutzkus, Alon and Globerson, Amir and Malach, Eran and Shalev-Shwartz, Shai},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8157--8166},
  year={2018}
}
@inproceedings{daniely2017sgd,
  title={{S}{G}{D} learns the conjugate kernel class of the network},
  author={Daniely, Amit},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2422--2430},
  year={2017}
}
@inproceedings{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in neural information processing systems},
  pages={6155--6166},
  year={2019}
}
@inproceedings{cao2019generalization,
  title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10835--10845},
  year={2019}
}
@inproceedings{frei2019algorithm,
  title={Algorithm-dependent generalization bounds for overparameterized deep residual networks},
  author={Frei, Spencer and Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14769--14779},
  year={2019}
}
@article{sagun2017empirical,
  title={Empirical analysis of the hessian of over-parametrized neural networks},
  author={Sagun, Levent and Evci, Utku and Guney, V Ugur and Dauphin, Yann and Bottou, Leon},
  journal={arXiv preprint arXiv:1706.04454},
  year={2017}
}
@inproceedings{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1019--1028},
  year={2017},
  organization={JMLR. org}
}
@article{poggio2017theory,
  title={Theory of deep learning III: explaining the non-overfitting puzzle},
  author={Poggio, Tomaso and Kawaguchi, Kenji and Liao, Qianli and Miranda, Brando and Rosasco, Lorenzo and Boix, Xavier and Hidary, Jack and Mhaskar, Hrushikesh},
  journal={arXiv preprint arXiv:1801.00173},
  year={2017}
}
@article{wu2017towards,
  title={Towards understanding generalization of deep learning: Perspective of loss landscapes},
  author={Wu, Lei and Zhu, Zhanxing and others},
  journal={arXiv preprint arXiv:1706.10239},
  year={2017}
}
@inproceedings{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in neural information processing systems},
  pages={6155--6166},
  year={2019}
}
@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}


@article{bartlett2002rademacher,
  title={Rademacher and Gaussian complexities: Risk bounds and structural results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Nov},
  pages={463--482},
  year={2002}
}

@inproceedings{arora2019fine,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019}
}

@inproceedings{yehudai2019power,
  title={On the power and limitations of random features for understanding neural networks},
  author={Yehudai, Gilad and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6594--6604},
  year={2019}
}
@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}

@article{DBLP:journals/corr/abs-1801-00173,
  author    = {Tomaso A. Poggio and
               Kenji Kawaguchi and
               Qianli Liao and
               Brando Miranda and
               Lorenzo Rosasco and
               Xavier Boix and
               Jack Hidary and
               Hrushikesh Mhaskar},
  title     = {Theory of Deep Learning {III:} explaining the non-overfitting puzzle},
  journal   = {CoRR},
  volume    = {abs/1801.00173},
  year      = {2018},
  opturl       = {http://arxiv.org/abs/1801.00173},
  archivePrefix = {arXiv},
  eprint    = {1801.00173},
  timestamp = {Mon, 13 Aug 2018 16:47:56 +0200},
  bibopturl    = {https://dblp.org/rec/journals/corr/abs-1801-00173.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/SagunBL16,
  author    = {Levent Sagun and
               L{\'{e}}on Bottou and
               Yann LeCun},
  title     = {Singularity of the {H}essian in Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1611.07476},
  year      = {2016},
  opturl       = {http://arxiv.org/abs/1611.07476},
  archivePrefix = {arXiv},
  eprint    = {1611.07476},
  timestamp = {Mon, 13 Aug 2018 16:46:41 +0200},
  bibopturl    = {https://dblp.org/rec/journals/corr/SagunBL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Simsekli17,
  title={Fractional Langevin Monte Carlo: exploring levy driven stochastic differential equations for Markov Chain Monte Carlo},
  author={{\c{S}}im{\v{S}}ekli, Umut},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3200--3209},
  year={2017}
}



@article{mandt_stochastic_2018,
  title={Stochastic gradient descent as approximate {B}ayesian inference},
  author={Mandt, Stephan and Hoffman, Matthew D and Blei, David M},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={4873--4907},
  year={2017},
  publisher={JMLR. org}
}


@inproceedings{sun2017relative,
  title={Relative Fisher information and natural gradient for learning large modular models},
  author={Sun, Ke and Nielsen, Frank},
  booktitle={International Conference on Machine Learning},
  pages={3289--3298},
  year={2017}
}

@inproceedings{pennington_spectrum_2018,
  title={The spectrum of the fisher information matrix of a single-hidden-layer neural network},
  author={Pennington, Jeffrey and Worah, Pratik},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5410--5419},
  year={2018}
}

@article{hoffman2014no,
  title={The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.},
  author={Hoffman, Matthew D and Gelman, Andrew},
  journal={J. Mach. Learn. Res.},
  volume={15},
  number={1},
  pages={1593--1623},
  year={2014}
}


@inproceedings{neyshabur_exploring_2017,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in neural information processing systems},
  pages={5947--5956},
  year={2017}
}

@article{amari_learning_2003,
    title = {Learning and inference in hierarchical models with singularities},
    volume = {34},
    copyright = {Copyright © 2003 Wiley Periodicals, Inc., A Wiley Company},
    optissn = {1520-684X},
    opturl = {https://onlinelibrary.wiley.com/optdoi/abs/10.1002/scj.10353},
    optdoi = {10.1002/scj.10353},
    abstract = {When we infer the underlying rule which generates a large amount of data, we assume a family of hierarchical statistical models and estimate an appropriate model and its parameters. In this case, the parameter space of the model usually includes singularities, and interesting phenomena, different from those appearing in conventional inference theory, are observed. In this paper, we review the studies of singular models in learning and inference which are being extensively developed in Japan, and elucidate the mechanisms of strange behavior by using simple models. © 2003 Wiley Periodicals, Inc. Syst Comp Jpn, 34(7): 34–42, 2003; Published online in Wiley InterScience (www.interscience.wiley.com). optdoi 10.1002/scj.10353},
    language = {en},
    number = {7},
    opturldate = {2020-09-20},
    journal = {Systems and Computers in Japan},
    author = {Amari, Shun-ichi and Ozeki, Tomoko and Park, Hyeyoung},
    year = {2003},
    pages = {34--42},
    file = {Snapshot:/Users/suswei/Zotero/storage/GH5TP8AX/scj.html:text/html}
}


@inproceedings{bousquet2003introduction,
  title={Introduction to statistical learning theory},
  author={Bousquet, Olivier and Boucheron, St{\'e}phane and Lugosi, G{\'a}bor},
  booktitle={Summer School on Machine Learning},
  pages={169--207},
  year={2003},
  organization={Springer}
}

@book{watanabe_algebraic_2009,
    address = {USA},
    title = {Algebraic {Geometry} and {Statistical} {Learning} {Theory}},
    optisbn = {978-0-521-86467-1},
    abstract = {Sure to be influential, Watanabe's book lays the foundations for the use of algebraic geometry in statistical learning theory. Many models/machines are singular: mixture models, neural networks, HMMs, Bayesian networks, stochastic context-free grammars are major examples. The theory achieved here underpins accurate estimation techniques in the presence of singularities.},
    publisher = {Cambridge University Press},
    author = {Watanabe, Sumio},
    year = {2009},
    keywords = {singular learning theory},
    file = {Watanabe_2009_Algebraic Geometry and Statistical Learning Theory.pdf:/Users/suswei/Zotero/storage/NU5LQX34/Watanabe_2009_Algebraic Geometry and Statistical Learning Theory.pdf:application/pdf}
}

@inproceedings{zhang_understanding_2017,
    title = {Understanding deep learning requires rethinking generalization},
    opturl = {http://arxiv.org/abs/1611.03530},
    abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
    opturldate = {2020-02-24},
    booktitle = {Proceedings of the 5th {International} {Conference} on {Learning} {Representations}},
    author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
    month = feb,
    year = {2017},
    note = {arXiv: 1611.03530},
    keywords = {generalization, deep learning}
}

@article{watanabe_stochastic_2006,
    title = {Stochastic {Complexities} of {Gaussian} {Mixtures} in {Variational} {Bayesian} {Approximation}},
    volume = {7},
    optissn = {1532-4435},
    abstract = {Bayesian learning has been widely used and proved to be effective in many data modeling problems. However, computations involved in it require huge costs and generally cannot be performed exactly. The variational Bayesian approach, proposed as an approximation of Bayesian learning, has provided computational tractability and good generalization performance in many applications. The properties and capabilities of variational Bayesian learning itself have not been clarified yet. It is still unknown how good approximation the variational Bayesian approach can achieve. In this paper, we discuss variational Bayesian learning of Gaussian mixture models and derive upper and lower bounds of variational stochastic complexities. The variational stochastic complexity, which corresponds to the minimum variational free energy and a lower bound of the Bayesian evidence, not only becomes important in addressing the model selection problem, but also enables us to discuss the accuracy of the variational Bayesian approach as an approximation of true Bayesian learning.},
    journal = {The Journal of Machine Learning Research},
    author = {Watanabe, Kazuho and Watanabe, Sumio},
    month = dec,
    year = {2006},
    keywords = {singular learning theory, variational free energy},
    pages = {625--644},
    file = {Watanabe_Watanabe_2006_Stochastic Complexities of Gaussian Mixtures in Variational Bayesian.pdf:/Users/suswei/Zotero/storage/3YFMUG2D/Watanabe_Watanabe_2006_Stochastic Complexities of Gaussian Mixtures in Variational Bayesian.pdf:application/pdf}
}

@article{thomas_information_2019,
    title = {Information matrices and generalization},
    opturl = {http://arxiv.org/abs/1906.07774},
    abstract = {This work revisits the use of information criteria to characterize the generalization of deep learning models. In particular, we empirically demonstrate the effectiveness of the Takeuchi information criterion (TIC), an extension of the Akaike information criterion (AIC) for misspecified models, in estimating the generalization gap, shedding light on why quantities such as the number of parameters cannot quantify generalization. The TIC depends on both the Hessian of the loss H and the covariance of the gradients C. By exploring the similarities and differences between these two matrices as well as the Fisher information matrix F, we study the interplay between noise and curvature in deep models. We also address the question of whether C is a reasonable approximation to F, as is commonly assumed.},
    opturldate = {2020-02-24},
    journal = {arXiv:1906.07774 [cs, stat]},
    author = {Thomas, Valentin and Pedregosa, Fabian and van Merriënboer, Bart and Mangazol, Pierre-Antoine and Bengio, Yoshua and Roux, Nicolas Le},
    month = jun,
    year = {2019},
    note = {arXiv: 1906.07774},
    keywords = {information criteria, generalization, deep learning},
    file = {Thomas et al_2019_Information matrices and generalization.pdf:/Users/suswei/Zotero/storage/YFKVFJZK/Thomas et al_2019_Information matrices and generalization.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/L6T8MWLR/1906.html:text/html}
}

@article{sun_lightlike_2020,
    title = {Lightlike {Neuromanifolds}, {Occam}'s {Razor} and {Deep} {Learning}},
    opturl = {http://arxiv.org/abs/1905.11027},
    abstract = {How do deep neural networks benefit from a very high dimensional parameter space? Their high complexity vs stunning generalization performance forms an intriguing paradox. We took an information-theoretic approach. We find that the locally varying dimensionality of the parameter space can be studied by the discipline of singular semi-Riemannian geometry. We adapt Fisher information to this singular neuromanifold. We use a new prior to interpolate between Jeffreys' prior and the Gaussian prior. We derive a minimum description length of a deep learning model, where the spectrum of the Fisher information matrix plays a key role to reduce the model complexity.},
    opturldate = {2020-02-24},
    journal = {arXiv:1905.11027 [cs, stat]},
    author = {Sun, Ke and Nielsen, Frank},
    month = feb,
    year = {2020},
    note = {arXiv: 1905.11027},
    keywords = {information criteria, marginal likelihood, deep learning},
    file = {Sun_Nielsen_2020_Lightlike Neuromanifolds, Occam's Razor and Deep Learning.pdf:/Users/suswei/Zotero/storage/QD5WHKM6/QD5WHKM6.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/FKMHHXU7/1905.html:text/html}
}

@article{lamont_information-based_2018,
    title = {Information-based inference for singular models and finite sample sizes: {A} frequentist information criterion},
    shorttitle = {Information-based inference for singular models and finite sample sizes},
    opturl = {http://arxiv.org/abs/1506.05855},
    abstract = {In the information-based paradigm of inference, model selection is performed by selecting the candidate model with the best estimated predictive performance. The success of this approach depends on the accuracy of the estimate of the predictive complexity. In the large-sample-size limit of a regular model, the predictive performance is well estimated by the Akaike Information Criterion (AIC). However, this approximation can either significantly under or over-estimating the complexity in a wide range of important applications where models are either non-regular or finite-sample-size corrections are significant. We introduce an improved approximation for the complexity that is used to define a new information criterion: the Frequentist Information Criterion (QIC). QIC extends the applicability of information-based inference to the finite-sample-size regime of regular models and to singular models. We demonstrate the power and the comparative advantage of QIC in a number of example analyses.},
    opturldate = {2020-02-24},
    journal = {arXiv:1506.05855 [physics, stat]},
    author = {LaMont, Colin H. and Wiggins, Paul A.},
    month = jun,
    year = {2018},
    note = {arXiv: 1506.05855},
    keywords = {information criteria, singular model},
    file = {LaMont_Wiggins_2018_Information-based inference for singular models and finite sample sizes.pdf:/Users/suswei/Zotero/storage/EZZQUNTJ/LaMont_Wiggins_2018_Information-based inference for singular models and finite sample sizes.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/IU595QPT/1506.html:text/html}
}

@article{imai_estimating_2019,
    title = {Estimating {Real} {Log} {Canonical} {Thresholds}},
    opturl = {http://arxiv.org/abs/1906.01341},
    abstract = {Evaluation of the marginal likelihood plays an important role in model selection problems. The widely applicable Bayesian information criterion (WBIC) and singular Bayesian information criterion (sBIC) give approximations to the log marginal likelihood, which can be applied to both regular and singular models. When the real log canonical thresholds are known, the performance of sBIC is considered to be better than that of WBIC, but only few real log canonical thresholds are known. In this paper, we propose a new estimator of the real log canonical thresholds based on the variance of thermodynamic integration with an inverse temperature. In addition, we propose an application to make sBIC widely applicable. Finally, we investigate the performance of the estimator and model selection by simulation studies and application to real data.},
    opturldate = {2020-02-24},
    journal = {arXiv:1906.01341 [math, stat]},
    author = {Imai, Toru},
    month = aug,
    year = {2019},
    note = {arXiv: 1906.01341},
    keywords = {RLCT, information criteria, marginal likelihood, singular model, thermodynamic integration},
    file = {Imai_2019_Estimating Real Log Canonical Thresholds.pdf:/Users/suswei/Zotero/storage/SGTAZW77/SGTAZW77.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/XS5A6IJP/1906.html:text/html}
}

@article{grzegorczyk_targeting_2017,
    title = {Targeting {Bayes} factors with direct-path non-equilibrium thermodynamic integration},
    volume = {32},
    optissn = {1613-9658},
    opturl = {https://optdoi.org/10.1007/s00180-017-0721-7},
    optdoi = {10.1007/s00180-017-0721-7},
    abstract = {Thermodynamic integration (TI) for computing marginal likelihoods is based on an inverse annealing path from the prior to the posterior distribution. In many cases, the resulting estimator suffers from high variability, which particularly stems from the prior regime. When comparing complex models with differences in a comparatively small number of parameters, intrinsic errors from sampling fluctuations may outweigh the differences in the log marginal likelihood estimates. In the present article, we propose a TI scheme that directly targets the log Bayes factor. The method is based on a modified annealing path between the posterior distributions of the two models compared, which systematically avoids the high variance prior regime. We combine this scheme with the concept of non-equilibrium TI to minimise discretisation errors from numerical integration. Results obtained on Bayesian regression models applied to standard benchmark data, and a complex hierarchical model applied to biopathway inference, demonstrate a significant reduction in estimator variance over state-of-the-art TI methods.},
    language = {en},
    number = {2},
    opturldate = {2020-02-25},
    journal = {Computational Statistics},
    author = {Grzegorczyk, Marco and Aderhold, Andrej and Husmeier, Dirk},
    month = jun,
    year = {2017},
    keywords = {marginal likelihood, Bayes factor, thermodynamic integration},
    pages = {717--761},
    file = {Grzegorczyk et al_2017_Targeting Bayes factors with direct-path non-equilibrium thermodynamic.pdf:/Users/suswei/Zotero/storage/P43WVTTR/Grzegorczyk et al_2017_Targeting Bayes factors with direct-path non-equilibrium thermodynamic.pdf:application/pdf}
}

@article{grunwald_minimum_2019,
    title = {Minimum {Description} {Length} {Revisited}},
    optissn = {2661-3352, 2661-3344},
    opturl = {http://arxiv.org/abs/1908.08484},
    optdoi = {10.1142/S2661335219300018},
    abstract = {This is an up-to-date introduction to and overview of the Minimum Description Length (MDL) Principle, a theory of inductive inference that can be applied to general problems in statistics, machine learning and pattern recognition. While MDL was originally based on data compression ideas, this introduction can be read without any knowledge thereof. It takes into account all major developments since 2007, the last time an extensive overview was written. These include new methods for model selection and averaging and hypothesis testing, as well as the first completely general definition of \{{\textbackslash}em MDL estimators\}. Incorporating these developments, MDL can be seen as a powerful extension of both penalized likelihood and Bayesian approaches, in which penalization functions and prior distributions are replaced by more general luckiness functions, average-case methodology is replaced by a more robust worst-case approach, and in which methods classically viewed as highly distinct, such as AIC vs BIC and cross-validation vs Bayes can, to a large extent, be viewed from a unified perspective.},
    opturldate = {2020-02-24},
    journal = {International Journal of Mathematics for Industry},
    author = {Grünwald, Peter and Roos, Teemu},
    month = nov,
    year = {2019},
    note = {arXiv: 1908.08484},
    keywords = {generalization, minimum description length},
    pages = {S2661335219300018},
    file = {Grünwald_Roos_2019_Minimum Description Length Revisited.pdf:/Users/suswei/Zotero/storage/RFR8ALMB/Grünwald_Roos_2019_Minimum Description Length Revisited.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/QGPIYGM4/1908.html:text/html}
}

@article{gelman_understanding_2013,
    title = {Understanding predictive information criteria for {Bayesian} models},
    volume = {24},
    optdoi = {10.1007/s11222-013-9416-2},
    abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected
out-of-sample-prediction error using a biascorrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
    journal = {Statistics and Computing},
    author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
    month = jul,
    year = {2013},
    keywords = {information criteria, WBIC, AIC, DIC, WAIC},
    file = {Gelman et al_2013_Understanding predictive information criteria for Bayesian models.pdf:/Users/suswei/Zotero/storage/ZCUM2QEN/Gelman et al_2013_Understanding predictive information criteria for Bayesian models.pdf:application/pdf;Full Text PDF:/Users/suswei/Zotero/storage/TGGUHEXK/Gelman et al. - 2013 - Understanding predictive information criteria for .pdf:application/pdf;Snapshot:/Users/suswei/Zotero/storage/PBHXJIG4/1307.html:text/html}
}

@article{drton_bayesian_2017,
    title = {A {Bayesian} information criterion for singular models},
    volume = {79},
    opturl = {http://arxiv.org/abs/1309.0911},
    abstract = {We consider approximate Bayesian model choice for model selection problems that involve models whose Fisher-information matrices may fail to be invertible along other competing submodels. Such singular models do not obey the regularity conditions underlying the derivation of Schwarz's Bayesian information criterion (BIC) and the penalty structure in BIC generally does not reflect the frequentist large-sample behavior of their marginal likelihood. While large-sample theory for the marginal likelihood of singular models has been developed recently, the resulting approximations depend on the true parameter value and lead to a paradox of circular reasoning. Guided by examples such as determining the number of components of mixture models, the number of factors in latent factor models or the rank in reduced-rank regression, we propose a resolution to this paradox and give a practical extension of BIC for singular model selection problems.},
    number = {2},
    opturldate = {2020-02-24},
    journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
    author = {Drton, Mathias and Plummer, Martyn},
    year = {2017},
    note = {arXiv: 1309.0911},
    keywords = {RLCT, information criteria, marginal likelihood, singular model},
    pages = {323--380},
    file = {Drton_Plummer_2017_A Bayesian information criterion for singular models.pdf:/Users/suswei/Zotero/storage/WAQMMFKW/Drton_Plummer_2017_A Bayesian information criterion for singular models.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/W4PWR7WF/1309.html:text/html}
}

@inproceedings{dinh_sharp_2017,
    address = {Sydney, NSW, Australia},
    series = {{ICML}'17},
    title = {Sharp {Minima} {Can} {Generalize} {For} {Deep} {Nets}},
    volume = {70},
    opturl = {http://arxiv.org/abs/1703.04933},
    abstract = {Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter \& Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.},
    opturldate = {2020-02-24},
    booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
    publisher = {JMLR.org},
    author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
    month = may,
    year = {2017},
    note = {arXiv: 1703.04933},
    keywords = {generalization, deep learning, minima},
    pages = {1019--1028},
    file = {Dinh et al_2017_Sharp Minima Can Generalize For Deep Nets.pdf:/Users/suswei/Zotero/storage/KF35JUNP/Dinh et al_2017_Sharp Minima Can Generalize For Deep Nets.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/4HLSAWER/1703.html:text/html}
}

@article{mattingly_maximizing_2018,
    title = {Maximizing the information learned from finite data selects a simple model},
    volume = {115},
    copyright = {© 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
    optissn = {0027-8424, 1091-6490},
    opturl = {https://www.pnas.org/content/115/8/1760},
    optdoi = {10.1073/pnas.1715306115},
    abstract = {We use the language of uninformative Bayesian prior choice to study the selection of appropriately simple effective models. We advocate for the prior which maximizes the mutual information between parameters and predictions, learning as much as possible from limited data. When many parameters are poorly constrained by the available data, we find that this prior puts weight only on boundaries of the parameter space. Thus, it selects a lower-dimensional effective theory in a principled way, ignoring irrelevant parameter directions. In the limit where there are sufficient data to tightly constrain any number of parameters, this reduces to the Jeffreys prior. However, we argue that this limit is pathological when applied to the hyperribbon parameter manifolds generic in science, because it leads to dramatic dependence on effects invisible to experiment.},
    language = {en},
    number = {8},
    opturldate = {2020-02-24},
    journal = {Proceedings of the National Academy of Sciences},
    author = {Mattingly, Henry H. and Transtrum, Mark K. and Abbott, Michael C. and Machta, Benjamin B.},
    month = feb,
    year = {2018},
    pmid = {29434042},
    pages = {1760--1765},
    file = {Snapshot:/Users/suswei/Zotero/storage/KBZI3SET/1760.html:text/html;Mattingly et al_2018_Maximizing the information learned from finite data selects a simple model.pdf:/Users/suswei/Zotero/storage/9X9XAFYM/Mattingly et al_2018_Maximizing the information learned from finite data selects a simple model.pdf:application/pdf}
}

@article{lin_ideal-theoretic_2017,
    title = {Ideal-{Theoretic} {Strategies} for {Asymptotic} {Approximation} of {Marginal} {Likelihood} {Integrals}},
    volume = {8},
    optissn = {1309-3452},
    opturl = {http://arxiv.org/abs/1003.5338},
    optdoi = {10.18409/jas.v8i1.47},
    abstract = {The accurate asymptotic evaluation of marginal likelihood integrals is a fundamental problem in Bayesian statistics. Following the approach introduced by Watanabe, we translate this into a problem of computational algebraic geometry, namely, to determine the real log canonical threshold of a polynomial ideal, and we present effective methods for solving this problem. Our results are based on resolution of singularities. They apply to parametric models where the Kullback-Leibler distance is upper and lower bounded by scalar multiples of some sum of squared real analytic functions. Such models include finite state discrete models.},
    number = {1},
    opturldate = {2020-02-24},
    journal = {Journal of Algebraic Statistics},
    author = {Lin, Shaowei},
    month = feb,
    year = {2017},
    note = {arXiv: 1003.5338},
    keywords = {RLCT, information criteria, marginal likelihood, singular model},
    file = {Lin_2017_Ideal-Theoretic Strategies for Asymptotic Approximation of Marginal Likelihood.pdf:/Users/suswei/Zotero/storage/I4R6B56W/Lin_2017_Ideal-Theoretic Strategies for Asymptotic Approximation of Marginal Likelihood.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/PNWS7P5L/1003.html:text/html}
}

@article{friel_investigation_2017,
    title = {Investigation of the widely applicable {Bayesian} information criterion},
    volume = {27},
    opturl = {http://arxiv.org/abs/1501.05447},
    abstract = {The widely applicable Bayesian information criterion (WBIC) is a simple and fast approximation to the model evidence that has received little practical consideration. WBIC uses the fact that the log evidence can be written as an expectation, with respect to a powered posterior proportional to the likelihood raised to a power \$t{\textasciicircum}*{\textbackslash}in\{(0,1)\}\$, of the log deviance. Finding this temperature value \$t{\textasciicircum}*\$ is generally an intractable problem. We find that for a particular tractable statistical model that the mean squared error of an optimally-tuned version of WBIC with correct temperature \$t{\textasciicircum}*\$ is lower than an optimally-tuned version of thermodynamic integration (power posteriors). However in practice WBIC uses the a canonical choice of \$t=1/{\textbackslash}log(n)\$. Here we investigate the performance of WBIC in practice, for a range of statistical models, both regular models and singular models such as latent variable models or those with a hierarchical structure for which BIC cannot provide an adequate solution. Our findings are that, generally WBIC performs adequately when one uses informative priors, but it can systematically overestimate the evidence, particularly for small sample sizes.},
    opturldate = {2020-02-24},
    journal = {Statistics and Computing},
    author = {Friel, N. and McKeone, J. P. and Oates, C. J. and Pettitt, A. N.},
    year = {2017},
    note = {arXiv: 1501.05447},
    keywords = {information criteria, marginal likelihood, singular model, WBIC},
    pages = {833--844},
    file = {Friel et al_2017_Investigation of the widely applicable Bayesian information criterion.pdf:/Users/suswei/Zotero/storage/9K7FK9CR/Friel et al_2017_Investigation of the widely applicable Bayesian information criterion.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/T6X3SH3S/1501.html:text/html}
}

@article{daunizeau_variational_2018,
    title = {The variational {Laplace} approach to approximate {Bayesian} inference},
    opturl = {http://arxiv.org/abs/1703.02089},
    abstract = {Variational approaches to approximate Bayesian inference provide very efficient means of performing parameter estimation and model selection. Among these, so-called variational-Laplace or VL schemes rely on Gaussian approximations to posterior densities on model parameters. In this note, we review the main variants of VL approaches, that follow from considering nonlinear models of continuous and/or categorical data. En passant, we also derive a few novel theoretical results that complete the portfolio of existing analyses of variational Bayesian approaches, including investigations of their asymptotic convergence. We also suggest practical ways of extending existing VL approaches to hierarchical generative models that include (e.g., precision) hyperparameters.},
    opturldate = {2020-03-04},
    journal = {arXiv:1703.02089 [q-bio, stat]},
    author = {Daunizeau, Jean},
    month = jan,
    year = {2018},
    note = {arXiv: 1703.02089},
    keywords = {variational inference},
    file = {Full Text:/Users/suswei/Zotero/storage/R4QAUDSJ/Daunizeau - 2018 - The variational Laplace approach to approximate Ba.pdf:application/pdf}
}

@article{zhang_energyentropy_2018,
    title = {Energy-entropy competition and the effectiveness of stochastic gradient descent in machine learning},
    volume = {116},
    optissn = {0026-8976},
    opturl = {https://optdoi.org/10.1080/00268976.2018.1483535},
    optdoi = {10.1080/00268976.2018.1483535},
    abstract = {Finding parameters that minimise a loss function is at the core of many machine learning methods. The Stochastic Gradient Descent (SGD) algorithm is widely used and delivers state-of-the-art results for many problems. Nonetheless, SGD typically cannot find the global minimum, thus its empirical effectiveness is hitherto mysterious. We derive a correspondence between parameter inference and free energy minimisation in statistical physics. The degree of undersampling plays the role of temperature. Analogous to the energy–entropy competition in statistical physics, wide but shallow minima can be optimal if the system is undersampled, as is typical in many applications. Moreover, we show that the stochasticity in the algorithm has a non-trivial correlation structure which systematically biases it towards wide minima. We illustrate our argument with two prototypical models: image classification using deep learning and a linear neural network where we can analytically reveal the relationship between entropy and out-of-sample error.},
    number = {21-22},
    opturldate = {2020-03-04},
    journal = {Molecular Physics},
    author = {Zhang, Yao and Saxe, Andrew M. and Advani, Madhu S. and Lee, Alpha A.},
    month = nov,
    year = {2018},
    pages = {3214--3223}
}



@article{hayashi_variational_2020,
    title = {Variational {Approximation} {Error} in {Bayesian} {Non}-negative {Matrix} {Factorization}},
    opturl = {http://arxiv.org/abs/1809.02963},
    abstract = {Non-negative matrix factorization (NMF) is a knowledge discovery method that is used in many fields. Variational inference and Gibbs sampling methods for it are also wellknown. However, the variational approximation error has not been clarified yet, because NMF is not statistically regular and the prior distribution used in variational Bayesian NMF (VBNMF) has zero or divergence points. In this paper, using algebraic geometrical methods, we theoretically analyze the difference in negative log evidence (a.k.a. free energy) between VBNMF and Bayesian NMF, i.e., the Kullback-Leibler divergence between the variational posterior and the true posterior. We derive an upper bound for the learning coefficient (a.k.a. the real log canonical threshold) in Bayesian NMF. By using the upper bound, we find a lower bound for the approximation error, asymptotically. The result quantitatively shows how well the VBNMF algorithm can approximate Bayesian NMF; the lower bound depends on the hyperparameters and the true nonnegative rank. A numerical experiment demonstrates the theoretical result.},
    opturldate = {2020-03-02},
    journal = {arXiv:1809.02963 [cs, math, stat]},
    author = {Hayashi, Naoki},
    month = feb,
    year = {2020},
    note = {arXiv: 1809.02963},
    keywords = {RLCT, singular learning theory, variational free energy},
    file = {arXiv Fulltext PDF:/Users/suswei/Zotero/storage/II37PANI/II37PANI.pdf:application/pdf}
}

@inproceedings{yamada_information_2012,
    title = {Information criterion for variational {Bayes} learning in regular and singular cases},
    optdoi = {10.1109/SCIS-ISIS.2012.6505025},
    abstract = {Variational Bayes learning gives the accurate statistical estimation as Bayes learning with smaller computational cost. However, it has been difficult to estimate its generalization loss, because learning machines used in variational Bayes are not regular but singular, resulting that the conventional information criteria such as AIC, BIC, or DIC can not be applied. In this paper, we propose a new information criterion for variational Bayes learning, which is the unbiased estimator of the generalization loss for both cases when the posterior distribution is regular and singular. We show the theoretical support of the proposed information criterion, and its effectiveness is illustrated by numerical experiments.},
    booktitle = {The 6th {International} {Conference} on {Soft} {Computing} and {Intelligent} {Systems}, and {The} 13th {International} {Symposium} on {Advanced} {Intelligence} {Systems}},
    author = {Yamada, Koshi and Watanabe, Sumio},
    month = nov,
    year = {2012},
    note = {optissn: null},
    keywords = {RLCT, singular learning theory, variational free energy},
    pages = {1551--1555},
    file = {IEEE Xplore Abstract Record:/Users/suswei/Zotero/storage/F2CFAGRD/6505025.html:text/html;IEEE Xplore Full Text PDF:/Users/suswei/Zotero/storage/3K5G2KDB/Yamada and Watanabe - 2012 - Information criterion for variational Bayes learni.pdf:application/pdf}
}

@article{nakajima_variational_2007,
    title = {Variational {Bayes} {Solution} of {Linear} {Neural} {Networks} and {Its} {Generalization} {Performance}},
    volume = {19},
    opturl = {https://www.researchgate.net/publication/6457767_Variational_Bayes_Solution_of_Linear_Neural_Networks_and_Its_Generalization_Performance},
    number = {4},
    opturldate = {2020-03-03},
    journal = {Neural Computation},
    author = {Nakajima, Shinichi and Watanabe, Sumio},
    year = {2007},
    keywords = {singular learning theory, variational free energy, variational generalisation, linear neural networks},
    pages = {1112--53},
    file = {(PDF) Variational Bayes Solution of Linear Neural Networks and Its Generalization Performance:/Users/suswei/Zotero/storage/BG5Q6IBU/6457767_Variational_Bayes_Solution_of_Linear_Neural_Networks_and_Its_Generalization_Performance.html:text/html;Full Text:/Users/suswei/Zotero/storage/3N2M3L89/Nakajima and Watanabe - 2007 - Variational Bayes Solution of Linear Neural Networ.pdf:application/pdf}
}

@inproceedings{kaji_optimal_2009,
    address = {Berlin, Heidelberg},
    series = {Lecture {Notes} in {Computer} {Science}},
    title = {Optimal {Hyperparameters} for {Generalized} {Learning} and {Knowledge} {Discovery} in {Variational} {Bayes}},
    optisbn = {978-3-642-10677-4},
    optdoi = {10.1007/978-3-642-10677-4_54},
    abstract = {Variational Bayes learning is widely used in statistical models that contain hidden variables, for example, normal mixtures, binomial mixtures, and hidden Markov models. To derive the variational Bayes learning algorithm, we need to determine the hyperparameters in the a priori distribution. In the present paper, we propose two different methods by which to optimize the hyperparameters for the two different purposes. In the first method, the hyperparameter is determined for minimization of the generalization error. In the second method, the hyperparameter is chosen so that the unknown hidden structure in the data can be discovered. Experiments are conducted to show that the optimal hyperparameters are different for the generalized learning and knowledge discovery.},
    language = {en},
    booktitle = {Neural {Information} {Processing}},
    publisher = {Springer},
    author = {Kaji, Daisuke and Watanabe, Sumio},
    editor = {Leung, Chi Sing and Lee, Minho and Chan, Jonathan H.},
    year = {2009},
    keywords = {variational free energy, variational generalisation},
    pages = {476--483},
    file = {Springer Full Text PDF:/Users/suswei/Zotero/storage/ZG7YRBF9/Kaji and Watanabe - 2009 - Optimal Hyperparameters for Generalized Learning a.pdf:application/pdf}
}

@article{watanabe_alternative_2012,
    title = {An alternative view of variational {Bayes} and asymptotic approximations of free energy},
    volume = {86},
    optissn = {1573-0565},
    opturl = {https://optdoi.org/10.1007/s10994-011-5264-5},
    optdoi = {10.1007/s10994-011-5264-5},
    abstract = {Bayesian learning, widely used in many applied data-modeling problems, is often accomplished with approximation schemes because it requires intractable computation of the posterior distributions. In this study, we focus on two approximation methods, variational Bayes and local variational approximation. We show that the variational Bayes approach for statistical models with latent variables can be viewed as a special case of local variational approximation, where the log-sum-exp function is used to form the lower bound of the log-likelihood. The minimum variational free energy, the objective function of variational Bayes, is analyzed and related to the asymptotic theory of Bayesian learning. This analysis additionally implies a relationship between the generalization performance of the variational Bayes approach and the minimum variational free energy.},
    language = {en},
    number = {2},
    opturldate = {2020-03-03},
    journal = {Machine Learning},
    author = {Watanabe, Kazuho},
    month = feb,
    year = {2012},
    keywords = {variational free energy, variational generalisation},
    pages = {273--293},
    file = {Full Text:/Users/suswei/Zotero/storage/CTEATMY6/Watanabe - 2012 - An alternative view of variational Bayes and asymp.pdf:application/pdf}
}

@inproceedings{gao_degrees_2016,
    title = {Degrees of {Freedom} in {Deep} {Neural} {Networks}},
    opturl = {http://arxiv.org/abs/1603.09260},
    abstract = {In this paper, we explore degrees of freedom in deep sigmoidal neural networks. We show that the degrees of freedom in these models is related to the expected optimism, which is the expected difference between test error and training error. We provide an efficient Monte-Carlo method to estimate the degrees of freedom for multi-class classification methods. We show degrees of freedom are lower than the parameter count in a simple XOR network. We extend these results to neural nets trained on synthetic and real data, and investigate impact of network's architecture and different regularization choices. The degrees of freedom in deep networks are dramatically smaller than the number of parameters, in some real datasets several orders of magnitude. Further, we observe that for fixed number of parameters, deeper networks have less degrees of freedom exhibiting a regularization-by-depth.},
    opturldate = {2020-03-03},
    booktitle = {Proceedings of the {Thirty}-{Second} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
    author = {Gao, Tianxiang and Jojic, Vladimir},
    month = jun,
    year = {2016},
    note = {arXiv: 1603.09260},
    keywords = {BIC, deep learning, degrees of freedom},
    pages = {232--241},
    file = {arXiv Fulltext PDF:/Users/suswei/Zotero/storage/YIT5MND3/Gao and Jojic - 2016 - Degrees of Freedom in Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/MPNEF2Y8/1603.html:text/html}
}


@article{maddox_rethinking_2020,
  title={Rethinking Parameter Counting in Deep Models: Effective Dimensionality Revisited},
  author={Maddox, Wesley J and Benton, Gregory and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2003.02139},
  year={2020}
}

@article{aoyagi_zeta_2016,
    title = {Zeta function of learning theory and generalization error of three layered neural perceptron},
    abstract = {Recently, the purpose of obtaining the maximum poles of certain zeta functions arises in the learning theory when one is looking for the generalization errors of hierarchical learning models asymptotically . \$[3,4]\$ The zeta function of a learning model is deﬁned by the integral of its Kullback function and its a priom probability density function. Today, for several learning models, upper bounds of the main terms in their asymptotic forms were calculated, but not the exact values, so far. In this paper, we obtain the explicit value of the main term for a three layered neural network, which is one of hierarchical learning models.},
    language = {en},
    author = {Aoyagi, Miki},
    year = {2016},
    keywords = {RLCT, RLCT concrete},
    pages = {16},
    file = {Aoyagi - Zeta function of learning theory and generalizatio.pdf:/Users/suswei/Zotero/storage/6JGIDYX8/Aoyagi - Zeta function of learning theory and generalizatio.pdf:application/pdf}
}

@inproceedings{aoyagi_resolution_2006,
    title = {Resolution of {Singularities} and the {Generalization} {Error} with {Bayesian} {Estimation} for {Layered} {Neural} {Network}},
    abstract = {Hierarchical learning models such as layered neural networks have singular Fisher metrics, since their parameters are not identiﬁable. These are called non-regular learning models. The stochastic complexities of non-regular learning models in Bayesian estimation are asymptotically obtained by using poles of their zeta functions which are the integrals of their Kullback distances and their priori probability density functions [1, 2, 3]. However, for several examples, upper bounds of the main terms in asymptotic forms of the stochastic complexities were obtained but not the exact values, because of their computational complexities. In this paper, we show a computational way for obtaining the exact value of the layered neural network and we give the asymptotic form of its stochastic complexity explicitly.},
    author = {Aoyagi, Miki and Watanabe, Sumio},
    year = {2005},
    booktitle = {IEICE Trans.},
    pages = {2112?2124},

}

@article{aoyagi_stochastic_2005,
    title = {Stochastic complexities of reduced rank regression in {Bayesian} estimation},
    volume = {18},
    optissn = {08936080},
    opturl = {https://linkinghub.elsevier.com/retrieve/pii/S0893608005000559},
    optdoi = {10.1016/j.neunet.2005.03.014},
    abstract = {Reduced rank regression extracts an essential information from examples of input–output pairs. It is understood as a three-layer neural network with linear hidden units. However, reduced rank approximation is a non-regular statistical model which has a degenerate Fisher information matrix. Its generalization error had been left unknown even in statistics. In this paper, we give the exact asymptotic form of its generalization error in Bayesian estimation, based on resolution of learning machine singularities. For this purpose, the maximum pole of the zeta function for the learning theory is calculated. We propose a new method of recursive blowing-ups which yields the complete desingularization of the reduced rank approximation.},
    language = {en},
    number = {7},
    opturldate = {2020-03-27},
    journal = {Neural Networks},
    author = {Aoyagi, Miki and Watanabe, Sumio},
    month = sep,
    year = {2005},
    keywords = {RLCT, RLCT concrete},
    pages = {924--933},
    file = {Aoyagi and Watanabe - 2005 - Stochastic complexities of reduced rank regression.pdf:/Users/suswei/Zotero/storage/JPE5TTX9/Aoyagi and Watanabe - 2005 - Stochastic complexities of reduced rank regression.pdf:application/pdf}
}

@article{mescheder_adversarial_2018,
    title = {Adversarial {Variational} {Bayes}: {Unifying} {Variational} {Autoencoders} and {Generative} {Adversarial} {Networks}},
    shorttitle = {Adversarial {Variational} {Bayes}},
    opturl = {http://arxiv.org/abs/1701.04722},
    abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
    opturldate = {2020-03-30},
    journal = {arXiv:1701.04722 [cs]},
    author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
    month = jun,
    year = {2018},
    note = {arXiv: 1701.04722},
    keywords = {implicit VI},
    file = {arXiv.org Snapshot:/Users/suswei/Zotero/storage/HAW562DV/1701.html:text/html;Mescheder et al_2018_Adversarial Variational Bayes.pdf:/Users/suswei/Zotero/storage/4KV5LRWG/Mescheder et al_2018_Adversarial Variational Bayes.pdf:application/pdf}
}

@article{fukumizu_likelihood_2003,
    title = {Likelihood {Ratio} of {Unidentifiable} {Models} and {Multilayer} {Neural} {Networks}},
    volume = {31},
    optissn = {0090-5364},
    opturl = {https://www.jstor.org/stable/3448422},
    abstract = {This paper discusses the behavior of the maximum likelihood estimator (MLE), in the case that the true parameter cannot be identified uniquely. Among many statistical models with unidentifiability, neural network models are the main concern of this paper. It is known in some models with unidentifiability that the asymptotics of the likelihood ratio of the MLE has an unusually larger order. Using the framework of locally conic models put forth by Dacunha-Castelle and Gassiat as a generalization of Hartigan's idea, a useful sufficient condition of such larger orders is derived. This result is applied to neural network models, and a larger order is proved if the true function is given by a smaller model. Also, under the condition that the model has at least two redundant hidden units, a log n lower bound for the likelihood ratio is derived.},
    number = {3},
    opturldate = {2020-05-02},
    journal = {The Annals of Statistics},
    author = {Fukumizu, Kenji},
    year = {2003},
    note = {Publisher: Institute of Mathematical Statistics},
    pages = {833--851},
    file = {JSTOR Full Text PDF:/Users/suswei/Zotero/storage/V6TG9L7A/Fukumizu - 2003 - Likelihood Ratio of Unidentifiable Models and Mult.pdf:application/pdf}
}

@article{nakajima_generalization_2006,
    title = {Generalization {Performance} of {Subspace} {Bayes} {Approach} in {Linear} {Neural} {Networks}},
    volume = {E89-D},
    optissn = {0916-8532, 1745-1361},
    opturl = {http://search.ieice.org/bin/summary.php?id=e89-d_3_1128&category=D&year=2006&lang=E&abst=},
    optdoi = {10.1093/ietisy/e89-d.3.1128},
    abstract = {In unidentiﬁable models, the Bayes estimation has the advantage of generalization performance over the maximum likelihood estimation. However, accurate approximation of the posterior distribution requires huge computational costs. In this paper, we consider an alternative approximation method, which we call a subspace Bayes approach. A subspace Bayes approach is an empirical Bayes approach where a part of the parameters are regarded as hyperparameters. Consequently, in some threelayer models, this approach requires much less computational costs than Markov chain Monte Carlo methods. We show that, in three-layer linear neural networks, a subspace Bayes approach is asymptotically equivalent to a positive-part James-Stein type shrinkage estimation, and theoretically clarify its generalization error and training error. We also discuss the domination over the maximum likelihood estimation and the relation to the variational Bayes approach.},
    language = {en},
    number = {3},
    opturldate = {2020-05-02},
    journal = {IEICE Transactions on Information and Systems},
    author = {Nakajima, S.},
    month = mar,
    year = {2006},
    keywords = {delicate situation},
    pages = {1128--1138},
    file = {Nakajima - 2006 - Generalization Performance of Subspace Bayes Appro.pdf:/Users/suswei/Zotero/storage/RMZLY375/RMZLY375.pdf:application/pdf}
}

@article{nagata_exchange_2008,
    title = {Exchange {Monte} {Carlo} {Sampling} {From} {Bayesian} {Posterior} for {Singular} {Learning} {Machines}},
    volume = {19},
    optissn = {1045-9227, 1941-0093},
    opturl = {http://ieeexplore.ieee.org/document/4483537/},
    optdoi = {10.1109/TNN.2008.2000202},
    abstract = {Many singular learning machines such as neural networks, normal mixtures, Bayesian networks, and hidden Markov models belong to singular learning machines and are widely used in practical information systems. In these learning machines, it is well known that Bayesian learning provides better generalization performance than maximum-likelihood estimation. However, it needs huge computational cost to sample from a Bayesian posterior distribution of a singular learning machine by a conventional Markov chain Monte Carlo (MCMC) method, such as the metropolis algorithm, because of singularities. Recently, the exchange Monte Carlo (MC) method, which is well known as an improved algorithm of MCMC method, has been proposed to apply to Bayesian neural network learning in the literature. In this paper, we propose the idea that the exchange MC method has a better effect on Bayesian learning in singular learning machines than that in regular learning machines, and show its effectiveness by comparing the numerical stochastic complexity with the theoretical one.},
    language = {en},
    number = {7},
    opturldate = {2020-05-08},
    journal = {IEEE Transactions on Neural Networks},
    author = {Nagata, K. and Watanabe, S.},
    month = jul,
    year = {2008},
    keywords = {mcmc},
    pages = {1253--1266},
    file = {Nagata and Watanabe - 2008 - Exchange Monte Carlo Sampling From Bayesian Poster.pdf:/Users/suswei/Zotero/storage/DSNJB3AV/DSNJB3AV.pdf:application/pdf}
}

@article{wenzel_how_2020,
    title = {How {Good} is the {Bayes} {Posterior} in {Deep} {Neural} {Networks} {Really}?},
    opturl = {http://arxiv.org/abs/2002.02405},
    abstract = {During the past ﬁve years the Bayesian deep learning community has developed increasingly accurate and efﬁcient approximate inference procedures that allow for Bayesian inference in deep neural networks. However, despite this algorithmic progress and the promise of improved uncertainty quantiﬁcation and sample efﬁciency there are—as of early 2020—no publicized deployments of Bayesian neural networks in industrial practice. In this work we cast doubt on the current understanding of Bayes posteriors in popular deep neural networks: we demonstrate through careful MCMC sampling that the posterior predictive induced by the Bayes posterior yields systematically worse predictions compared to simpler methods including point estimates obtained from SGD. Furthermore, we demonstrate that predictive performance is improved signiﬁcantly through the use of a “cold posterior” that overcounts evidence. Such cold posteriors sharply deviate from the Bayesian paradigm but are commonly used as heuristic in Bayesian deep learning papers. We put forward several hypotheses that could explain cold posteriors and evaluate the hypotheses through experiments. Our work questions the goal of accurate posterior approximations in Bayesian deep learning: If the true Bayes posterior is poor, what is the use of more accurate approximations? Instead, we argue that it is timely to focus on understanding the origin of the improved performance of cold posteriors.},
    language = {en},
    opturldate = {2020-05-08},
    journal = {arXiv:2002.02405 [cs, stat]},
    author = {Wenzel, Florian and Roth, Kevin and Veeling, Bastiaan S. and Świątkowski, Jakub and Tran, Linh and Mandt, Stephan and Snoek, Jasper and Salimans, Tim and Jenatton, Rodolphe and Nowozin, Sebastian},
    month = feb,
    year = {2020},
    note = {arXiv: 2002.02405},
    keywords = {sgmcmc},
    file = {Wenzel et al. - 2020 - How Good is the Bayes Posterior in Deep Neural Net.pdf:/Users/suswei/Zotero/storage/I7TU2V4P/Wenzel et al. - 2020 - How Good is the Bayes Posterior in Deep Neural Net.pdf:application/pdf}
}

@article{wilson_bayesian_2020,
    title = {Bayesian {Deep} {Learning} and a {Probabilistic} {Perspective} of {Generalization}},
    opturl = {http://arxiv.org/abs/2002.08791},
    abstract = {The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased flexibility. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions.},
    opturldate = {2020-05-10},
    journal = {arXiv:2002.08791 [cs, stat]},
    author = {Wilson, Andrew Gordon and Izmailov, Pavel},
    month = apr,
    year = {2020},
    note = {arXiv: 2002.08791},
    file = {arXiv Fulltext PDF:/Users/suswei/Zotero/storage/3J7S9JWD/Wilson and Izmailov - 2020 - Bayesian Deep Learning and a Probabilistic Perspec.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/DDVESE82/2002.html:text/html;Full Text:/Users/suswei/Zotero/storage/HRMG5MWU/Wilson and Izmailov - 2020 - Bayesian Deep Learning and a Probabilistic Perspec.pdf:application/pdf}
}

@article{aoyagi_learning_2012,
    title = {Learning {Coefficient} of {Generalization} {Error} in {Bayesian} {Estimation} and {Vandermonde} {Matrix}-{Type} {Singularity}},
    volume = {24},
    optissn = {0899-7667, 1530-888X},
    opturl = {http://www.mitpressjournals.org/optdoi/10.1162/NECO_a_00271},
    optdoi = {10.1162/NECO_a_00271},
    language = {en},
    number = {6},
    opturldate = {2020-05-12},
    journal = {Neural Computation},
    author = {Aoyagi, Miki and Nagata, Kenji},
    month = jun,
    year = {2012},
    pages = {1569--1610},
    file = {Aoyagi and Nagata - 2012 - Learning Coefficient of Generalization Error in Ba.pdf:/Users/suswei/Zotero/storage/IQG8YSSA/Aoyagi and Nagata - 2012 - Learning Coefficient of Generalization Error in Ba.pdf:application/pdf}
}

@article{yu_implicit_2019,
    title = {Implicit {Posterior} {Variational} {Inference} for {Deep} {Gaussian} {Processes}},
    opturl = {http://arxiv.org/abs/1910.11998},
    abstract = {A multi-layer deep Gaussian process (DGP) model is a hierarchical composition of GP models with a greater expressive power. Exact DGP inference is intractable, which has motivated the recent development of deterministic and stochastic approximation methods. Unfortunately, the deterministic approximation methods yield a biased posterior belief while the stochastic one is computationally costly. This paper presents an implicit posterior variational inference (IPVI) framework for DGPs that can ideally recover an unbiased posterior belief and still preserve time efficiency. Inspired by generative adversarial networks, our IPVI framework achieves this by casting the DGP inference problem as a two-player game in which a Nash equilibrium, interestingly, coincides with an unbiased posterior belief. This consequently inspires us to devise a best-response dynamics algorithm to search for a Nash equilibrium (i.e., an unbiased posterior belief). Empirical evaluation shows that IPVI outperforms the state-of-the-art approximation methods for DGPs.},
    opturldate = {2020-05-14},
    journal = {arXiv:1910.11998 [cs, stat]},
    author = {Yu, Haibin and Chen, Yizhou and Dai, Zhongxiang and Low, Kian Hsiang and Jaillet, Patrick},
    month = oct,
    year = {2019},
    note = {arXiv: 1910.11998}
}

@article{watanabe_algebraic_2001,
    title = {Algebraic geometrical methods for hierarchical learning machines},
    volume = {14},
    optissn = {08936080},
    opturl = {https://linkinghub.elsevier.com/retrieve/pii/S0893608001000697},
    optdoi = {10.1016/S0893-6080(01)00069-7},
    abstract = {Hierarchical learning machines such as layered perceptrons, radial basis functions, gaussian mixtures are non-identiﬁable learning machines, whose Fisher information matrices are not positive deﬁnite. This fact shows that conventional statistical asymptotic theory can not be applied to the neural network learning theory, for example, either the Bayesian a posteriori probability distribution does not converge to the gaussian distribution, or the generalization error is not in proportion to the number of parameters. The purpose of this paper is to overcome this problem and to clarify the relation between the learning curve of a hierarchical learning machine and the algebraic geometrical structure of the parameter space. We establish an algorithm to calculate the Bayesian stochastic complexity based on blowing-up technology in algebraic geometry and prove that the Bayesian generalization error of a hierarchical learning machine is smaller than that of a regular statistical model, even if the true distribution is not contained in the parametric model.},
    language = {en},
    number = {8},
    opturldate = {2020-05-14},
    journal = {Neural Networks},
    author = {Watanabe, Sumio},
    month = oct,
    year = {2001},
    pages = {1049--1060},
    file = {Watanabe - 2001 - Algebraic geometrical methods for hierarchical lea.pdf:/Users/suswei/Zotero/storage/WAF3HX6S/Watanabe - 2001 - Algebraic geometrical methods for hierarchical lea.pdf:application/pdf}
}

@article{naesseth_reparameterization_2020,
    title = {Reparameterization {Gradients} through {Acceptance}-{Rejection} {Sampling} {Algorithms}},
    opturl = {http://arxiv.org/abs/1610.05683},
    abstract = {Variational inference using the reparameterization trick has enabled large-scale approximate Bayesian inference in complex probabilistic models, leveraging stochastic optimization to sidestep intractable expectations. The reparameterization trick is applicable when we can simulate a random variable by applying a differentiable deterministic function on an auxiliary random variable whose distribution is fixed. For many distributions of interest (such as the gamma or Dirichlet), simulation of random variables relies on acceptance-rejection sampling. The discontinuity introduced by the accept-reject step means that standard reparameterization tricks are not applicable. We propose a new method that lets us leverage reparameterization gradients even when variables are outputs of a acceptance-rejection sampling algorithm. Our approach enables reparameterization on a larger class of variational distributions. In several studies of real and synthetic data, we show that the variance of the estimator of the gradient is significantly lower than other state-of-the-art methods. This leads to faster convergence of stochastic gradient variational inference.},
    opturldate = {2020-05-15},
    journal = {arXiv:1610.05683 [stat]},
    author = {Naesseth, Christian A. and Ruiz, Francisco J. R. and Linderman, Scott W. and Blei, David M.},
    month = feb,
    year = {2020},
    note = {arXiv: 1610.05683},
    file = {Naesseth et al_2020_Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms.pdf:/Users/suswei/Zotero/storage/U7Z49726/Naesseth et al_2020_Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/B3R96TVW/1610.html:text/html}
}

@article{grover_variational_2014,
    title = {Variational {Rejection} {Sampling}},
    abstract = {Learning latent variable models with stochastic variational inference is challenging when the approximate posterior is far from the true posterior, due to high variance in the gradient estimates. We propose a novel rejection sampling step that discards samples from the variational posterior which are assigned low likelihoods by the model. Our approach provides an arbitrarily accurate approximation of the true posterior at the expense of extra computation. Using a new gradient estimator for the resulting unnormalized proposal distribution, we achieve average improvements of 3.71 nats and 0.21 nats over state-of-theart single-sample and multi-sample alternatives respectively for estimating marginal loglikelihoods using sigmoid belief networks on the MNIST dataset.},
    language = {en},
    author = {Grover, Aditya and Gummadi, Ramki and Lazaro-Gredilla, Miguel and Schuurmans, Dale and Ermon, Stefano},
    year = {2014},
    pages = {10},
    file = {Grover et al. - 2014 - Variational Rejection Sampling.pdf:/Users/suswei/Zotero/storage/K4SV95NS/Grover et al. - 2014 - Variational Rejection Sampling.pdf:application/pdf}
}

@article{geiger_scaling_2020,
    title = {Scaling description of generalization with number of parameters in deep learning},
    volume = {2020},
    optissn = {1742-5468},
    opturl = {http://arxiv.org/abs/1901.01608},
    optdoi = {10.1088/1742-5468/ab633c},
    abstract = {Supervised deep learning involves the training of neural networks with a large number \$N\$ of parameters. For large enough \$N\$, in the so-called over-parametrized regime, one can essentially fit the training data points. Sparsity-based arguments would suggest that the generalization error increases as \$N\$ grows past a certain threshold \$N{\textasciicircum}\{*\}\$. Instead, empirical studies have shown that in the over-parametrized regime, generalization error keeps decreasing with \$N\$. We resolve this paradox through a new framework. We rely on the so-called Neural Tangent Kernel, which connects large neural nets to kernel methods, to show that the initialization causes finite-size random fluctuations \${\textbackslash}{\textbar}f\_\{N\}-{\textbackslash}bar\{f\}\_\{N\}{\textbackslash}{\textbar}{\textbackslash}sim N{\textasciicircum}\{-1/4\}\$ of the neural net output function \$f\_\{N\}\$ around its expectation \${\textbackslash}bar\{f\}\_\{N\}\$. These affect the generalization error \${\textbackslash}epsilon\_\{N\}\$ for classification: under natural assumptions, it decays to a plateau value \${\textbackslash}epsilon\_\{{\textbackslash}infty\}\$ in a power-law fashion \${\textbackslash}sim N{\textasciicircum}\{-1/2\}\$. This description breaks down at a so-called jamming transition \$N=N{\textasciicircum}\{*\}\$. At this threshold, we argue that \${\textbackslash}{\textbar}f\_\{N\}{\textbackslash}{\textbar}\$ diverges. This result leads to a plausible explanation for the cusp in test error known to occur at \$N{\textasciicircum}\{*\}\$. Our results are confirmed by extensive empirical observations on the MNIST and CIFAR image datasets. Our analysis finally suggests that, given a computational envelope, the smallest generalization error is obtained using several networks of intermediate sizes, just beyond \$N{\textasciicircum}\{*\}\$, and averaging their outputs.},
    number = {2},
    opturldate = {2020-05-26},
    journal = {Journal of Statistical Mechanics: Theory and Experiment},
    author = {Geiger, Mario and Jacot, Arthur and Spigler, Stefano and Gabriel, Franck and Sagun, Levent and d'Ascoli, Stéphane and Biroli, Giulio and Hongler, Clément and Wyart, Matthieu},
    month = feb,
    year = {2020},
    note = {arXiv: 1901.01608},
    pages = {023401},
    file = {arXiv Fulltext PDF:/Users/suswei/Zotero/storage/3SJUAKLI/Geiger et al. - 2020 - Scaling description of generalization with number .pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/KMDD4YIR/1901.html:text/html}
}

@article{lee_deep_2018,
    title = {Deep {Neural} {Networks} as {Gaussian} {Processes}},
    opturl = {http://arxiv.org/abs/1711.00165},
    abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks.},
    opturldate = {2020-05-26},
    journal = {arXiv:1711.00165 [cs, stat]},
    author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
    month = mar,
    year = {2018},
    note = {arXiv: 1711.00165},
    file = {arXiv Fulltext PDF:/Users/suswei/Zotero/storage/GWHDK34G/Lee et al. - 2018 - Deep Neural Networks as Gaussian Processes.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/PRP4FNVW/1711.html:text/html}
}

@article{nitta_resolution_2017,
    title = {Resolution of {Singularities} {Introduced} by {Hierarchical} {Structure} in {Deep} {Neural} {Networks}},
    volume = {28},
    optissn = {2162-2388},
    optdoi = {10.1109/TNNLS.2016.2580741},
    abstract = {We present a theoretical analysis of singular points of artificial deep neural networks, resulting in providing deep neural network models having no critical points introduced by a hierarchical structure. It is considered that such deep neural network models have good nature for gradient-based optimization. First, we show that there exist a large number of critical points introduced by a hierarchical structure in deep neural networks as straight lines, depending on the number of hidden layers and the number of hidden neurons. Second, we derive a sufficient condition for deep neural networks having no critical points introduced by a hierarchical structure, which can be applied to general deep neural networks. It is also shown that the existence of critical points introduced by a hierarchical structure is determined by the rank and the regularity of weight matrices for a specific class of deep neural networks. Finally, two kinds of implementation methods of the sufficient conditions to have no critical points are provided. One is a learning algorithm that can avoid critical points introduced by the hierarchical structure during learning (called avoidant learning algorithm). The other is a neural network that does not have some critical points introduced by the hierarchical structure as an inherent property (called avoidant neural network).},
    number = {10},
    journal = {IEEE Transactions on Neural Networks and Learning Systems},
    author = {Nitta, Tohru},
    month = oct,
    year = {2017},
    note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
    pages = {2282--2293},
    file = {Nitta - 2017 - Resolution of Singularities Introduced by Hierarch.pdf:/Users/suswei/Zotero/storage/SF9CGWJU/SF9CGWJU.pdf:application/pdf;Nitta - 2017 - Resolution of Singularities Introduced by Hierarch.pdf:/Users/suswei/Zotero/storage/NP885JA5/Nitta - 2017 - Resolution of Singularities Introduced by Hierarch.pdf:application/pdf}
}

@article{bahri_statistical_2020,
    title = {Statistical {Mechanics} of {Deep} {Learning}},
    volume = {11},
    opturl = {https://optdoi.org/10.1146/annurev-conmatphys-031119-050745},
    optdoi = {10.1146/annurev-conmatphys-031119-050745},
    abstract = {The recent striking success of deep neural networks in machine learning raises profound questions about the theoretical principles underlying their success. For example, what can such deep networks compute? How can we train them? How does information propagate through them? Why can they generalize? And how can we teach them to imagine? We review recent work in which methods of physical analysis rooted in statistical mechanics have begun to provide conceptual insights into these questions. These insights yield connections between deep learning and diverse physical and mathematical topics, including random landscapes, spin glasses, jamming, dynamical phase transitions, chaos, Riemannian geometry, random matrix theory, free probability, and nonequilibrium statistical mechanics. Indeed, the fields of statistical mechanics and machine learning have long enjoyed a rich history of strongly coupled interactions, and recent advances at the intersection of statistical mechanics and deep learning suggest these interactions will only deepen going forward.},
    number = {1},
    opturldate = {2020-05-28},
    journal = {Annual Review of Condensed Matter Physics},
    author = {Bahri, Yasaman and Kadmon, Jonathan and Pennington, Jeffrey and Schoenholz, Sam S. and Sohl-Dickstein, Jascha and Ganguli, Surya},
    year = {2020},
    pages = {501--528},
    file = {Full Text PDF:/Users/suswei/Zotero/storage/NMCLBVB2/NMCLBVB2.pdf:application/pdf}
}


@inproceedings{hinton_keeping_1993,
  title={Keeping the neural networks simple by minimizing the description length of the weights},
  author={Hinton, Geoffrey E and Van Camp, Drew},
  booktitle={Proceedings of the sixth annual conference on Computational learning theory},
  pages={5--13},
  year={1993}
}

@incollection{mizutani_analysis_2010,
    title = {An analysis on negative curvature induced by singularity in multi-layer neural-network learning},
    opturl = {http://papers.nips.cc/paper/4046-an-analysis-on-negative-curvature-induced-by-singularity-in-multi-layer-neural-network-learning.pdf},
    opturldate = {2020-05-28},
    booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 23},
    publisher = {Curran Associates, Inc.},
    author = {Mizutani, Eiji and Dreyfus, Stuart},
    editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
    year = {2010},
    pages = {1669--1677},
    file = {NIPS Full Text PDF:/Users/suswei/Zotero/storage/AIM3Y6HC/Mizutani and Dreyfus - 2010 - An analysis on negative curvature induced by singu.pdf:application/pdf;NIPS Snapshot:/Users/suswei/Zotero/storage/8GSCSA3L/4046-an-analysis-on-negative-curvature-induced-by-singularity-in-multi-layer-neural-network-lea.html:text/html}
}

@article{nguyen_loss_2017,
    title = {The {Loss} {Surface} of {Deep} and {Wide} {Neural} {Networks}},
    abstract = {While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.},
    language = {en},
    author = {Nguyen, Quynh and Hein, Matthias},
    year = {2017},
    pages = {14},
    file = {Nguyen and Hein - The Loss Surface of Deep and Wide Neural Networks.pdf:/Users/suswei/Zotero/storage/WMLY2FLL/Nguyen and Hein - The Loss Surface of Deep and Wide Neural Networks.pdf:application/pdf}
}

@incollection{amari_geometrical_2002,
    title = {Geometrical {Singularities} in the {Neuromanifold} of {Multilayer} {Perceptrons}},
    opturl = {http://papers.nips.cc/paper/2015-geometrical-singularities-in-the-neuromanifold-of-multilayer-perceptrons.pdf},
    opturldate = {2020-05-28},
    booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 14},
    publisher = {MIT Press},
    author = {Amari, Shun-ichi and Park, Hyeyoung and Ozeki, Tomoko},
    editor = {Dietterich, T. G. and Becker, S. and Ghahramani, Z.},
    year = {2002},
    pages = {343--350},
    file = {NIPS Full Text PDF:/Users/suswei/Zotero/storage/2RVNXTY2/Amari et al. - 2002 - Geometrical Singularities in the Neuromanifold of .pdf:application/pdf;NIPS Snapshot:/Users/suswei/Zotero/storage/SLADRQ2N/2015-geometrical-singularities-in-the-neuromanifold-of-multilayer-perceptrons.html:text/html}
}

@book{calin_deep_2020,
    address = {Cham},
    series = {Springer {Series} in the {Data} {Sciences}},
    title = {Deep {Learning} {Architectures}: {A} {Mathematical} {Approach}},
    optisbn = {978-3-030-36720-6 978-3-030-36721-3},
    shorttitle = {Deep {Learning} {Architectures}},
    opturl = {http://link.springer.com/10.1007/978-3-030-36721-3},
    language = {en},
    opturldate = {2020-05-30},
    publisher = {Springer International Publishing},
    author = {Calin, Ovidiu},
    year = {2020},
    optdoi = {10.1007/978-3-030-36721-3},
    file = {Calin - 2020 - Deep Learning Architectures A Mathematical Approa.pdf:/Users/suswei/Zotero/storage/NH7FLJ53/Calin - 2020 - Deep Learning Architectures A Mathematical Approa.pdf:application/pdf}
}

@article{amari_singularities_2006,
    title = {Singularities {Affect} {Dynamics} of {Learning} in {Neuromanifolds}},
    language = {en},
    author = {Amari, Shun-ichi and Park, Hyeyoung and Ozeki, Tomoko},
    year = {2006},
    pages = {59},
    file = {Amari et al. - Singularities Affect Dynamics of Learning in Neuro.pdf:/Users/suswei/Zotero/storage/9XCQATNU/9XCQATNU.pdf:application/pdf}
}

@incollection{karakida_normalization_2019,
    title = {The {Normalization} {Method} for {Alleviating} {Pathological} {Sharpness} in {Wide} {Neural} {Networks}},
    opturl = {http://papers.nips.cc/paper/8869-the-normalization-method-for-alleviating-pathological-sharpness-in-wide-neural-networks.pdf},
    booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
    publisher = {Curran Associates, Inc.},
    author = {Karakida, Ryo and Akaho, Shotaro and Amari, Shun-ichi},
    editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
    year = {2019},
    pages = {6406--6416},
    file = {Full Text:/Users/suswei/Zotero/storage/THK5T3TA/Karakida et al. - 2019 - The Normalization Method for Alleviating Pathologi.pdf:application/pdf}
}

@article{karakida_universal_2019,
    title = {Universal {Statistics} of {Fisher} {Information} in {Deep} {Neural} {Networks}: {Mean} {Field} {Approach}},
    shorttitle = {Universal {Statistics} of {Fisher} {Information} in {Deep} {Neural} {Networks}},
    opturl = {http://arxiv.org/abs/1806.01316},
    abstract = {The Fisher information matrix (FIM) is a fundamental quantity to represent the characteristics of a stochastic model, including deep neural networks (DNNs). The present study reveals novel statistics of FIM that are universal among a wide class of DNNs. To this end, we use random weights and large width limits, which enables us to utilize mean field theories. We investigate the asymptotic statistics of the FIM's eigenvalues and reveal that most of them are close to zero while the maximum eigenvalue takes a huge value. Because the landscape of the parameter space is defined by the FIM, it is locally flat in most dimensions, but strongly distorted in others. Moreover, we demonstrate the potential usage of the derived statistics in learning strategies. First, small eigenvalues that induce flatness can be connected to a norm-based capacity measure of generalization ability. Second, the maximum eigenvalue that induces the distortion enables us to quantitatively estimate an appropriately sized learning rate for gradient methods to converge.},
    opturldate = {2020-06-04},
    journal = {arXiv:1806.01316 [cond-mat, stat]},
    author = {Karakida, Ryo and Akaho, Shotaro and Amari, Shun-ichi},
    month = oct,
    year = {2019},
    note = {arXiv: 1806.01316},
    file = {arXiv Fulltext PDF:/Users/suswei/Zotero/storage/L7UJGEDZ/L7UJGEDZ.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/9MIVUKIU/1806.html:text/html}
}

@article{liang_fisher-rao_2019,
    title = {Fisher-{Rao} {Metric}, {Geometry}, and {Complexity} of {Neural} {Networks}},
    opturl = {http://arxiv.org/abs/1711.01530},
    abstract = {We study the relationship between geometry and capacity measures for deep neural networks from an invariance viewpoint. We introduce a new notion of capacity --- the Fisher-Rao norm --- that possesses desirable invariance properties and is motivated by Information Geometry. We discover an analytical characterization of the new capacity measure, through which we establish norm-comparison inequalities and further show that the new measure serves as an umbrella for several existing norm-based complexity measures. We discuss upper bounds on the generalization error induced by the proposed measure. Extensive numerical experiments on CIFAR-10 support our theoretical findings. Our theoretical analysis rests on a key structural lemma about partial derivatives of multi-layer rectifier networks.},
    opturldate = {2020-06-04},
    journal = {arXiv:1711.01530 [cs, stat]},
    author = {Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
    month = feb,
    year = {2019},
    note = {arXiv: 1711.01530},
    file = {arXiv Fulltext PDF:/Users/suswei/Zotero/storage/IPS4UDME/Liang et al. - 2019 - Fisher-Rao Metric, Geometry, and Complexity of Neu.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/XCK4WS57/1711.html:text/html}
}

@article{watanabe_asymptotic_2010,
    title = {Asymptotic {Learning} {Curve} and {Renormalizable} {Condition} in {Statistical} {Learning} {Theory}},
    volume = {233},
    optissn = {1742-6596},
    opturl = {http://arxiv.org/abs/1001.2957},
    optdoi = {10.1088/1742-6596/233/1/012014},
    abstract = {Bayes statistics and statistical physics have the common mathematical structure, where the log likelihood function corresponds to the random Hamiltonian. Recently, it was discovered that the asymptotic learning curves in Bayes estimation are subject to a universal law, even if the log likelihood function can not be approximated by any quadratic form. However, it is left unknown what mathematical property ensures such a universal law. In this paper, we define a renormalizable condition of the statistical estimation problem, and show that, under such a condition, the asymptotic learning curves are ensured to be subject to the universal law, even if the true distribution is unrealizable and singular for a statistical model. Also we study a nonrenormalizable case, in which the learning curves have the different asymptotic behaviors from the universal law.},
    opturldate = {2020-06-15},
    journal = {Journal of Physics: Conference Series},
    author = {Watanabe, Sumio},
    month = jun,
    year = {2010},
    note = {arXiv: 1001.2957},
    pages = {012014},
    file = {arXiv Fulltext PDF:/Users/suswei/Zotero/storage/DYWUVXP9/DYWUVXP9.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/HLGQFUHM/1001.html:text/html}
}

@article{martin_rethinking_2019,
    title = {Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior},
    shorttitle = {Rethinking generalization requires revisiting old ideas},
    opturl = {http://arxiv.org/abs/1710.09553},
    abstract = {We describe an approach to understand the peculiar and counterintuitive generalization properties of deep neural networks. The approach involves going beyond worst-case theoretical capacity control frameworks that have been popular in machine learning in recent years to revisit old ideas in the statistical mechanics of neural networks. Within this approach, we present a prototypical Very Simple Deep Learning (VSDL) model, whose behavior is controlled by two control parameters, one describing an effective amount of data, or load, on the network (that decreases when noise is added to the input), and one with an effective temperature interpretation (that increases when algorithms are early stopped). Using this model, we describe how a very simple application of ideas from the statistical mechanics theory of generalization provides a strong qualitative description of recently-observed empirical results regarding the inability of deep neural networks not to overfit training data, discontinuous learning and sharp transitions in the generalization properties of learning algorithms, etc.},
    opturldate = {2020-06-15},
    journal = {arXiv:1710.09553 [cs, stat]},
    author = {Martin, Charles H. and Mahoney, Michael W.},
    month = feb,
    year = {2019},
    note = {arXiv: 1710.09553},
    file = {arXiv Fulltext PDF:/Users/suswei/Zotero/storage/4AMX2QXC/Martin and Mahoney - 2019 - Rethinking generalization requires revisiting old .pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/7YTWQZWN/1710.html:text/html}
}

@article{karakida_pathological_2019,
    title = {Pathological spectra of the {Fisher} information metric and its variants in deep neural networks},
    author = {Karakida, Ryo},
    year = {2019},
    file = {Full Text PDF:/Users/suswei/Zotero/storage/U476BKLA/1910.05992.pdf:application/pdf}
}

@article{aida_renormalization_2005,
    title = {Renormalization {Group} in {Bayesian} {Statistical} {Inference}},
    volume = {157},
    optissn = {0375-9687},
    opturl = {https://academic.oup.com/ptps/article-lookup/optdoi/10.1143/PTPS.157.296},
    optdoi = {10.1143/PTPS.157.296},
    language = {en},
    opturldate = {2020-06-22},
    journal = {Progress of Theoretical Physics Supplement},
    author = {Aida, Toshiaki},
    year = {2005},
    keywords = {renormalization},
    pages = {296--299},
    file = {Aida - 2005 - Renormalization Group in Bayesian Statistical Infe.pdf:/Users/suswei/Zotero/storage/9WVKEJHL/Aida - 2005 - Renormalization Group in Bayesian Statistical Infe.pdf:application/pdf}
}

@article{mehta_exact_2014,
    title = {An exact mapping between the {Variational} {Renormalization} {Group} and {Deep} {Learning}},
    opturl = {http://arxiv.org/abs/1410.3831},
    abstract = {Deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data. Recently, such techniques have yielded record-breaking results on a diverse set of difficult machine learning tasks in computer vision, speech recognition, and natural language processing. Despite the enormous success of deep learning, relatively little is understood theoretically about why these techniques are so successful at feature learning and compression. Here, we show that deep learning is intimately related to one of the most important and successful techniques in theoretical physics, the renormalization group (RG). RG is an iterative coarse-graining scheme that allows for the extraction of relevant features (i.e. operators) as a physical system is examined at different length scales. We construct an exact mapping from the variational renormalization group, first introduced by Kadanoff, and deep learning architectures based on Restricted Boltzmann Machines (RBMs). We illustrate these ideas using the nearest-neighbor Ising Model in one and two-dimensions. Our results suggests that deep learning algorithms may be employing a generalized RG-like scheme to learn relevant features from data.},
    opturldate = {2020-06-22},
    journal = {arXiv:1410.3831 [cond-mat, stat]},
    author = {Mehta, Pankaj and Schwab, David J.},
    month = oct,
    year = {2014},
    note = {arXiv: 1410.3831},
    keywords = {renormalization},
    file = {arXiv Fulltext PDF:/Users/suswei/Zotero/storage/GDL5SYU9/Mehta and Schwab - 2014 - An exact mapping between the Variational Renormali.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/KXDA5TMA/1410.html:text/html}
}

@article{thierry-mieg_connections_2019,
    title = {Connections between physics, mathematics, and deep learning},
    volume = {2},
    optissn = {26322714},
    opturl = {http://journals.andromedapublisher.com/index.php/LHEP/article/view/110/74},
    optdoi = {10.31526/LHEP.3.2019.110},
    abstract = {Starting from Fermat’s principle of least action, which governs classical and quantum mechanics and from the theory of exterior differential forms, which governs the geometry of curved manifolds, we show how to derive the equations governing neural networks in an intrinsic, coordinate-invariant way, where the loss function plays the role of the Hamiltonian. To be covariant, these equations imply a layer metric which is instrumental in pretraining and explains the role of conjugation when using complex numbers. The differential formalism clariﬁes the relation of the gradient descent optimizer with Aristotelian and Newtonian mechanics. The Bayesian paradigm is then analyzed as a renormalizable theory yielding a new derivation of the Bayesian information criterion. We hope that this formal presentation of the differential geometry of neural networks will encourage some physicists to dive into deep learning and, reciprocally, that the specialists of deep learning will better appreciate the close interconnection of their subject with the foundations of classical and quantum ﬁeld theory.},
    language = {en},
    number = {3},
    opturldate = {2020-06-22},
    journal = {Letters in High Energy Physics},
    author = {Thierry-Mieg, Jean and {NCBI, National Library of Medicine, National Institutes of Health, 8600 Rockville Pike, Bethesda, MD 20894, USA}},
    month = sep,
    year = {2019},
    keywords = {renormalization},
    file = {Thierry-Mieg and NCBI, National Library of Medicine, National Institutes of Health, 8600 Rockville Pike, Bethesda, MD 20894, USA - 2019 - Connections between physics, mathematics, and deep.pdf:/Users/suswei/Zotero/storage/J94U7MKP/Thierry-Mieg and NCBI, National Library of Medicine, National Institutes of Health, 8600 Rockville Pike, Bethesda, MD 20894, USA - 2019 - Connections between physics, mathematics, and deep.pdf:application/pdf}
}

@article{watanabe_asymptotic_nodate,
    title = {Asymptotic {Equivalence} of {Bayes} {Cross} {Validation} and {Widely} {Applicable} {Information} {Criterion} in {Singular} {Learning} {Theory}},
    abstract = {In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2λ/n, where λ is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion.},
    language = {en},
    author = {Watanabe, Sumio},
    pages = {24},
    file = {Watanabe - Asymptotic Equivalence of Bayes Cross Validation a.pdf:/Users/suswei/Zotero/storage/NAHCKS2H/Watanabe - Asymptotic Equivalence of Bayes Cross Validation a.pdf:application/pdf;Watanabe - Asymptotic Equivalence of Bayes Cross Validation a.pdf:/Users/suswei/Zotero/storage/B5N5ZILQ/Watanabe - Asymptotic Equivalence of Bayes Cross Validation a.pdf:application/pdf}
}

@inproceedings{hagiwara_problem_2000,
    title = {On the problem in model selection of neural network regression in overrealizable scenario},
    volume = {6},
    optdoi = {10.1109/IJCNN.2000.859438},
    abstract = {In this article, we analyze the expected training error and the expected generalization error in a special case of overrealizable scenario, in which output data is a Gaussian noise sequence. Firstly, we derived the upper bound of the expected training error of a network, which is independent of input probability distributions. Secondly, based on the first result, we derived the lower bound for the expected generalization error of a network, provided that the inputs are not stochastic. From the first result, it is clear that we should evaluate the degree of overfitting of a network to noise component in data more larger than the evaluation in NIC. From the second result, the expected generalization error, which is directly associated with the model selection criterion, is larger than in NIC. These results suggest that the model selection criterion in overrealizable scenario will be larger than NIC if inputs are not stochastic. Additionally, the results of numerical experiments agree with our theoretical results.},
    booktitle = {Proceedings of the {IEEE}-{INNS}-{ENNS} {International} {Joint} {Conference} on {Neural} {Networks}. {IJCNN} 2000. {Neural} {Computing}: {New} {Challenges} and {Perspectives} for the {New} {Millennium}},
    author = {Hagiwara, K. and Kuno, K. and Usui, S.},
    month = jul,
    year = {2000},
    note = {optissn: 1098-7576},
    pages = {461--466 vol.6},
    file = {IEEE Xplore Full Text PDF:/Users/suswei/Zotero/storage/TDR6VHDN/Hagiwara et al. - 2000 - On the problem in model selection of neural networ.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/suswei/Zotero/storage/J6MCEJVN/859438.html:text/html}
}

@article{zwiernik_asymptotic_nodate,
    title = {An {Asymptotic} {Behaviour} of the {Marginal} {Likelihood} for {General} {Markov} {Models}},
    abstract = {The standard Bayesian Information Criterion (BIC) is derived under regularity conditions which are not always satisﬁed in the case of graphical models with hidden variables. In this paper we derive the BIC for the binary graphical tree models where all the inner nodes of a tree represent binary hidden variables. This provides an extension of a similar formula given by Rusakov and Geiger for naive Bayes models. The main tool used in this paper is the connection between the growth behavior of marginal likelihood integrals and the real log-canonical threshold.},
    language = {en},
    author = {Zwiernik, Piotr},
    pages = {28},
    file = {Zwiernik - An Asymptotic Behaviour of the Marginal Likelihood.pdf:/Users/suswei/Zotero/storage/88Z5I2M3/Zwiernik - An Asymptotic Behaviour of the Marginal Likelihood.pdf:application/pdf}
}

@article{bhattacharya_nonasymptotic_2020,
    title = {Nonasymptotic {Laplace} approximation under model misspecification},
    opturl = {http://arxiv.org/abs/2005.07844},
    abstract = {We present non-asymptotic two-sided bounds to the log-marginal likelihood in Bayesian inference. The classical Laplace approximation is recovered as the leading term. Our derivation permits model misspeciﬁcation and allows the parameter dimension to grow with the sample size. We do not make any assumptions about the asymptotic shape of the posterior, and instead require certain regularity conditions on the likelihood ratio and that the posterior to be suﬃciently concentrated.},
    language = {en},
    opturldate = {2020-06-24},
    journal = {arXiv:2005.07844 [math, stat]},
    author = {Bhattacharya, Anirban and Pati, Debdeep},
    month = jun,
    year = {2020},
    note = {arXiv: 2005.07844},
    keywords = {misspecification},
    file = {Bhattacharya and Pati - 2020 - Nonasymptotic Laplace approximation under model mi.pdf:/Users/suswei/Zotero/storage/WNBTQX6I/Bhattacharya and Pati - 2020 - Nonasymptotic Laplace approximation under model mi.pdf:application/pdf}
}

@article{lv_model_2014,
    title = {Model selection principles in misspecified models},
    volume = {76},
    optissn = {13697412},
    opturl = {http://optdoi.wiley.com/10.1111/rssb.12023},
    optdoi = {10.1111/rssb.12023},
    abstract = {Model selection is of fundamental importance to high dimensional modelling featured in many contemporary applications. Classical principles of model selection include the Bayesian principle and the Kullback–Leibler divergence principle, which lead to the Bayesian information criterion and Akaike information criterion respectively, when models are correctly speciﬁed. Yet model misspeciﬁcation is unavoidable in practice. We derive novel asymptotic expansions of the two well-known principles in misspeciﬁed generalized linear models, which give the generalized Bayesian information criterion and generalized Akaike information criterion. A speciﬁc form of prior probabilities motivated by the Kullback–Leibler divergence principle leads to the generalized Bayesian information criterion with prior probability, GBICp, which can be naturally decomposed as the sum of the negative maximum quasi-log-likelihood, a penalty on model dimensionality, and a penalty on model misspeciﬁcation directly. Numerical studies demonstrate the advantage of the new methods for model selection in both correctly speciﬁed and misspeciﬁed models.},
    language = {en},
    number = {1},
    opturldate = {2020-06-24},
    journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
    author = {Lv, Jinchi and Liu, Jun S.},
    month = jan,
    year = {2014},
    keywords = {misspecification},
    pages = {141--167},
    file = {Lv and Liu - 2014 - Model selection principles in misspecified models.pdf:/Users/suswei/Zotero/storage/SJK3SCWF/Lv and Liu - 2014 - Model selection principles in misspecified models.pdf:application/pdf}
}

@article{lee_wide_2019,
    title = {Wide {Neural} {Networks} of {Any} {Depth} {Evolve} as {Linear} {Models} {Under} {Gradient} {Descent}},
    opturl = {http://arxiv.org/abs/1902.06720},
    abstract = {A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the inﬁnite width limit, they are governed by a linear model obtained from the ﬁrst-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the inﬁnite width limit, we nevertheless ﬁnd excellent empirical agreement between the predictions of the original network and those of the linearized version even for ﬁnite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.},
    language = {en},
    opturldate = {2020-06-24},
    journal = {arXiv:1902.06720 [cs, stat]},
    author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S. and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
    month = dec,
    year = {2019},
    note = {arXiv: 1902.06720},
    file = {Full Text:/Users/suswei/Zotero/storage/PVMWUFIH/Lee et al. - 2019 - Wide Neural Networks of Any Depth Evolve as Linear.pdf:application/pdf}
}

@article{lee_gaussian_nodate,
    title = {Gaussian {Predictions} from {Gradient} {Descent} {Training} of {Wide} {Neural} {Networks}},
    language = {en},
    author = {Lee, Jaehoon and Xiao, Lechao and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
    pages = {6},
    file = {Lee et al. - Gaussian Predictions from Gradient Descent Trainin.pdf:/Users/suswei/Zotero/storage/XQALUFHD/Lee et al. - Gaussian Predictions from Gradient Descent Trainin.pdf:application/pdf}
}

@article{hou_minimal_2019,
    title = {Minimal model of permutation symmetry in unsupervised learning},
    volume = {52},
    optissn = {1751-8113, 1751-8121},
    opturl = {http://arxiv.org/abs/1904.13052},
    optdoi = {10.1088/1751-8121/ab3f3f},
    abstract = {Permutation of any two hidden units yields invariant properties in typical deep generative neural networks. This permutation symmetry plays an important role in understanding the computation performance of a broad class of neural networks with two or more hidden units. However, a theoretical study of the permutation symmetry is still lacking. Here, we propose a minimal model with only two hidden units in a restricted Boltzmann machine, which aims to address how the permutation symmetry affects the critical learning data size at which the concept-formation (or spontaneous symmetry breaking in physics language) starts, and moreover semi-rigorously prove a conjecture that the critical data size is independent of the number of hidden units once this number is finite. Remarkably, we find that the embedded correlation between two receptive fields of hidden units reduces the critical data size. In particular, the weakly-correlated receptive fields have the benefit of significantly reducing the minimal data size that triggers the transition, given less noisy data. Inspired by the theory, we also propose an efficient fully-distributed algorithm to infer the receptive fields of hidden units. Furthermore, our minimal model reveals that the permutation symmetry can also be spontaneously broken following the spontaneous symmetry breaking. Overall, our results demonstrate that the unsupervised learning is a progressive combination of spontaneous symmetry breaking and permutation symmetry breaking which are both spontaneous processes driven by data streams (observations). All these effects can be analytically probed based on the minimal model, providing theoretical insights towards understanding unsupervised learning in a more general context.},
    number = {41},
    opturldate = {2020-07-01},
    journal = {Journal of Physics A: Mathematical and Theoretical},
    author = {Hou, Tianqi and Wong, K. Y. Michael and Huang, Haiping},
    month = oct,
    year = {2019},
    note = {arXiv: 1904.13052},
    keywords = {spontaneous symmetry breaking},
    pages = {414001}
}

@article{vehtari_survey_2012,
    title = {A survey of {Bayesian} predictive methods for model assessment, selection and comparison},
    volume = {6},
    optissn = {1935-7516},
    opturl = {https://projecteuclid.org/euclid.ssu/1356628931},
    optdoi = {10.1214/12-SS102},
    abstract = {To date, several methods exist in the statistical literature for model assessment, which purport themselves specifically as Bayesian predictive methods. The decision theoretic assumptions on which these methods are based are not always clearly stated in the original articles, however. The aim of this survey is to provide a unified review of Bayesian predictive model assessment and selection methods, and of methods closely related to them. We review the various assumptions that are made in this context and discuss the connections between different approaches, with an emphasis on how each method approximates the expected utility of using a Bayesian model for the purpose of predicting future data.},
    language = {EN},
    opturldate = {2020-07-25},
    journal = {Statistics Surveys},
    author = {Vehtari, Aki and Ojanen, Janne},
    year = {2012},
    mrnumber = {MR3011074},
    zmnumber = {1302.62011},
    note = {Publisher: The author, under a Creative Commons Attribution License},
    pages = {142--228},
    file = {Full Text PDF:/Users/suswei/Zotero/storage/JPUUB572/Vehtari and Ojanen - 2012 - A survey of Bayesian predictive methods for model .pdf:application/pdf;Snapshot:/Users/suswei/Zotero/storage/NJQHNKI3/1356628931.html:text/html}
}

@article{hwang_prediction_1997,
    title = {Prediction {Intervals} for {Artificial} {Neural} {Networks}},
    volume = {92},
    optissn = {0162-1459, 1537-274X},
    opturl = {http://www.tandfonline.com/optdoi/abs/10.1080/01621459.1997.10474027},
    optdoi = {10.1080/01621459.1997.10474027},
    language = {en},
    number = {438},
    opturldate = {2020-07-27},
    journal = {Journal of the American Statistical Association},
    author = {Hwang, J. T. Gene and Ding, A. Adam},
    month = jun,
    year = {1997},
    pages = {748--757},
    file = {Hwang and Ding - 1997 - Prediction Intervals for Artificial Neural Network.pdf:/Users/suswei/Zotero/storage/BPNMIZQE/Hwang and Ding - 1997 - Prediction Intervals for Artificial Neural Network.pdf:application/pdf}
}

@article{pourzanjani_improving_nodate,
    title = {Improving the {Identiﬁability} of {Neural} {Networks} for {Bayesian} {Inference}},
    language = {en},
    author = {Pourzanjani, Arya A and Jiang, Richard M and Petzold, Linda R},
    pages = {5},
    file = {Pourzanjani et al. - Improving the Identiﬁability of Neural Networks fo.pdf:/Users/suswei/Zotero/storage/3QFWACVP/Pourzanjani et al. - Improving the Identiﬁability of Neural Networks fo.pdf:application/pdf;Pourzanjani et al. - Improving the Identiﬁability of Neural Networks fo.pdf:/Users/suswei/Zotero/storage/Y99N76TW/Pourzanjani et al. - Improving the Identiﬁability of Neural Networks fo.pdf:application/pdf}
}

@article{vlacic_neural_2019,
    title = {Neural network identifiability for a family of sigmoidal nonlinearities},
    opturl = {http://arxiv.org/abs/1906.06994},
    abstract = {This paper addresses the following question of neural network identifiability: Does the input-output map realized by a feed-forward neural network with respect to a given nonlinearity uniquely specify the network architecture, weights, and biases? Existing literature on the subject Sussman 1992, Albertini, Sontag et al. 1993, Fefferman 1994 suggests that the answer should be yes, up to certain symmetries induced by the nonlinearity, and provided the networks under consideration satisfy certain "genericity conditions". The results in Sussman 1992 and Albertini, Sontag et al. 1993 apply to networks with a single hidden layer and in Fefferman 1994 the networks need to be fully connected. In an effort to answer the identifiability question in greater generality, we derive necessary genericity conditions for the identifiability of neural networks of arbitrary depth and connectivity with an arbitrary nonlinearity. Moreover, we construct a family of nonlinearities for which these genericity conditions are minimal, i.e., both necessary and sufficient. This family is large enough to approximate many commonly encountered nonlinearities to arbitrary precision in the uniform norm.},
    opturldate = {2020-07-27},
    journal = {arXiv:1906.06994 [cs, math, stat]},
    author = {Vlačić, Verner and Bölcskei, Helmut},
    month = jun,
    year = {2019},
    note = {arXiv: 1906.06994},
    file = {arXiv Fulltext PDF:/Users/suswei/Zotero/storage/STXQ9L3U/Vlačić and Bölcskei - 2019 - Neural network identifiability for a family of sig.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/62MRR88E/1906.html:text/html}
}

@article{rolnick_reverse-engineering_nodate,
    title = {Reverse-{Engineering} {Deep} {ReLU} {Networks}},
    abstract = {The output of a neural network depends on its ar­ chitecture and weights in a highly nonlinear way, and it is often assumed that a network’s param­ eters cannot be recovered from its output. Here, we prove that, in fact, it is frequently possible to reconstruct the architecture, weights, and biases of a deep ReLU network by observing only its output. We leverage the fact that every ReLU net­ work deﬁnes a piecewise linear function, where the boundaries between linear regions correspond to inputs for which some neuron in the network switches between inactive and active ReLU states. By dissecting the set of region boundaries into components associated with particular neurons, we show both theoretically and empirically that it is possible to recover the weights of neurons and their arrangement within the network, up to isomorphism.},
    language = {en},
    author = {Rolnick, David and Körding, Konrad P},
    pages = {10},
    file = {Rolnick and Körding - Reverse-Engineering Deep ReLU Networks.pdf:/Users/suswei/Zotero/storage/TAI2T276/Rolnick and Körding - Reverse-Engineering Deep ReLU Networks.pdf:application/pdf}
}

@article{lee_difficulties_2002,
    title = {Difficulties in {Estimating} the {Normalizing} {Constant} of the {Posterior} for a {Neural} {Network}},
    volume = {11},
    optissn = {1061-8600},
    opturl = {https://www.jstor.org/stable/1391136},
    abstract = {This article reviews a number of methods for estimating normalizing constants in the context of neural network regression. Model selection or model averaging within the Bayesian approach requires computation of the normalizing constant of the posterior. This integral can be challenging to estimate, particularly for a neural network where the posterior contours are neither unimodal nor Gaussian-shaped. Surprisingly, all of the methods discussed in this article have a large amount of difficulty with this problem.},
    number = {1},
    opturldate = {2020-07-28},
    journal = {Journal of Computational and Graphical Statistics},
    author = {Lee, Herbert K. H.},
    year = {2002},
    note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
    pages = {222--235},
    file = {JSTOR Full Text PDF:/Users/suswei/Zotero/storage/PM85FSCP/Lee - 2002 - Difficulties in Estimating the Normalizing Constan.pdf:application/pdf}
}

@article{javid_compromise-free_2020,
    title = {Compromise-free {Bayesian} neural networks},
    opturl = {http://arxiv.org/abs/2004.12211},
    abstract = {We conduct a thorough analysis of the relationship between the out-of-sample performance and the Bayesian evidence (marginal likelihood) of Bayesian neural networks (BNNs), as well as looking at the performance of ensembles of BNNs, both using the Boston housing dataset. Using the state-of-the-art in nested sampling, we numerically sample the full (non-Gaussian and multimodal) network posterior and obtain numerical estimates of the Bayesian evidence, considering network models with up to 156 trainable parameters. The networks have between zero and four hidden layers, either \${\textbackslash}tanh\$ or \$ReLU\$ activation functions, and with and without hierarchical priors. The ensembles of BNNs are obtained by determining the posterior distribution over networks, from the posterior samples of individual BNNs re-weighted by the associated Bayesian evidence values. There is good correlation between out-of-sample performance and evidence, as well as a remarkable symmetry between the evidence versus model size and out-of-sample performance versus model size planes. Networks with \$ReLU\$ activation functions have consistently higher evidences than those with \${\textbackslash}tanh\$ functions, and this is reflected in their out-of-sample performance. Ensembling over architectures acts to further improve performance relative to the individual BNNs.},
    opturldate = {2020-08-03},
    journal = {arXiv:2004.12211 [cs, stat]},
    author = {Javid, Kamran and Handley, Will and Hobson, Mike and Lasenby, Anthony},
    month = jun,
    year = {2020},
    note = {arXiv: 2004.12211},
    file = {arXiv.org Snapshot:/Users/suswei/Zotero/storage/VY4F2R92/2004.html:text/html;arXiv Fulltext PDF:/Users/suswei/Zotero/storage/I6A67DXY/Javid et al. - 2020 - Compromise-free Bayesian neural networks.pdf:application/pdf}
}


@book{watanabe_mathematical_2018,
    title = {Mathematical {Theory} of {Bayesian} {Statistics}},
    optisbn = {978-1-4822-3808-2},
    opturl = {https://books.google.gr/books?id=GqZYDwAAQBAJ},
    publisher = {CRC Press},
    author = {Watanabe, S.},
    year = {2018}
}


@article{kobyzev_normalizing_2020,
    title = {Normalizing {Flows}: {An} {Introduction} and {Review} of {Current} {Methods}},
    optissn = {0162-8828, 2160-9292, 1939-3539},
    shorttitle = {Normalizing {Flows}},
    opturl = {http://arxiv.org/abs/1908.09257},
    optdoi = {10.1109/TPAMI.2020.2992934},
    abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
    opturldate = {2020-08-08},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
    year = {2020},
    note = {arXiv: 1908.09257},
    keywords = {NF},
    pages = {1--1},
    file = {arXiv Fulltext PDF:/Users/suswei/Zotero/storage/K6RQR2XD/Kobyzev et al. - 2020 - Normalizing Flows An Introduction and Review of C.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/Y73TYICV/1908.html:text/html}
}

@incollection{watanabe_effect_2003,
    title = {The {Effect} of {Singularities} in a {Learning} {Machine} when the {True} {Parameters} {Do} {Not} {Lie} on such {Singularities}},
    opturl = {http://papers.nips.cc/paper/2255-the-effect-of-singularities-in-a-learning-machine-when-the-true-parameters-do-not-lie-on-such-singularities.pdf},
    opturldate = {2020-08-09},
    booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 15},
    publisher = {MIT Press},
    author = {Watanabe, Sumio and Amari, Shun-ichi},
    editor = {Becker, S. and Thrun, S. and Obermayer, K.},
    year = {2003},
    pages = {407--414},
    file = {Watanabe_Amari_2003_The Effect of Singularities in a Learning Machine when the True Parameters Do.pdf:/Users/suswei/Zotero/storage/PBJSUA97/Watanabe_Amari_2003_The Effect of Singularities in a Learning Machine when the True Parameters Do.pdf:application/pdf;NIPS Snapshot:/Users/suswei/Zotero/storage/ZSMNFTB5/2255-the-effect-of-singularities-in-a-learning-machine-when-the-true-parameters-do-not-lie-on-s.html:text/html}
}

@article{lin_hypersurfaces_2014,
    title = {Hypersurfaces and their singularities in partial correlation testing},
    volume = {14},
    optissn = {1615-3375, 1615-3383},
    opturl = {http://arxiv.org/abs/1209.0285},
    optdoi = {10.1007/s10208-014-9205-0},
    abstract = {An asymptotic theory is developed for computing volumes of regions in the parameter space of a directed Gaussian graphical model that are obtained by bounding partial correlations. We study these volumes using the method of real log canonical thresholds from algebraic geometry. Our analysis involves the computation of the singular loci of correlation hypersurfaces. Statistical applications include the strong-faithfulness assumption for the PC-algorithm, and the quantification of confounder bias in causal inference. A detailed analysis is presented for trees, bow-ties, tripartite graphs, and complete graphs.},
    number = {5},
    opturldate = {2020-08-09},
    journal = {Foundations of Computational Mathematics},
    author = {Lin, Shaowei and Uhler, Caroline and Sturmfels, Bernd and Bühlmann, Peter},
    month = oct,
    year = {2014},
    note = {arXiv: 1209.0285},
    pages = {1079--1116},
    file = {Lin et al_2014_Hypersurfaces and their singularities in partial correlation testing.pdf:/Users/suswei/Zotero/storage/K79ZHDQR/Lin et al_2014_Hypersurfaces and their singularities in partial correlation testing.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/L3ZKFUFY/1209.html:text/html}
}

@book{drton_lectures_2009,
    address = {Basel},
    title = {Lectures on {Algebraic} {Statistics}},
    optisbn = {978-3-7643-8904-8 978-3-7643-8905-5},
    opturl = {http://link.springer.com/10.1007/978-3-7643-8905-5},
    language = {en},
    opturldate = {2020-08-09},
    publisher = {Birkhäuser Basel},
    author = {Drton, Mathias and Sturmfels, Bernd and Sullivant, Seth},
    year = {2009},
    optdoi = {10.1007/978-3-7643-8905-5},
    file = {Drton et al. - 2009 - Lectures on Algebraic Statistics.pdf:/Users/suswei/Zotero/storage/T7HAXL66/Drton et al. - 2009 - Lectures on Algebraic Statistics.pdf:application/pdf}
}

@article{smith_stochastic_2018,
  title={Stochastic natural gradient descent draws posterior samples in function space},
  author={Smith, Samuel L and Duckworth, Daniel and Rezchikov, Semon and Le, Quoc V and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1806.09597},
  year={2018}
}


@article{papamarkou_challenges_2019,
    title = {Challenges in {Bayesian} inference via {Markov} chain {Monte} {Carlo} for neural networks},
    opturl = {http://arxiv.org/abs/1910.06539},
    abstract = {Markov chain Monte Carlo (MCMC) methods and neural networks are instrumental in tackling inferential and prediction problems. However, Bayesian inference based on joint use of MCMC methods and of neural networks is limited. This paper reviews the main challenges posed by neural networks to MCMC developments, including lack of parameter identifiability due to weight symmetries, prior specification effects, and consequently high computational cost and convergence failure. Population and manifold MCMC algorithms are combined to demonstrate these challenges via multilayer perceptron (MLP) examples and to develop case studies for assessing the capacity of approximate inference methods to uncover the posterior covariance of neural network parameters. Some of these challenges, such as high computational cost arising from the application of neural networks to big data and parameter identifiability arising from weight symmetries, stimulate research towards more scalable approximate MCMC methods or towards MCMC methods in reduced parameter spaces.},
    opturldate = {2020-08-24},
    journal = {arXiv:1910.06539 [cs, stat]},
    author = {Papamarkou, Theodore and Hinkle, Jacob and Young, M. Todd and Womble, David},
    month = nov,
    year = {2019},
    note = {arXiv: 1910.06539},
    file = {arXiv.org Snapshot:/Users/suswei/Zotero/storage/6PA4A7WX/1910.html:text/html;arXiv Fulltext PDF:/Users/suswei/Zotero/storage/QQ2AA4QU/Papamarkou et al. - 2019 - Challenges in Bayesian inference via Markov chain .pdf:application/pdf}
}

@article{moore_symmetrized_nodate,
    title = {Symmetrized {Variational} {Inference}},
    abstract = {We introduce a framework for modeling parameter symmetries in variational inference by explicitly mixing a base approximating density over a symmetry group. We show that this can be done tractably for the case of a Gaussian mixture over the orthogonal group under an isotropic variance assumption. Initial results show that inference with a symmetrized posterior avoids component collapse and leads to improved predictive performance.},
    language = {en},
    author = {Moore, David A},
    pages = {8},
    file = {Moore - Symmetrized Variational Inference.pdf:/Users/suswei/Zotero/storage/G7DR5YSL/Moore - Symmetrized Variational Inference.pdf:application/pdf}
}

@article{kristiadi_being_2020,
  title={Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks},
  author={Kristiadi, Agustinus and Hein, Matthias and Hennig, Philipp},
  journal={arXiv preprint arXiv:2002.10118},
  year={2020}
}

@article{snoek_scalable_2015,
    title = {Scalable {Bayesian} {Optimization} {Using} {Deep} {Neural} {Networks}},
    opturl = {http://arxiv.org/abs/1502.05700},
    abstract = {Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.},
    opturldate = {2020-08-27},
    journal = {arXiv:1502.05700 [stat]},
    author = {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Md Mostofa Ali and Prabhat and Adams, Ryan P.},
    month = jul,
    year = {2015},
    note = {arXiv: 1502.05700},
    file = {Snoek et al_2015_Scalable Bayesian Optimization Using Deep Neural Networks.pdf:/Users/suswei/Zotero/storage/YSQ34Z7Z/Snoek et al_2015_Scalable Bayesian Optimization Using Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/suswei/Zotero/storage/VFVSNH8P/1502.html:text/html}
}

@inproceedings{watanabe_almost_2007,
    title = {Almost {All} {Learning} {Machines} are {Singular}},
    optdoi = {10.1109/FOCI.2007.371500},
    abstract = {A learning machine is called singular if its Fisher information matrix is singular. Almost all learning machines used in information processing are singular, for example, layered neural networks, normal mixtures, binomial mixtures, Bayes networks, hidden Markov models, Boltzmann machines, stochastic context-free grammars, and reduced rank regressions are singular. In singular learning machines, the likelihood function can not be approximated by any quadratic form of the parameter. Moreover, neither the distribution of the maximum likelihood estimator nor the Bayes a posteriori distribution converges to the normal distribution, even if the number of training samples tends to infinity. Therefore, the conventional statistical learning theory does not hold in singular learning machines. This paper establishes the new mathematical foundation for singular learning machines. We propose that, by using resolution of singularities, the likelihood function can be represented as the standard form, by which we can prove the asymptotic behavior of the generalization errors of the maximum likelihood method and the Bayes estimation. The result will be a base on which training algorithms of singular learning machines are devised and optimized},
    booktitle = {2007 {IEEE} {Symposium} on {Foundations} of {Computational} {Intelligence}},
    author = {Watanabe, Sumio},
    month = apr,
    year = {2007},
    pages = {383--388},
    file = {IEEE Xplore Abstract Record:/Users/suswei/Zotero/storage/336TFJ3R/4233934.html:text/html}
}


@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{watanabe_widely_2013,
    title = {A {Widely} {Applicable} {Bayesian} {Information} {Criterion}},
    volume = {14},
    optissn = {optissn 1533-7928},
    opturl = {http://www.jmlr.org/papers/v14/watanabe13a.html},
    number = {Mar},
    opturldate = {2020-02-24},
    journal = {Journal of Machine Learning Research},
    author = {Watanabe, Sumio},
    year = {2013},
    keywords = {RLCT, information criteria, marginal likelihood, singular model, thermodynamic integration},
    pages = {867--897}
}

@book{kollar_birational_1998,
    series = {Cambridge {Tracts} in {Mathematics}},
    title = {Birational {Geometry} of {Algebraic} {Varieties}},
    publisher = {Cambridge University Press},
    author = {Kollár, Janos and Mori, Shigefumi},
    year = {1998},
    optdoi = {10.1017/CBO9780511662560}
}


@article{chaudhari2019entropy,
  title="{Entropy-SGD}: Biasing gradient descent into wide valleys",
  author={P.~Chaudhari and A.~Choromanska and S.~Soatto and Y.~LeCun and C.~Baldassi and C.~Borgs and J.~Chayes and L.~Sagun and R.~Zecchina},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  year={2019}
}

@inproceedings{hinton1993keeping,
  title={Keeping the neural networks simple by minimizing the description length of the weights},
  author={Hinton, Geoffrey E and Van Camp, Drew},
  booktitle={Proceedings of the sixth annual conference on Computational learning theory},
  pages={5--13},
  year={1993}
}

@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press}
}

@article{smith2017bayesian,
  title={A {B}ayesian perspective on generalization and stochastic gradient descent},
  author={Smith, Samuel L and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.06451},
  year={2017}
}


@article{jastrzkebski2017three,
  title="Three factors influencing minima in {SGD}",
  author={Jastrzebski, Stanislaw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1711.04623},
  year={2017}
}

@article{Balasubramanian:1996cond.mat..1030B,
       author = {{Balasubramanian}, Vijay},
        title = "Statistical Inference, {Occam's} Razor and Statistical Mechanics on The Space of Probability Distributions",
journal = {Neural Computation},
volume = {9},
number = {2},
pages = {349-368},
year = {1997},
optdoi = {10.1162/neco.1997.9.2.349},
opturl = {         https://optdoi.org/10.1162/neco.1997.9.2.349},
     keywords = {Condensed Matter, Bayesian Analysis, Nonlinear Sciences - Adaptation and Self-Organizing Systems},
archivePrefix = {arXiv},
 primaryClass = {cond-mat},
       adsopturl = {https://ui.adsabs.harvard.edu/abs/1996cond.mat..1030B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{
phuong2020functional,
title={Functional vs. parametric equivalence of {R}e{L}{U} networks},
author={Mary Phuong and Christoph H. Lampert},
booktitle={International Conference on Learning Representations},
year={2020},
opturl={https://openreview.net/forum?id=Bylx-TNKvH}
}

@article{hendrycks2016gaussian,
  title={Gaussian error linear units ({G}{E}{L}{U}s)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@article{ramachandran2017swish,
  title={Swish: a self-gated activation function},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  year={2017}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hestness_2017,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md and Ali, Mostofa and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@book{boothby1986introduction,
  title={An introduction to differentiable manifolds and {R}iemannian geometry},
  author={Boothby, William M},
  year={1986},
  publisher={Academic press}
}