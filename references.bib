
@article{aoyagi_learning_2012,
	title = {Learning {Coefficient} of {Generalization} {Error} in {Bayesian} {Estimation} and {Vandermonde} {Matrix}-{Type} {Singularity}},
	volume = {24},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/NECO_a_00271},
	doi = {10.1162/NECO_a_00271},
	language = {en},
	number = {6},
	urldate = {2020-04-10},
	journal = {Neural Computation},
	author = {Aoyagi, Miki and Nagata, Kenji},
	month = jun,
	year = {2012},
	pages = {1569--1610}
}

@article{avron_randomized_2011,
	title = {Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix},
	volume = {58},
	doi = {10.1145/1944345.1944349},
	number = {2},
	journal = {Journal of the ACM},
	author = {Avron, Haim and Toledo, Sivan},
	year = {2011},
	pages = {1--34}
}

@book{kollar_birational_1998,
	series = {Cambridge {Tracts} in {Mathematics}},
	title = {Birational {Geometry} of {Algebraic} {Varieties}},
	publisher = {Cambridge University Press},
	author = {Kollár, Janos and Mori, Shigefumi},
	year = {1998},
	doi = {10.1017/CBO9780511662560}
}

@article{aoyagi_stochastic_2005,
	title = {Stochastic complexities of reduced rank regression in {Bayesian} estimation},
	volume = {18},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608005000559},
	doi = {10.1016/j.neunet.2005.03.014},
	abstract = {Reduced rank regression extracts an essential information from examples of input–output pairs. It is understood as a three-layer neural network with linear hidden units. However, reduced rank approximation is a non-regular statistical model which has a degenerate Fisher information matrix. Its generalization error had been left unknown even in statistics. In this paper, we give the exact asymptotic form of its generalization error in Bayesian estimation, based on resolution of learning machine singularities. For this purpose, the maximum pole of the zeta function for the learning theory is calculated. We propose a new method of recursive blowing-ups which yields the complete desingularization of the reduced rank approximation.},
	language = {en},
	number = {7},
	urldate = {2020-03-27},
	journal = {Neural Networks},
	author = {Aoyagi, Miki and Watanabe, Sumio},
	month = sep,
	year = {2005},
	keywords = {RLCT, RLCT concrete},
	pages = {924--933}
}

@article{aoyagi_resolution_2006,
	title = {Resolution of {Singularities} and the {Generalization} {Error} with {Bayesian} {Estimation} for {Layered} {Neural} {Network}},
	abstract = {Hierarchical learning models such as layered neural networks have singular Fisher metrics, since their parameters are not identiﬁable. These are called non-regular learning models. The stochastic complexities of non-regular learning models in Bayesian estimation are asymptotically obtained by using poles of their zeta functions which are the integrals of their Kullback distances and their priori probability density functions [1, 2, 3]. However, for several examples, upper bounds of the main terms in asymptotic forms of the stochastic complexities were obtained but not the exact values, because of their computational complexities. In this paper, we show a computational way for obtaining the exact value of the layered neural network and we give the asymptotic form of its stochastic complexity explicitly.},
	language = {en},
	author = {Aoyagi, Miki and Watanabe, Sumio},
	year = {2006},
	keywords = {RLCT, RLCT concrete},
	pages = {34}
}

@article{aoyagi_zeta_2016,
	title = {Zeta function of learning theory and generalization error of three layered neural perceptron},
	abstract = {Recently, the purpose of obtaining the maximum poles of certain zeta functions arises in the learning theory when one is looking for the generalization errors of hierarchical learning models asymptotically . \$[3,4]\$ The zeta function of a learning model is deﬁned by the integral of its Kullback function and its a priom probability density function. Today, for several learning models, upper bounds of the main terms in their asymptotic forms were calculated, but not the exact values, so far. In this paper, we obtain the explicit value of the main term for a three layered neural network, which is one of hierarchical learning models.},
	language = {en},
	author = {Aoyagi, Miki},
	year = {2016},
	keywords = {RLCT, RLCT concrete},
	pages = {16}
}

@article{mescheder_adversarial_2018,
	title = {Adversarial {Variational} {Bayes}: {Unifying} {Variational} {Autoencoders} and {Generative} {Adversarial} {Networks}},
	shorttitle = {Adversarial {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1701.04722},
	abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
	urldate = {2020-03-30},
	journal = {arXiv:1701.04722 [cs]},
	author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
	month = jun,
	year = {2018},
	note = {arXiv: 1701.04722},
	keywords = {implicit VI}
}

@article{gholami_anode_2019,
	title = {{ANODE}: {Unconditionally} {Accurate} {Memory}-{Efficient} {Gradients} for {Neural} {ODEs}},
	shorttitle = {{ANODE}},
	url = {http://arxiv.org/abs/1902.10298},
	abstract = {Residual neural networks can be viewed as the forward Euler discretization of an Ordinary Differential Equation (ODE) with a unit time step. This has recently motivated researchers to explore other discretization approaches and train ODE based networks. However, an important challenge of neural ODEs is their prohibitive memory cost during gradient backpropogation. Recently a method proposed in [8], claimed that this memory overhead can be reduced from O(LN\_t), where N\_t is the number of time steps, down to O(L) by solving forward ODE backwards in time, where L is the depth of the network. However, we will show that this approach may lead to several problems: (i) it may be numerically unstable for ReLU/non-ReLU activations and general convolution operators, and (ii) the proposed optimize-then-discretize approach may lead to divergent training due to inconsistent gradients for small time step sizes. We discuss the underlying problems, and to address them we propose ANODE, an Adjoint based Neural ODE framework which avoids the numerical instability related problems noted above, and provides unconditionally accurate gradients. ANODE has a memory footprint of O(L) + O(N\_t), with the same computational cost as reversing ODE solve. We furthermore, discuss a memory efficient algorithm which can further reduce this footprint with a trade-off of additional computational cost. We show results on Cifar-10/100 datasets using ResNet and SqueezeNext neural networks.},
	urldate = {2020-03-24},
	journal = {arXiv:1902.10298 [cs]},
	author = {Gholami, Amir and Keutzer, Kurt and Biros, George},
	month = jul,
	year = {2019},
	note = {arXiv: 1902.10298},
	keywords = {\_tablet}
}

@inproceedings{gao_degrees_2016,
	title = {Degrees of {Freedom} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1603.09260},
	abstract = {In this paper, we explore degrees of freedom in deep sigmoidal neural networks. We show that the degrees of freedom in these models is related to the expected optimism, which is the expected difference between test error and training error. We provide an efficient Monte-Carlo method to estimate the degrees of freedom for multi-class classification methods. We show degrees of freedom are lower than the parameter count in a simple XOR network. We extend these results to neural nets trained on synthetic and real data, and investigate impact of network's architecture and different regularization choices. The degrees of freedom in deep networks are dramatically smaller than the number of parameters, in some real datasets several orders of magnitude. Further, we observe that for fixed number of parameters, deeper networks have less degrees of freedom exhibiting a regularization-by-depth.},
	urldate = {2020-03-03},
	booktitle = {Proceedings of the {Thirty}-{Second} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	author = {Gao, Tianxiang and Jojic, Vladimir},
	month = jun,
	year = {2016},
	note = {arXiv: 1603.09260},
	keywords = {BIC, deep learning, degrees of freedom},
	pages = {232--241}
}

@article{champion_data-driven_2019,
	title = {Data-driven discovery of coordinates and governing equations},
	volume = {116},
	copyright = {Copyright © 2019 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by/4.0/This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/116/45/22445},
	doi = {10.1073/pnas.1906995116},
	abstract = {The discovery of governing equations from scientific data has the potential to transform data-rich fields that lack well-characterized quantitative descriptions. Advances in sparse regression are currently enabling the tractable identification of both the structure and parameters of a nonlinear dynamical system from data. The resulting models have the fewest terms necessary to describe the dynamics, balancing model complexity with descriptive ability, and thus promoting interpretability and generalizability. This provides an algorithmic approach to Occam’s razor for model discovery. However, this approach fundamentally relies on an effective coordinate system in which the dynamics have a simple representation. In this work, we design a custom deep autoencoder network to discover a coordinate transformation into a reduced space where the dynamics may be sparsely represented. Thus, we simultaneously learn the governing equations and the associated coordinate system. We demonstrate this approach on several example high-dimensional systems with low-dimensional behavior. The resulting modeling framework combines the strengths of deep neural networks for flexible representation and sparse identification of nonlinear dynamics (SINDy) for parsimonious models. This method places the discovery of coordinates and models on an equal footing.},
	language = {en},
	number = {45},
	urldate = {2020-03-24},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Champion, Kathleen and Lusch, Bethany and Kutz, J. Nathan and Brunton, Steven L.},
	month = nov,
	year = {2019},
	pmid = {31636218},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	pages = {22445--22451}
}

@article{yang_adversarial_2019,
	title = {Adversarial {Uncertainty} {Quantification} in {Physics}-{Informed} {Neural} {Networks}},
	volume = {394},
	issn = {00219991},
	url = {http://arxiv.org/abs/1811.04026},
	doi = {10.1016/j.jcp.2019.05.027},
	abstract = {We present a deep learning framework for quantifying and propagating uncertainty in systems governed by non-linear differential equations using physics-informed neural networks. Specifically, we employ latent variable models to construct probabilistic representations for the system states, and put forth an adversarial inference procedure for training them on data, while constraining their predictions to satisfy given physical laws expressed by partial differential equations. Such physics-informed constraints provide a regularization mechanism for effectively training deep generative models as surrogates of physical systems in which the cost of data acquisition is high, and training data-sets are typically small. This provides a flexible framework for characterizing uncertainty in the outputs of physical systems due to randomness in their inputs or noise in their observations that entirely bypasses the need for repeatedly sampling expensive experiments or numerical simulators. We demonstrate the effectiveness of our approach through a series of examples involving uncertainty propagation in non-linear conservation laws, and the discovery of constitutive laws for flow through porous media directly from noisy data.},
	urldate = {2020-03-24},
	journal = {Journal of Computational Physics},
	author = {Yang, Yibo and Perdikaris, Paris},
	month = oct,
	year = {2019},
	note = {arXiv: 1811.04026},
	pages = {136--152}
}

@article{geng_coercing_2020,
	title = {Coercing {Machine} {Learning} to {Output} {Physically} {Accurate} {Results}},
	volume = {406},
	issn = {00219991},
	url = {http://arxiv.org/abs/1910.09671},
	doi = {10.1016/j.jcp.2019.109099},
	abstract = {Many machine/deep learning artificial neural networks are trained to simply be interpolation functions that map input variables to output values interpolated from the training data in a linear/nonlinear fashion. Even when the input/output pairs of the training data are physically accurate (e.g. the results of an experiment or numerical simulation), interpolated quantities can deviate quite far from being physically accurate. Although one could project the output of a network into a physically feasible region, such a postprocess is not captured by the energy function minimized when training the network; thus, the final projected result could incorrectly deviate quite far from the training data. We propose folding any such projection or postprocess directly into the network so that the final result is correctly compared to the training data by the energy function. Although we propose a general approach, we illustrate its efficacy on a specific convolutional neural network that takes in human pose parameters (joint rotations) and outputs a prediction of vertex positions representing a triangulated cloth mesh. While the original network outputs vertex positions with erroneously high stretching and compression energies, the new network trained with our physics prior remedies these issues producing highly improved results.},
	urldate = {2020-03-24},
	journal = {Journal of Computational Physics},
	author = {Geng, Zhenglin and Johnson, Dan and Fedkiw, Ronald},
	month = apr,
	year = {2020},
	note = {arXiv: 1910.09671},
	pages = {109099}
}

@article{toth_hamiltonian_2020,
	title = {Hamiltonian {Generative} {Networks}},
	url = {http://arxiv.org/abs/1909.13789},
	abstract = {The Hamiltonian formalism plays a central role in classical and quantum physics. Hamiltonians are the main tool for modelling the continuous time evolution of systems with conserved quantities, and they come equipped with many useful properties, like time reversibility and smooth interpolation in time. These properties are important for many machine learning problems - from sequence prediction to reinforcement learning and density modelling - but are not typically provided out of the box by standard tools such as recurrent neural networks. In this paper, we introduce the Hamiltonian Generative Network (HGN), the first approach capable of consistently learning Hamiltonian dynamics from high-dimensional observations (such as images) without restrictive domain assumptions. Once trained, we can use HGN to sample new trajectories, perform rollouts both forward and backward in time and even speed up or slow down the learned dynamics. We demonstrate how a simple modification of the network architecture turns HGN into a powerful normalising flow model, called Neural Hamiltonian Flow (NHF), that uses Hamiltonian dynamics to model expressive densities. We hope that our work serves as a first practical demonstration of the value that the Hamiltonian formalism can bring to deep learning.},
	urldate = {2020-03-24},
	journal = {arXiv:1909.13789 [cs, stat]},
	author = {Toth, Peter and Rezende, Danilo Jimenez and Jaegle, Andrew and Racanière, Sébastien and Botev, Aleksandar and Higgins, Irina},
	month = feb,
	year = {2020},
	note = {arXiv: 1909.13789}
}

@article{behrmann_invertible_2019,
	title = {Invertible {Residual} {Networks}},
	url = {http://arxiv.org/abs/1811.00995},
	abstract = {We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible ResNets perform competitively with both state-of-the-art image classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.},
	urldate = {2020-03-24},
	journal = {arXiv:1811.00995 [cs, stat]},
	author = {Behrmann, Jens and Grathwohl, Will and Chen, Ricky T. Q. and Duvenaud, David and Jacobsen, Jörn-Henrik},
	month = may,
	year = {2019},
	note = {arXiv: 1811.00995}
}

@article{regazzoni_machine_2019,
	title = {Machine learning for fast and reliable solution of time-dependent differential equations},
	volume = {397},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999119305364},
	doi = {10.1016/j.jcp.2019.07.050},
	abstract = {We propose a data-driven Model Order Reduction (MOR) technique, based on Artiﬁcial Neural Networks (ANNs), applicable to dynamical systems arising from Ordinary Diﬀerential Equations (ODEs) or time-dependent Partial Diﬀerential Equations (PDEs). Unlike modelbased approaches, the proposed approach is non-intrusive since it just requires a collection of input-output pairs generated through the high-ﬁdelity (HF) ODE or PDE model. We formulate our model reduction problem as a maximum-likelihood problem, in which we look for the model that minimizes, in a class of candidate models, the error on the available inputoutput pairs. Speciﬁcally, we represent candidate models by means of ANNs, which we train to learn the dynamics of the HF model from the training input-output data. We prove that ANN models are able to approximate every time-dependent model described by ODEs with any desired level of accuracy. We test the proposed technique on diﬀerent problems, including the model reduction of two large-scale models. Two of the HF systems of ODEs here considered stem from the spatial discretization of a parabolic and an hyperbolic PDE respectively, which sheds light on a promising ﬁeld of application of the proposed technique.},
	language = {en},
	urldate = {2020-03-24},
	journal = {Journal of Computational Physics},
	author = {Regazzoni, F. and Dedè, L. and Quarteroni, A.},
	month = nov,
	year = {2019},
	pages = {108852}
}

@article{zhu_physics-constrained_2019,
	title = {Physics-{Constrained} {Deep} {Learning} for {High}-dimensional {Surrogate} {Modeling} and {Uncertainty} {Quantification} without {Labeled} {Data}},
	volume = {394},
	issn = {00219991},
	url = {http://arxiv.org/abs/1901.06314},
	doi = {10.1016/j.jcp.2019.05.024},
	abstract = {Surrogate modeling and uncertainty quantification tasks for PDE systems are most often considered as supervised learning problems where input and output data pairs are used for training. The construction of such emulators is by definition a small data problem which poses challenges to deep learning approaches that have been developed to operate in the big data regime. Even in cases where such models have been shown to have good predictive capability in high dimensions, they fail to address constraints in the data implied by the PDE model. This paper provides a methodology that incorporates the governing equations of the physical model in the loss/likelihood functions. The resulting physics-constrained, deep learning models are trained without any labeled data (e.g. employing only input data) and provide comparable predictive responses with data-driven models while obeying the constraints of the problem at hand. This work employs a convolutional encoder-decoder neural network approach as well as a conditional flow-based generative model for the solution of PDEs, surrogate model construction, and uncertainty quantification tasks. The methodology is posed as a minimization problem of the reverse Kullback-Leibler (KL) divergence between the model predictive density and the reference conditional density, where the later is defined as the Boltzmann-Gibbs distribution at a given inverse temperature with the underlying potential relating to the PDE system of interest. The generalization capability of these models to out-of-distribution input is considered. Quantification and interpretation of the predictive uncertainty is provided for a number of problems.},
	urldate = {2020-03-24},
	journal = {Journal of Computational Physics},
	author = {Zhu, Yinhao and Zabaras, Nicholas and Koutsourelakis, Phaedon-Stelios and Perdikaris, Paris},
	month = oct,
	year = {2019},
	note = {arXiv: 1901.06314},
	pages = {56--81}
}

@article{yang_pointflow_2019,
	title = {{PointFlow}: {3D} {Point} {Cloud} {Generation} with {Continuous} {Normalizing} {Flows}},
	shorttitle = {{PointFlow}},
	url = {http://arxiv.org/abs/1906.12320},
	abstract = {As 3D point clouds become the representation of choice for multiple vision and graphics applications, the ability to synthesize or reconstruct high-resolution, high-fidelity point clouds becomes crucial. Despite the recent success of deep learning models in discriminative tasks of point clouds, generating point clouds remains challenging. This paper proposes a principled probabilistic framework to generate 3D point clouds by modeling them as a distribution of distributions. Specifically, we learn a two-level hierarchy of distributions where the first level is the distribution of shapes and the second level is the distribution of points given a shape. This formulation allows us to both sample shapes and sample an arbitrary number of points from a shape. Our generative model, named PointFlow, learns each level of the distribution with a continuous normalizing flow. The invertibility of normalizing flows enables the computation of the likelihood during training and allows us to train our model in the variational inference framework. Empirically, we demonstrate that PointFlow achieves state-of-the-art performance in point cloud generation. We additionally show that our model can faithfully reconstruct point clouds and learn useful representations in an unsupervised manner. The code will be available at https://github.com/stevenygd/PointFlow.},
	urldate = {2020-03-24},
	journal = {arXiv:1906.12320 [cs]},
	author = {Yang, Guandao and Huang, Xun and Hao, Zekun and Liu, Ming-Yu and Belongie, Serge and Hariharan, Bharath},
	month = sep,
	year = {2019},
	note = {arXiv: 1906.12320}
}

@article{bai_deep_2019,
	title = {Deep {Equilibrium} {Models}},
	url = {http://arxiv.org/abs/1909.01377},
	abstract = {We present a new approach to modeling sequential data: the deep equilibrium model (DEQ). Motivated by an observation that the hidden layers of many existing deep sequence models converge towards some fixed point, we propose the DEQ approach that directly finds these equilibrium points via root-finding. Such a method is equivalent to running an infinite depth (weight-tied) feedforward network, but has the notable advantage that we can analytically backpropagate through the equilibrium point using implicit differentiation. Using this approach, training and prediction in these networks require only constant memory, regardless of the effective "depth" of the network. We demonstrate how DEQs can be applied to two state-of-the-art deep sequence models: self-attention transformers and trellis networks. On large-scale language modeling tasks, such as the WikiText-103 benchmark, we show that DEQs 1) often improve performance over these state-of-the-art models (for similar parameter counts); 2) have similar computational requirements to existing models; and 3) vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88\% memory reduction in our experiments. The code is available at https://github.com/locuslab/deq .},
	urldate = {2020-03-24},
	journal = {arXiv:1909.01377 [cs, stat]},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	month = oct,
	year = {2019},
	note = {arXiv: 1909.01377}
}

@article{hoyer_neural_2019,
	title = {Neural reparameterization improves structural optimization},
	url = {http://arxiv.org/abs/1909.04240},
	abstract = {Structural optimization is a popular method for designing objects such as bridge trusses, airplane wings, and optical devices. Unfortunately, the quality of solutions depends heavily on how the problem is parameterized. In this paper, we propose using the implicit bias over functions induced by neural networks to improve the parameterization of structural optimization. Rather than directly optimizing densities on a grid, we instead optimize the parameters of a neural network which outputs those densities. This reparameterization leads to different and often better solutions. On a selection of 116 structural optimization tasks, our approach produces the best design 50\% more often than the best baseline method.},
	urldate = {2020-03-24},
	journal = {arXiv:1909.04240 [cs, stat]},
	author = {Hoyer, Stephan and Sohl-Dickstein, Jascha and Greydanus, Sam},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.04240}
}

@article{liu_neural_2019,
	title = {Neural {Similarity} {Learning}},
	url = {http://arxiv.org/abs/1910.13003},
	abstract = {Inner product-based convolution has been the founding stone of convolutional neural networks (CNNs), enabling end-to-end learning of visual representation. By generalizing inner product with a bilinear matrix, we propose the neural similarity which serves as a learnable parametric similarity measure for CNNs. Neural similarity naturally generalizes the convolution and enhances flexibility. Further, we consider the neural similarity learning (NSL) in order to learn the neural similarity adaptively from training data. Specifically, we propose two different ways of learning the neural similarity: static NSL and dynamic NSL. Interestingly, dynamic neural similarity makes the CNN become a dynamic inference network. By regularizing the bilinear matrix, NSL can be viewed as learning the shape of kernel and the similarity measure simultaneously. We further justify the effectiveness of NSL with a theoretical viewpoint. Most importantly, NSL shows promising performance in visual recognition and few-shot learning, validating the superiority of NSL over the inner product-based convolution counterparts.},
	urldate = {2020-03-24},
	journal = {arXiv:1910.13003 [cs, stat]},
	author = {Liu, Weiyang and Liu, Zhen and Rehg, James M. and Song, Le},
	month = dec,
	year = {2019},
	note = {arXiv: 1910.13003}
}

@inproceedings{he_ode-inspired_2019,
	address = {Long Beach, CA, USA},
	title = {{ODE}-{Inspired} {Network} {Design} for {Single} {Image} {Super}-{Resolution}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953823/},
	doi = {10.1109/CVPR.2019.00183},
	abstract = {Single image super-resolution, as a high dimensional structured prediction problem, aims to characterize ﬁnegrain information given a low-resolution sample. Recent advances in convolutional neural networks are introduced into super-resolution and push forward progress in this ﬁeld. Current studies have achieved impressive performance by manually designing deep residual neural networks but overly relies on practical experience. In this paper, we propose to adopt an ordinary differential equation (ODE)-inspired design scheme for single image superresolution, which have brought us a new understanding of ResNet in classiﬁcation problems. Not only is it interpretable for super-resolution but it provides a reliable guideline on network designs. By casting the numerical schemes in ODE as blueprints, we derive two types of network structures: LF-block and RK-block, which correspond to the Leapfrog method and Runge-Kutta method in numerical ordinary differential equations. We evaluate our models on benchmark datasets, and the results show that our methods surpass the state-of-the-arts while keeping comparable parameters and operations.},
	language = {en},
	urldate = {2020-03-24},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Xiangyu and Mo, Zitao and Wang, Peisong and Liu, Yang and Yang, Mingyuan and Cheng, Jian},
	month = jun,
	year = {2019},
	pages = {1732--1741}
}

@article{chen_particle_2019,
	title = {Particle {Flow} {Bayes}' {Rule}},
	url = {http://arxiv.org/abs/1902.00640},
	abstract = {We present a particle flow realization of Bayes' rule, where an ODE-based neural operator is used to transport particles from a prior to its posterior after a new observation. We prove that such an ODE operator exists. Its neural parameterization can be trained in a meta-learning framework, allowing this operator to reason about the effect of an individual observation on the posterior, and thus generalize across different priors, observations and to sequential Bayesian inference. We demonstrated the generalization ability of our particle flow Bayes operator in several canonical and high dimensional examples.},
	urldate = {2020-03-24},
	journal = {arXiv:1902.00640 [cs, stat]},
	author = {Chen, Xinshi and Dai, Hanjun and Song, Le},
	month = dec,
	year = {2019},
	note = {arXiv: 1902.00640}
}

@article{han_solving_2018,
	title = {Solving high-dimensional partial differential equations using deep learning},
	volume = {115},
	copyright = {© 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/34/8505},
	doi = {10.1073/pnas.1718942115},
	abstract = {Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the “curse of dimensionality.” This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic PDEs. To this end, the PDEs are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black–Scholes equation, the Hamilton–Jacobi–Bellman equation, and the Allen–Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships.},
	language = {en},
	number = {34},
	urldate = {2020-03-24},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Han, Jiequn and Jentzen, Arnulf and E, Weinan},
	month = aug,
	year = {2018},
	pmid = {30082389},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	pages = {8505--8510}
}

@article{saemundsson_variational_2020,
	title = {Variational {Integrator} {Networks} for {Physically} {Structured} {Embeddings}},
	url = {http://arxiv.org/abs/1910.09349},
	abstract = {Learning workable representations of dynamical systems is becoming an increasingly important problem in a number of application areas. By leveraging recent work connecting deep neural networks to systems of differential equations, we propose {\textbackslash}emph\{variational integrator networks\}, a class of neural network architectures designed to preserve the geometric structure of physical systems. This class of network architectures facilitates accurate long-term prediction, interpretability, and data-efficient learning, while still remaining highly flexible and capable of modeling complex behavior. We demonstrate that they can accurately learn dynamical systems from both noisy observations in phase space and from image pixels within which the unknown dynamics are embedded.},
	urldate = {2020-03-24},
	journal = {arXiv:1910.09349 [cs, stat]},
	author = {Saemundsson, Steindor and Terenin, Alexander and Hofmann, Katja and Deisenroth, Marc Peter},
	month = mar,
	year = {2020},
	note = {arXiv: 1910.09349}
}

@article{grathwohl_ffjord_2018,
	title = {{FFJORD}: {Free}-form {Continuous} {Dynamics} for {Scalable} {Reversible} {Generative} {Models}},
	shorttitle = {{FFJORD}},
	url = {http://arxiv.org/abs/1810.01367},
	abstract = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.},
	urldate = {2020-03-24},
	journal = {arXiv:1810.01367 [cs, stat]},
	author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.01367}
}

@article{salman_deep_2018,
	title = {Deep {Diffeomorphic} {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/1810.03256},
	abstract = {The Normalizing Flow (NF) models a general probability density by estimating an invertible transformation applied on samples drawn from a known distribution. We introduce a new type of NF, called Deep Diffeomorphic Normalizing Flow (DDNF). A diffeomorphic flow is an invertible function where both the function and its inverse are smooth. We construct the flow using an ordinary differential equation (ODE) governed by a time-varying smooth vector field. We use a neural network to parametrize the smooth vector field and a recursive neural network (RNN) for approximating the solution of the ODE. Each cell in the RNN is a residual network implementing one Euler integration step. The architecture of our flow enables efficient likelihood evaluation, straightforward flow inversion, and results in highly flexible density estimation. An end-to-end trained DDNF achieves competitive results with state-of-the-art methods on a suite of density estimation and variational inference tasks. Finally, our method brings concepts from Riemannian geometry that, we believe, can open a new research direction for neural density estimation.},
	urldate = {2020-03-24},
	journal = {arXiv:1810.03256 [cs, stat]},
	author = {Salman, Hadi and Yadollahpour, Payman and Fletcher, Tom and Batmanghelich, Kayhan},
	month = nov,
	year = {2018},
	note = {arXiv: 1810.03256}
}

@article{zhang_anodev2_2019,
	title = {{ANODEV2}: {A} {Coupled} {Neural} {ODE} {Evolution} {Framework}},
	shorttitle = {{ANODEV2}},
	url = {http://arxiv.org/abs/1906.04596},
	abstract = {It has been observed that residual networks can be viewed as the explicit Euler discretization of an Ordinary Differential Equation (ODE). This observation motivated the introduction of so-called Neural ODEs, which allow more general discretization schemes with adaptive time stepping. Here, we propose ANODEV2, which is an extension of this approach that also allows evolution of the neural network parameters, in a coupled ODE-based formulation. The Neural ODE method introduced earlier is in fact a special case of this new more general framework. We present the formulation of ANODEV2, derive optimality conditions, and implement a coupled reaction-diffusion-advection version of this framework in PyTorch. We present empirical results using several different configurations of ANODEV2, testing them on multiple models on CIFAR-10. We report results showing that this coupled ODE-based framework is indeed trainable, and that it achieves higher accuracy, as compared to the baseline models as well as the recently-proposed Neural ODE approach.},
	urldate = {2020-03-24},
	journal = {arXiv:1906.04596 [cs, stat]},
	author = {Zhang, Tianjun and Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Gonzalez, Joseph and Biros, George and Mahoney, Michael},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.04596}
}

@article{e_mean-field_2018,
	title = {A {Mean}-{Field} {Optimal} {Control} {Formulation} of {Deep} {Learning}},
	url = {http://arxiv.org/abs/1807.01083},
	abstract = {Recent work linking deep neural networks and dynamical systems opened up new avenues to analyze deep learning. In particular, it is observed that new insights can be obtained by recasting deep learning as an optimal control problem on difference or differential equations. However, the mathematical aspects of such a formulation have not been systematically explored. This paper introduces the mathematical formulation of the population risk minimization problem in deep learning as a mean-field optimal control problem. Mirroring the development of classical optimal control, we state and prove optimality conditions of both the Hamilton-Jacobi-Bellman type and the Pontryagin type. These mean-field results reflect the probabilistic nature of the learning problem. In addition, by appealing to the mean-field Pontryagin's maximum principle, we establish some quantitative relationships between population and empirical learning problems. This serves to establish a mathematical foundation for investigating the algorithmic and theoretical connections between optimal control and deep learning.},
	urldate = {2020-03-24},
	journal = {arXiv:1807.01083 [cs, math]},
	author = {E, Weinan and Han, Jiequn and Li, Qianxiao},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.01083}
}

@article{garbuno-inigo_interacting_2019,
	title = {Interacting {Langevin} {Diffusions}: {Gradient} {Structure} {And} {Ensemble} {Kalman} {Sampler}},
	shorttitle = {Interacting {Langevin} {Diffusions}},
	url = {http://arxiv.org/abs/1903.08866},
	abstract = {Solving inverse problems without the use of derivatives or adjoints of the forward model is highly desirable in many applications arising in science and engineering. In this paper, we propose a new version of such a methodology, a framework for its analysis, and numerical evidence of the practicality of the method proposed. Our starting point is an ensemble of over-damped Langevin diffusions which interact through a single preconditioner computed as the empirical ensemble covariance. We demonstrate that the nonlinear Fokker-Planck equation arising from the mean-field limit of the associated stochastic differential equation (SDE) has a novel gradient flow structure, built on the Wasserstein metric and the covariance matrix of the noisy flow. Using this structure, we investigate large time properties of the Fokker-Planck equation, showing that its invariant measure coincides with that of a single Langevin diffusion, and demonstrating exponential convergence to the invariant measure in a number of settings. We introduce a new noisy variant on ensemble Kalman inversion (EKI) algorithms found from the original SDE by replacing exact gradients with ensemble differences; this defines the ensemble Kalman sampler (EKS). Numerical results are presented which demonstrate its efficacy as a derivative-free approximate sampler for the Bayesian posterior arising from inverse problems.},
	urldate = {2020-03-24},
	journal = {arXiv:1903.08866 [math]},
	author = {Garbuno-Inigo, Alfredo and Hoffmann, Franca and Li, Wuchen and Stuart, Andrew M.},
	month = oct,
	year = {2019},
	note = {arXiv: 1903.08866}
}

@article{he_identity_2016,
	title = {Identity {Mappings} in {Deep} {Residual} {Networks}},
	url = {http://arxiv.org/abs/1603.05027},
	abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
	urldate = {2020-03-24},
	journal = {arXiv:1603.05027 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jul,
	year = {2016},
	note = {arXiv: 1603.05027}
}

@article{veit_residual_2016,
	title = {Residual {Networks} {Behave} {Like} {Ensembles} of {Relatively} {Shallow} {Networks}},
	url = {http://arxiv.org/abs/1605.06431},
	abstract = {In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
	urldate = {2020-03-24},
	journal = {arXiv:1605.06431 [cs]},
	author = {Veit, Andreas and Wilber, Michael and Belongie, Serge},
	month = oct,
	year = {2016},
	note = {arXiv: 1605.06431}
}

@article{hardt_identity_2018,
	title = {Identity {Matters} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1611.04231},
	abstract = {An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as {\textbackslash}emph\{batch normalization\}, but was also key to the immense success of {\textbackslash}emph\{residual networks\}. In this work, we put the principle of {\textbackslash}emph\{identity parameterization\} on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for linear feed-forward networks in their standard parameterization is substantially more delicate. Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size. Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.},
	urldate = {2020-03-24},
	journal = {arXiv:1611.04231 [cs, stat]},
	author = {Hardt, Moritz and Ma, Tengyu},
	month = jul,
	year = {2018},
	note = {arXiv: 1611.04231}
}

@article{greff_highway_2017,
	title = {Highway and {Residual} {Networks} learn {Unrolled} {Iterative} {Estimation}},
	url = {http://arxiv.org/abs/1612.07771},
	abstract = {The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer. In this report, we argue that this view is incomplete and does not adequately explain several recent findings. We propose an alternative viewpoint based on unrolled iterative estimation -- a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. We demonstrate that this viewpoint directly leads to the construction of Highway and Residual networks. Finally we provide preliminary experiments to discuss the similarities and differences between the two architectures.},
	urldate = {2020-03-24},
	journal = {arXiv:1612.07771 [cs]},
	author = {Greff, Klaus and Srivastava, Rupesh K. and Schmidhuber, Jürgen},
	month = mar,
	year = {2017},
	note = {arXiv: 1612.07771}
}

@article{haber_stable_2018,
	title = {Stable {Architectures} for {Deep} {Neural} {Networks}},
	volume = {34},
	issn = {0266-5611, 1361-6420},
	url = {http://arxiv.org/abs/1705.03341},
	doi = {10.1088/1361-6420/aa9a90},
	abstract = {Deep neural networks have become invaluable tools for supervised machine learning, e.g., classification of text or images. While often offering superior results over traditional techniques and successfully expressing complicated patterns in data, deep architectures are known to be challenging to design and train such that they generalize well to new data. Important issues with deep architectures are numerical instabilities in derivative-based learning algorithms commonly called exploding or vanishing gradients. In this paper we propose new forward propagation techniques inspired by systems of Ordinary Differential Equations (ODE) that overcome this challenge and lead to well-posed learning problems for arbitrarily deep networks. The backbone of our approach is our interpretation of deep learning as a parameter estimation problem of nonlinear dynamical systems. Given this formulation, we analyze stability and well-posedness of deep learning and use this new understanding to develop new network architectures. We relate the exploding and vanishing gradient phenomenon to the stability of the discrete ODE and present several strategies for stabilizing deep learning for very deep networks. While our new architectures restrict the solution space, several numerical experiments show their competitiveness with state-of-the-art networks.},
	number = {1},
	urldate = {2020-03-24},
	journal = {Inverse Problems},
	author = {Haber, Eldad and Ruthotto, Lars},
	month = jan,
	year = {2018},
	note = {arXiv: 1705.03341},
	pages = {014004}
}

@article{sun_neupde_2019,
	title = {{NeuPDE}: {Neural} {Network} {Based} {Ordinary} and {Partial} {Differential} {Equations} for {Modeling} {Time}-{Dependent} {Data}},
	shorttitle = {{NeuPDE}},
	url = {http://arxiv.org/abs/1908.03190},
	abstract = {We propose a neural network based approach for extracting models from dynamic data using ordinary and partial differential equations. In particular, given a time-series or spatio-temporal dataset, we seek to identify an accurate governing system which respects the intrinsic differential structure. The unknown governing model is parameterized by using both (shallow) multilayer perceptrons and nonlinear differential terms, in order to incorporate relevant correlations between spatio-temporal samples. We demonstrate the approach on several examples where the data is sampled from various dynamical systems and give a comparison to recurrent networks and other data-discovery methods. In addition, we show that for MNIST and Fashion MNIST, our approach lowers the parameter cost as compared to other deep neural networks.},
	urldate = {2020-03-24},
	journal = {arXiv:1908.03190 [cs, stat]},
	author = {Sun, Yifan and Zhang, Linan and Schaeffer, Hayden},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.03190}
}

@article{chen_neural_2019,
	title = {Neural {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	urldate = {2020-03-23},
	journal = {arXiv:1806.07366 [cs, stat]},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	month = dec,
	year = {2019},
	note = {arXiv: 1806.07366}
}

@article{liu_deep_2019,
	title = {Deep {Learning} {Theory} {Review}: {An} {Optimal} {Control} and {Dynamical} {Systems} {Perspective}},
	shorttitle = {Deep {Learning} {Theory} {Review}},
	url = {http://arxiv.org/abs/1908.10920},
	abstract = {Attempts from different disciplines to provide a fundamental understanding of deep learning have advanced rapidly in recent years, yet a uniﬁed framework remains relatively limited. In this article, we provide one possible way to align existing branches of deep learning theory through the lens of dynamical system and optimal control. By viewing deep neural networks as discrete-time nonlinear dynamical systems, we can analyze how information propagates through layers using mean ﬁeld theory. When optimization algorithms are further recast as controllers, the ultimate goal of training processes can be formulated as an optimal control problem. In addition, we can reveal convergence and generalization properties by studying the stochastic dynamics of optimization algorithms. This viewpoint features a wide range of theoretical study from information bottleneck to statistical physics. It also provides a principled way for hyper-parameter tuning when optimal control theory is introduced. Our framework ﬁts nicely with supervised learning and can be extended to other learning problems, such as Bayesian learning, adversarial training, and speciﬁc forms of meta learning, without efforts. The review aims to shed lights on the importance of dynamics and optimal control when developing deep learning theory.},
	language = {en},
	urldate = {2020-03-21},
	journal = {arXiv:1908.10920 [cs, eess, stat]},
	author = {Liu, Guan-Horng and Theodorou, Evangelos A.},
	month = sep,
	year = {2019},
	note = {arXiv: 1908.10920}
}

@article{maddox_rethinking_2020,
	title = {Rethinking {Parameter} {Counting} in {Deep} {Models}: {Effective} {Dimensionality} {Revisited}},
	shorttitle = {Rethinking {Parameter} {Counting} in {Deep} {Models}},
	url = {http://arxiv.org/abs/2003.02139},
	abstract = {Neural networks appear to have mysterious generalization properties when using parameter counting as a proxy for complexity. Indeed, neural networks often have many more parameters than there are data points, yet still provide good generalization performance. Moreover, when we measure generalization as a function of parameters, we see double descent behaviour, where the test error decreases, increases, and then again decreases. We show that many of these properties become understandable when viewed through the lens of effective dimensionality, which measures the dimensionality of the parameter space determined by the data. We relate effective dimensionality to posterior contraction in Bayesian deep learning, model selection, double descent, and functional diversity in loss surfaces, leading to a richer understanding of the interplay between parameters and functions in deep models.},
	urldate = {2020-03-05},
	journal = {arXiv:2003.02139 [cs, stat]},
	author = {Maddox, Wesley J. and Benton, Gregory and Wilson, Andrew Gordon},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.02139}
}
@article{daunizeau_variational_2018,
	title = {The variational {Laplace} approach to approximate {Bayesian} inference},
	url = {http://arxiv.org/abs/1703.02089},
	abstract = {Variational approaches to approximate Bayesian inference provide very efficient means of performing parameter estimation and model selection. Among these, so-called variational-Laplace or VL schemes rely on Gaussian approximations to posterior densities on model parameters. In this note, we review the main variants of VL approaches, that follow from considering nonlinear models of continuous and/or categorical data. En passant, we also derive a few novel theoretical results that complete the portfolio of existing analyses of variational Bayesian approaches, including investigations of their asymptotic convergence. We also suggest practical ways of extending existing VL approaches to hierarchical generative models that include (e.g., precision) hyperparameters.},
	urldate = {2020-03-04},
	journal = {arXiv:1703.02089 [q-bio, stat]},
	author = {Daunizeau, Jean},
	month = jan,
	year = {2018},
	note = {arXiv: 1703.02089}
}

@inproceedings{le_bayesian_2018,
	title = {A {Bayesian} {Perspective} on {Generalization} and {Stochastic} {Gradient} {Descent}},
	url = {https://openreview.net/forum?id=BJij4yg0Z},
	abstract = {We consider two questions at the heart of machine learning; how can we predict if a minimum will generalize to the test set, and why does stochastic gradient descent find minima that generalize...},
	urldate = {2020-03-04},
	author = {Le, Samuel L. Smith {and} Quoc V.},
	month = feb,
	year = {2018}
}

@article{zhang_energyentropy_2018,
	title = {Energy–entropy competition and the effectiveness of stochastic gradient descent in machine learning},
	volume = {116},
	issn = {0026-8976},
	url = {https://doi.org/10.1080/00268976.2018.1483535},
	doi = {10.1080/00268976.2018.1483535},
	abstract = {Finding parameters that minimise a loss function is at the core of many machine learning methods. The Stochastic Gradient Descent (SGD) algorithm is widely used and delivers state-of-the-art results for many problems. Nonetheless, SGD typically cannot find the global minimum, thus its empirical effectiveness is hitherto mysterious. We derive a correspondence between parameter inference and free energy minimisation in statistical physics. The degree of undersampling plays the role of temperature. Analogous to the energy–entropy competition in statistical physics, wide but shallow minima can be optimal if the system is undersampled, as is typical in many applications. Moreover, we show that the stochasticity in the algorithm has a non-trivial correlation structure which systematically biases it towards wide minima. We illustrate our argument with two prototypical models: image classification using deep learning and a linear neural network where we can analytically reveal the relationship between entropy and out-of-sample error.},
	number = {21-22},
	urldate = {2020-03-04},
	journal = {Molecular Physics},
	author = {Zhang, Yao and Saxe, Andrew M. and Advani, Madhu S. and Lee, Alpha A.},
	month = nov,
	year = {2018},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00268976.2018.1483535},
	pages = {3214--3223}
}

@article{watanabe_alternative_2012,
	title = {An alternative view of variational {Bayes} and asymptotic approximations of free energy},
	volume = {86},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-011-5264-5},
	doi = {10.1007/s10994-011-5264-5},
	abstract = {Bayesian learning, widely used in many applied data-modeling problems, is often accomplished with approximation schemes because it requires intractable computation of the posterior distributions. In this study, we focus on two approximation methods, variational Bayes and local variational approximation. We show that the variational Bayes approach for statistical models with latent variables can be viewed as a special case of local variational approximation, where the log-sum-exp function is used to form the lower bound of the log-likelihood. The minimum variational free energy, the objective function of variational Bayes, is analyzed and related to the asymptotic theory of Bayesian learning. This analysis additionally implies a relationship between the generalization performance of the variational Bayes approach and the minimum variational free energy.},
	language = {en},
	number = {2},
	urldate = {2020-03-03},
	journal = {Machine Learning},
	author = {Watanabe, Kazuho},
	month = feb,
	year = {2012},
	keywords = {variational free energy, variational generalisation},
	pages = {273--293}
}

@article{watanabe_stochastic_2006,
	title = {Stochastic {Complexities} of {Gaussian} {Mixtures} in {Variational} {Bayesian} {Approximation}},
	volume = {7},
	issn = {1532-4435},
	abstract = {Bayesian learning has been widely used and proved to be effective in many data modeling problems. However, computations involved in it require huge costs and generally cannot be performed exactly. The variational Bayesian approach, proposed as an approximation of Bayesian learning, has provided computational tractability and good generalization performance in many applications. The properties and capabilities of variational Bayesian learning itself have not been clarified yet. It is still unknown how good approximation the variational Bayesian approach can achieve. In this paper, we discuss variational Bayesian learning of Gaussian mixture models and derive upper and lower bounds of variational stochastic complexities. The variational stochastic complexity, which corresponds to the minimum variational free energy and a lower bound of the Bayesian evidence, not only becomes important in addressing the model selection problem, but also enables us to discuss the accuracy of the variational Bayesian approach as an approximation of true Bayesian learning.},
	journal = {The Journal of Machine Learning Research},
	author = {Watanabe, Kazuho and Watanabe, Sumio},
	month = dec,
	year = {2006},
	keywords = {singular learning theory, variational free energy},
	pages = {625--644}
}

@inproceedings{kaji_optimal_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Optimal {Hyperparameters} for {Generalized} {Learning} and {Knowledge} {Discovery} in {Variational} {Bayes}},
	isbn = {978-3-642-10677-4},
	doi = {10.1007/978-3-642-10677-4_54},
	abstract = {Variational Bayes learning is widely used in statistical models that contain hidden variables, for example, normal mixtures, binomial mixtures, and hidden Markov models. To derive the variational Bayes learning algorithm, we need to determine the hyperparameters in the a priori distribution. In the present paper, we propose two different methods by which to optimize the hyperparameters for the two different purposes. In the first method, the hyperparameter is determined for minimization of the generalization error. In the second method, the hyperparameter is chosen so that the unknown hidden structure in the data can be discovered. Experiments are conducted to show that the optimal hyperparameters are different for the generalized learning and knowledge discovery.},
	language = {en},
	booktitle = {Neural {Information} {Processing}},
	publisher = {Springer},
	author = {Kaji, Daisuke and Watanabe, Sumio},
	editor = {Leung, Chi Sing and Lee, Minho and Chan, Jonathan H.},
	year = {2009},
	keywords = {variational free energy, variational generalisation},
	pages = {476--483}
}

@article{nakajima_variational_2007,
	title = {Variational {Bayes} {Solution} of {Linear} {Neural} {Networks} and {Its} {Generalization} {Performance}},
	volume = {19},
	url = {https://www.researchgate.net/publication/6457767_Variational_Bayes_Solution_of_Linear_Neural_Networks_and_Its_Generalization_Performance},
	number = {4},
	urldate = {2020-03-03},
	journal = {Neural Computation},
	author = {Nakajima, Shinichi and Watanabe, Sumio},
	year = {2007},
	keywords = {linear neural networks, singular learning theory, variational},
	pages = {1112--53}
}

@inproceedings{yamada_information_2012,
	title = {Information criterion for variational {Bayes} learning in regular and singular cases},
	doi = {10.1109/SCIS-ISIS.2012.6505025},
	abstract = {Variational Bayes learning gives the accurate statistical estimation as Bayes learning with smaller computational cost. However, it has been difficult to estimate its generalization loss, because learning machines used in variational Bayes are not regular but singular, resulting that the conventional information criteria such as AIC, BIC, or DIC can not be applied. In this paper, we propose a new information criterion for variational Bayes learning, which is the unbiased estimator of the generalization loss for both cases when the posterior distribution is regular and singular. We show the theoretical support of the proposed information criterion, and its effectiveness is illustrated by numerical experiments.},
	booktitle = {The 6th {International} {Conference} on {Soft} {Computing} and {Intelligent} {Systems}, and {The} 13th {International} {Symposium} on {Advanced} {Intelligence} {Systems}},
	author = {Yamada, Koshi and Watanabe, Sumio},
	month = nov,
	year = {2012},
	note = {ISSN: null},
	pages = {1551--1555}
}

@article{hayashi_variational_2020,
	title = {Variational {Approximation} {Error} in {Bayesian} {Non}-negative {Matrix} {Factorization}},
	url = {http://arxiv.org/abs/1809.02963},
	abstract = {Non-negative matrix factorization (NMF) is a knowledge discovery method that is used in many fields. Variational inference and Gibbs sampling methods for it are also wellknown. However, the variational approximation error has not been clarified yet, because NMF is not statistically regular and the prior distribution used in variational Bayesian NMF (VBNMF) has zero or divergence points. In this paper, using algebraic geometrical methods, we theoretically analyze the difference in negative log evidence (a.k.a. free energy) between VBNMF and Bayesian NMF, i.e., the Kullback-Leibler divergence between the variational posterior and the true posterior. We derive an upper bound for the learning coefficient (a.k.a. the real log canonical threshold) in Bayesian NMF. By using the upper bound, we find a lower bound for the approximation error, asymptotically. The result quantitatively shows how well the VBNMF algorithm can approximate Bayesian NMF; the lower bound depends on the hyperparameters and the true nonnegative rank. A numerical experiment demonstrates the theoretical result.},
	urldate = {2020-03-02},
	journal = {arXiv:1809.02963 [cs, math, stat]},
	author = {Hayashi, Naoki},
	month = feb,
	year = {2020},
	note = {arXiv: 1809.02963},
	keywords = {RLCT, singular learning theory, variational}
}

@article{gelman_understanding_2013,
	title = {Understanding predictive information criteria for {Bayesian} models},
	volume = {24},
	doi = {10.1007/s11222-013-9416-2},
	abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected
out-of-sample-prediction error using a biascorrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
	journal = {Statistics and Computing},
	author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
	month = jul,
	year = {2013},
	keywords = {AIC, DIC, WAIC, WBIC, information criteria}
}

@article{friel_investigation_2017,
	title = {Investigation of the widely applicable {Bayesian} information criterion},
	volume = {27},
	url = {http://arxiv.org/abs/1501.05447},
	abstract = {The widely applicable Bayesian information criterion (WBIC) is a simple and fast approximation to the model evidence that has received little practical consideration. WBIC uses the fact that the log evidence can be written as an expectation, with respect to a powered posterior proportional to the likelihood raised to a power \$t{\textasciicircum}*{\textbackslash}in\{(0,1)\}\$, of the log deviance. Finding this temperature value \$t{\textasciicircum}*\$ is generally an intractable problem. We find that for a particular tractable statistical model that the mean squared error of an optimally-tuned version of WBIC with correct temperature \$t{\textasciicircum}*\$ is lower than an optimally-tuned version of thermodynamic integration (power posteriors). However in practice WBIC uses the a canonical choice of \$t=1/{\textbackslash}log(n)\$. Here we investigate the performance of WBIC in practice, for a range of statistical models, both regular models and singular models such as latent variable models or those with a hierarchical structure for which BIC cannot provide an adequate solution. Our findings are that, generally WBIC performs adequately when one uses informative priors, but it can systematically overestimate the evidence, particularly for small sample sizes.},
	urldate = {2020-02-24},
	journal = {Statistics and Computing},
	author = {Friel, N. and McKeone, J. P. and Oates, C. J. and Pettitt, A. N.},
	year = {2017},
	note = {arXiv: 1501.05447},
	keywords = {WBIC, information criteria, marginal likelihood, singular model},
	pages = {833--844}
}

@article{drton_bayesian_2017,
	title = {A {Bayesian} information criterion for singular models},
	volume = {79},
	url = {http://arxiv.org/abs/1309.0911},
	abstract = {We consider approximate Bayesian model choice for model selection problems that involve models whose Fisher-information matrices may fail to be invertible along other competing submodels. Such singular models do not obey the regularity conditions underlying the derivation of Schwarz's Bayesian information criterion (BIC) and the penalty structure in BIC generally does not reflect the frequentist large-sample behavior of their marginal likelihood. While large-sample theory for the marginal likelihood of singular models has been developed recently, the resulting approximations depend on the true parameter value and lead to a paradox of circular reasoning. Guided by examples such as determining the number of components of mixture models, the number of factors in latent factor models or the rank in reduced-rank regression, we propose a resolution to this paradox and give a practical extension of BIC for singular model selection problems.},
	number = {2},
	urldate = {2020-02-24},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Drton, Mathias and Plummer, Martyn},
	year = {2017},
	note = {arXiv: 1309.0911},
	keywords = {RLCT, information criteria, marginal likelihood, singular model},
	pages = {323--380}
}

@inproceedings{dinh_sharp_2017,
	address = {Sydney, NSW, Australia},
	series = {{ICML}'17},
	title = {Sharp {Minima} {Can} {Generalize} {For} {Deep} {Nets}},
	volume = {70},
	url = {http://arxiv.org/abs/1703.04933},
	abstract = {Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter \& Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.},
	urldate = {2020-02-24},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
	month = may,
	year = {2017},
	note = {arXiv: 1703.04933},
	keywords = {deep learning, generalization, minima},
	pages = {1019--1028}
}

@inproceedings{zhang_understanding_2017,
	title = {Understanding deep learning requires rethinking generalization},
	url = {http://arxiv.org/abs/1611.03530},
	abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
	urldate = {2020-02-24},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Learning} {Representations}},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	month = feb,
	year = {2017},
	note = {arXiv: 1611.03530},
	keywords = {deep learning, generalization}
}

@book{watanabe_algebraic_2009,
	address = {USA},
	title = {Algebraic {Geometry} and {Statistical} {Learning} {Theory}},
	isbn = {978-0-521-86467-1},
	abstract = {Sure to be influential, Watanabe's book lays the foundations for the use of algebraic geometry in statistical learning theory. Many models/machines are singular: mixture models, neural networks, HMMs, Bayesian networks, stochastic context-free grammars are major examples. The theory achieved here underpins accurate estimation techniques in the presence of singularities.},
	publisher = {Cambridge University Press},
	author = {Watanabe, Sumio},
	year = {2009},
	keywords = {singular learning theory}
}

@article{watanabe_widely_2013,
	title = {A {Widely} {Applicable} {Bayesian} {Information} {Criterion}},
	volume = {14},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v14/watanabe13a.html},
	number = {Mar},
	urldate = {2020-02-24},
	journal = {Journal of Machine Learning Research},
	author = {Watanabe, Sumio},
	year = {2013},
	keywords = {RLCT, information criteria, marginal likelihood, singular model, thermodynamic integration},
	pages = {867--897}
}

@article{imai_estimating_2019,
	title = {Estimating {Real} {Log} {Canonical} {Thresholds}},
	url = {http://arxiv.org/abs/1906.01341},
	abstract = {Evaluation of the marginal likelihood plays an important role in model selection problems. The widely applicable Bayesian information criterion (WBIC) and singular Bayesian information criterion (sBIC) give approximations to the log marginal likelihood, which can be applied to both regular and singular models. When the real log canonical thresholds are known, the performance of sBIC is considered to be better than that of WBIC, but only few real log canonical thresholds are known. In this paper, we propose a new estimator of the real log canonical thresholds based on the variance of thermodynamic integration with an inverse temperature. In addition, we propose an application to make sBIC widely applicable. Finally, we investigate the performance of the estimator and model selection by simulation studies and application to real data.},
	urldate = {2020-02-24},
	journal = {arXiv:1906.01341 [math, stat]},
	author = {Imai, Toru},
	month = aug,
	year = {2019},
	note = {arXiv: 1906.01341},
	keywords = {RLCT, information criteria, marginal likelihood, singular model, thermodynamic integration}
}

@article{grzegorczyk_targeting_2017,
	title = {Targeting {Bayes} factors with direct-path non-equilibrium thermodynamic integration},
	volume = {32},
	issn = {1613-9658},
	url = {https://doi.org/10.1007/s00180-017-0721-7},
	doi = {10.1007/s00180-017-0721-7},
	abstract = {Thermodynamic integration (TI) for computing marginal likelihoods is based on an inverse annealing path from the prior to the posterior distribution. In many cases, the resulting estimator suffers from high variability, which particularly stems from the prior regime. When comparing complex models with differences in a comparatively small number of parameters, intrinsic errors from sampling fluctuations may outweigh the differences in the log marginal likelihood estimates. In the present article, we propose a TI scheme that directly targets the log Bayes factor. The method is based on a modified annealing path between the posterior distributions of the two models compared, which systematically avoids the high variance prior regime. We combine this scheme with the concept of non-equilibrium TI to minimise discretisation errors from numerical integration. Results obtained on Bayesian regression models applied to standard benchmark data, and a complex hierarchical model applied to biopathway inference, demonstrate a significant reduction in estimator variance over state-of-the-art TI methods.},
	language = {en},
	number = {2},
	urldate = {2020-02-25},
	journal = {Computational Statistics},
	author = {Grzegorczyk, Marco and Aderhold, Andrej and Husmeier, Dirk},
	month = jun,
	year = {2017},
	keywords = {Bayes factor, marginal likelihood, thermodynamic integration},
	pages = {717--761}
}

@article{wenzel_how_2020,
	title = {How {Good} is the {Bayes} {Posterior} in {Deep} {Neural} {Networks} {Really}?},
	url = {http://arxiv.org/abs/2002.02405},
	abstract = {During the past five years the Bayesian deep learning community has developed increasingly accurate and efficient approximate inference procedures that allow for Bayesian inference in deep neural networks. However, despite this algorithmic progress and the promise of improved uncertainty quantification and sample efficiency there are---as of early 2020---no publicized deployments of Bayesian neural networks in industrial practice. In this work we cast doubt on the current understanding of Bayes posteriors in popular deep neural networks: we demonstrate through careful MCMC sampling that the posterior predictive induced by the Bayes posterior yields systematically worse predictions compared to simpler methods including point estimates obtained from SGD. Furthermore, we demonstrate that predictive performance is improved significantly through the use of a "cold posterior" that overcounts evidence. Such cold posteriors sharply deviate from the Bayesian paradigm but are commonly used as heuristic in Bayesian deep learning papers. We put forward several hypotheses that could explain cold posteriors and evaluate the hypotheses through experiments. Our work questions the goal of accurate posterior approximations in Bayesian deep learning: If the true Bayes posterior is poor, what is the use of more accurate approximations? Instead, we argue that it is timely to focus on understanding the origin of the improved performance of cold posteriors.},
	urldate = {2020-02-25},
	journal = {arXiv:2002.02405 [cs, stat]},
	author = {Wenzel, Florian and Roth, Kevin and Veeling, Bastiaan S. and Świątkowski, Jakub and Tran, Linh and Mandt, Stephan and Snoek, Jasper and Salimans, Tim and Jenatton, Rodolphe and Nowozin, Sebastian},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.02405}
}

@article{mattingly_maximizing_2018,
	title = {Maximizing the information learned from finite data selects a simple model},
	volume = {115},
	copyright = {© 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/8/1760},
	doi = {10.1073/pnas.1715306115},
	abstract = {We use the language of uninformative Bayesian prior choice to study the selection of appropriately simple effective models. We advocate for the prior which maximizes the mutual information between parameters and predictions, learning as much as possible from limited data. When many parameters are poorly constrained by the available data, we find that this prior puts weight only on boundaries of the parameter space. Thus, it selects a lower-dimensional effective theory in a principled way, ignoring irrelevant parameter directions. In the limit where there are sufficient data to tightly constrain any number of parameters, this reduces to the Jeffreys prior. However, we argue that this limit is pathological when applied to the hyperribbon parameter manifolds generic in science, because it leads to dramatic dependence on effects invisible to experiment.},
	language = {en},
	number = {8},
	urldate = {2020-02-24},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mattingly, Henry H. and Transtrum, Mark K. and Abbott, Michael C. and Machta, Benjamin B.},
	month = feb,
	year = {2018},
	pmid = {29434042},
	pages = {1760--1765}
}

@article{lin_ideal-theoretic_2017,
	title = {Ideal-{Theoretic} {Strategies} for {Asymptotic} {Approximation} of {Marginal} {Likelihood} {Integrals}},
	volume = {8},
	issn = {1309-3452},
	url = {http://arxiv.org/abs/1003.5338},
	doi = {10.18409/jas.v8i1.47},
	abstract = {The accurate asymptotic evaluation of marginal likelihood integrals is a fundamental problem in Bayesian statistics. Following the approach introduced by Watanabe, we translate this into a problem of computational algebraic geometry, namely, to determine the real log canonical threshold of a polynomial ideal, and we present effective methods for solving this problem. Our results are based on resolution of singularities. They apply to parametric models where the Kullback-Leibler distance is upper and lower bounded by scalar multiples of some sum of squared real analytic functions. Such models include finite state discrete models.},
	number = {1},
	urldate = {2020-02-24},
	journal = {Journal of Algebraic Statistics},
	author = {Lin, Shaowei},
	month = feb,
	year = {2017},
	note = {arXiv: 1003.5338},
	keywords = {RLCT, information criteria, marginal likelihood, singular model}
}

@article{lamont_information-based_2018,
	title = {Information-based inference for singular models and finite sample sizes: {A} frequentist information criterion},
	shorttitle = {Information-based inference for singular models and finite sample sizes},
	url = {http://arxiv.org/abs/1506.05855},
	abstract = {In the information-based paradigm of inference, model selection is performed by selecting the candidate model with the best estimated predictive performance. The success of this approach depends on the accuracy of the estimate of the predictive complexity. In the large-sample-size limit of a regular model, the predictive performance is well estimated by the Akaike Information Criterion (AIC). However, this approximation can either significantly under or over-estimating the complexity in a wide range of important applications where models are either non-regular or finite-sample-size corrections are significant. We introduce an improved approximation for the complexity that is used to define a new information criterion: the Frequentist Information Criterion (QIC). QIC extends the applicability of information-based inference to the finite-sample-size regime of regular models and to singular models. We demonstrate the power and the comparative advantage of QIC in a number of example analyses.},
	urldate = {2020-02-24},
	journal = {arXiv:1506.05855 [physics, stat]},
	author = {LaMont, Colin H. and Wiggins, Paul A.},
	month = jun,
	year = {2018},
	note = {arXiv: 1506.05855},
	keywords = {information criteria, singular model}
}

@article{zwiernik_asymptotic_2011,
	title = {An {Asymptotic} {Behaviour} of the {Marginal} {Likelihood} for {General} {Markov} {Models}},
	volume = {12},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v12/zwiernik11a.html},
	number = {Nov},
	urldate = {2020-02-24},
	journal = {Journal of Machine Learning Research},
	author = {Zwiernik, Piotr},
	year = {2011},
	keywords = {BIC, RLCT, marginal likelihood},
	pages = {3283--3310}
}

@article{sun_lightlike_2020,
	title = {Lightlike {Neuromanifolds}, {Occam}'s {Razor} and {Deep} {Learning}},
	url = {http://arxiv.org/abs/1905.11027},
	abstract = {How do deep neural networks benefit from a very high dimensional parameter space? Their high complexity vs stunning generalization performance forms an intriguing paradox. We took an information-theoretic approach. We find that the locally varying dimensionality of the parameter space can be studied by the discipline of singular semi-Riemannian geometry. We adapt Fisher information to this singular neuromanifold. We use a new prior to interpolate between Jeffreys' prior and the Gaussian prior. We derive a minimum description length of a deep learning model, where the spectrum of the Fisher information matrix plays a key role to reduce the model complexity.},
	urldate = {2020-02-24},
	journal = {arXiv:1905.11027 [cs, stat]},
	author = {Sun, Ke and Nielsen, Frank},
	month = feb,
	year = {2020},
	note = {arXiv: 1905.11027},
	keywords = {deep learning, information criteria, marginal likelihood}
}

@article{thomas_information_2019,
	title = {Information matrices and generalization},
	url = {http://arxiv.org/abs/1906.07774},
	abstract = {This work revisits the use of information criteria to characterize the generalization of deep learning models. In particular, we empirically demonstrate the effectiveness of the Takeuchi information criterion (TIC), an extension of the Akaike information criterion (AIC) for misspecified models, in estimating the generalization gap, shedding light on why quantities such as the number of parameters cannot quantify generalization. The TIC depends on both the Hessian of the loss H and the covariance of the gradients C. By exploring the similarities and differences between these two matrices as well as the Fisher information matrix F, we study the interplay between noise and curvature in deep models. We also address the question of whether C is a reasonable approximation to F, as is commonly assumed.},
	urldate = {2020-02-24},
	journal = {arXiv:1906.07774 [cs, stat]},
	author = {Thomas, Valentin and Pedregosa, Fabian and van Merriënboer, Bart and Mangazol, Pierre-Antoine and Bengio, Yoshua and Roux, Nicolas Le},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.07774},
	keywords = {deep learning, generalization, information criteria}
}

@article{grunwald_minimum_2019,
	title = {Minimum {Description} {Length} {Revisited}},
	issn = {2661-3352, 2661-3344},
	url = {http://arxiv.org/abs/1908.08484},
	doi = {10.1142/S2661335219300018},
	abstract = {This is an up-to-date introduction to and overview of the Minimum Description Length (MDL) Principle, a theory of inductive inference that can be applied to general problems in statistics, machine learning and pattern recognition. While MDL was originally based on data compression ideas, this introduction can be read without any knowledge thereof. It takes into account all major developments since 2007, the last time an extensive overview was written. These include new methods for model selection and averaging and hypothesis testing, as well as the first completely general definition of \{{\textbackslash}em MDL estimators\}. Incorporating these developments, MDL can be seen as a powerful extension of both penalized likelihood and Bayesian approaches, in which penalization functions and prior distributions are replaced by more general luckiness functions, average-case methodology is replaced by a more robust worst-case approach, and in which methods classically viewed as highly distinct, such as AIC vs BIC and cross-validation vs Bayes can, to a large extent, be viewed from a unified perspective.},
	urldate = {2020-02-24},
	journal = {International Journal of Mathematics for Industry},
	author = {Grünwald, Peter and Roos, Teemu},
	month = nov,
	year = {2019},
	note = {arXiv: 1908.08484},
	keywords = {generalization, minimum description length},
	pages = {S2661335219300018}
}