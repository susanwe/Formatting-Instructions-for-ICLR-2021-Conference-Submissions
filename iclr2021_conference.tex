\documentclass{article} % For LaTeX2e
\usepackage{iclr2021_conference,times}
\usepackage{iclr2021_extras}
\usepackage{graphicx,amsmath,amsthm}
\usepackage{showlabels}
\usepackage[skip=5pt]{subcaption}
\usepackage{booktabs}

	
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}

\graphicspath{{graphics/}}

\title{Deep Learning is Singular, and That's Good}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

\RequirePackage{algorithm}
\RequirePackage{algorithmic}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\def\be{\begin{equation}}
\def\ee{\end{equation}}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\def\l{\,|\,}
\def\lto{\longrightarrow}


%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
In singular models, the optimal set of parameters forms an algebraic or analytic set with singularities and classical statistical inference cannot be applied to such models. This is significant for deep learning as neural networks are singular, which in particular means that ``dividing" by the determinant of the Hessian in deep learning and, relatedly, employing the Laplace approximation are not appropriate. Despite its potential for addressing fundamental issues in deep learning, singular learning theory appears to have made little inroads into the developing canon of deep learning theory. Here we present an invitation to key ideas of singular learning theory via a mix of theory and experiments, with the goal of making the topic accessible to a wider audience. 
\end{abstract}

\section{Introduction}

It has been understood for close to twenty years that neural networks are singular statistical models \cite{amari_learning_2003, watanabe_almost_2007}. This means, in particular, that the set of network weights equivalent to the true model (under the Kullback-Leibler divergence) forms a real analytic variety which fails to be an analytic manifold due to the presence of singularities. It has been shown by Sumio Watanabe that the geometry of these singularities controls quantities of interest in statistical learning theory, e.g., the generalization error. Singular learning theory \citep{watanabe_algebraic_2009} is the study of singular models and requires very different tools from the study of regular statistical models. The breadth of knowledge demanded by singular learning theory -- Bayesian statistics, empirical processes and algebraic geometry -- is rewarded with profound and surprising results which reveal singular models are different from regular models in practically important ways.
To illustrate the relevance of singular learning theory to deep learning, each section illustrates a key takeaway idea\footnote{The code to reproduce all experiments in the paper will be released on Github. For now, see the zip file.}. 

\textbf{The real log canonical threshold (RLCT) is the correct way to count the effective number of parameters in a deep neural network (DNN)} (Section \ref{section:no_flat_minima}). 
    To every (model, truth, prior) triplet is associated a birational invariant known as the real log canonical threshold (RLCT). The RLCT can be understood in simple cases as half the number of normal directions to the set of true parameters. We will explain why this matters much more than the curvature of those directions (as measured for example by eigenvalues of the Hessian) laying bare some of the confusion over ``flat'' minima \citep{chaudhari2019entropy, smith2017bayesian, jastrzkebski2017three, Zhang:2018MolPh.116.3214Z}.

\textbf{For singular models, the Bayes predictive distribution is superior to MAP and MLE} (Section \ref{section:gen_error}). In regular statistical models,  the 1) Bayes predictive distribution, 2) \textit{maximum a posteriori} (MAP) estimator, and 3) maximum likelihood estimator (MLE) have asymptotically equivalent generalization error (as measured by the Kullback-Leibler divergence). This is not so in singular models. We illustrate in our experiments that even ``being Bayesian'' in just the final layers improves generalization over MAP. Our experiments further confirm that the Laplace approximation of the predictive distribution \citet{le_bayesian_2018,zhang_energyentropy_2018} is not only inappropriate but poor performing.

\textbf{The complexity of a singularity is inversely proportional to the RLCT} (Section \ref{section:simple_func}). In singular models the RLCT depends on the (model, truth, prior) triplet whereas in regular models it depends only on the (model, prior) pair. In particular, the complexity of the true distribution relative to the supposed model has an inverse relationship with the RLCT. We verify this experimentally with a simple family of ReLU networks. This makes concrete the theoretical principle that in singular learning theory, the generalisation error depends not just on the network architecture but also on the complexity of the data. %Furthermore we empirically observe that the complexity of a singularity is inversely proportional to the RLCT.

\section{Related work}
In classical learning theory, generalization is explained by measures of capacity such as the $l_2$ norm, Radamacher complexity, and VC dimension \citep{bousquet2003introduction}. It has become clear however that these measures cannot capture the empirical success of DNNs \citep{zhang_understanding_2017}. 
For instance, over-parameterized neural networks can easily fit random labels \cite{zhang2016understanding,du2018gradient,allen2019convergence} while having good generalization performance, which indicates that complexity measures such as Rademacher complexity is very large.
There is also a slate of work on generalization bounds in deep learning. Uniform convergence bounds \citep{neyshabur2015norm,bartlett2017spectrally,neyshabur2018towards,neyshabur2019towards,arora2018stronger} usually cannot provide non-vacuous bounds.
Data-dependent bounds \citep{brutzkus2017sgd,li2018learning,allen2019learning} consider the ``classifiability'' of the data distribution in generalization analysis of neural networks.
Algorithm-dependent bounds \cite{daniely2017sgd,arora2019fine,yehudai2019power,cao2019generalization} consider the relation of Gaussian initialization and the training dynamics of (stochastic) gradient descent to kernels methods, for example neural tangent kernels \cite{jacot2018neural}.

In contrast to many of the aforementioned works, we are interested in estimating the conditional \textit{distribution} $q(y|x)$. Specifically, we measure the generalization error of some estimate $\hat q_n(y|x)$ in terms of the Kullback-Leibler divergence between $q$ and $\hat q_n$, see \eqref{eq:Gn}. The next section gives a crash course on singular learning theory. The rest of the paper illustrates the key ideas listed in the introduction. Since we cover much ground in this short note, we will review other relevant work along the way, in particular literature on ``flatness", the Laplace approximation in deep learning, etc. 

\section{Singular Learning Theory}
%Rethinking generalization will likely require devising capacity measures suitable to the peculiarities of deep learning, e.g. non-identifiability and degenerate Fisher information matrix. 
To understand why classical measures of capacity fail to say anything meaningful about DNNs, it is important to distinguish between two different types of statistical models. Again, recall we are interested in estimating the true (and unknown) conditional distribution $q(y|x)$ with a class of models $\{p(y|x,w): w \in W\}$ where $W \subset \mathbb R^d$ is the parameter space. Let 
$$
W_0 = \{w \in W: p(y|x,w)=q(y|x)\}
$$
 be the set of true parameters. We say the model is \textit{identifiable} if $W_0$ is a singleton. Let $q(x)$ be the distribution of $x$. The Fisher information matrix associated with the model $\{p(y|x,w)\}$ is the matrix-valued function on $W$ defined by
 \begin{equation*}
 I(w)_{ij} = \int\!\int \frac{\partial}{\partial w_i}[ \log p(y|x,w) ] \frac{\partial}{\partial w_j}[ \log p(y|x,w) ] q(y|x) q(x) dx dy,
 \label{eq:FIM}
 \end{equation*}
if this integral is finite. 
Following the conventions in \cite{watanabe_algebraic_2009}, we have the following bifurcation of statistical models.
%\begin{definition}
A statistical model $p(y|x,w)$ is called \textbf{regular} if it is 1) identifiable and 2) has positive-definite Fisher information matrix. A statistical model is called \textbf{strictly singular} if it is not regular. 
%{\cite[Theorem 7.2]{watanabe_algebraic_2009}}.
%\end{definition}
It is straightforward to see neural networks are singular models. Indeed, a multilayer neural network statistical model is non-identifiable and has degenerate Fisher information matrix \emph{at every point of the parameter space}. We document this routine calculation in Appendix \ref{appendix:nn_singular}. 
%Classical tools from statistical inference are hence inappropriate for DNNs, e.g., one should not ``divide" by the determinant of the Hessian in deep learning. 


Let  $\varphi(w)$ be a prior on the model parameters $w$.
To every (model, truth, prior) triplet, we can associate the zeta function,
%\begin{equation}
$
\zeta(z) = \int K(w)^z \varphi(w) \,dw, z \in \mathbb C,
$
%\end{equation} 
where $K(w)$ is the Kullback-Leibler divergence between the model $p(y|x,w)$ and the true distribution $q(y|x)$:
\begin{equation}
%$$
    K(w) := \int \!\int q(y|x) \log \frac{ q(y|x) }{ p(y|x,w) } q(x) \,dx \,dy.
%$$
\label{eq:KL}
\end{equation}
%Then to every triplet of (model, truth, prior), we can associate the following central quantity of singular learning theory.
%\begin{definition}[Real log canonical threshold (RLCT)]
For a (model, truth, prior) triplet $(p(y|x,w),q(y|x),\varphi)$, let $-\lambda$ be the maximum pole of the corresponding zeta function. We call $\lambda$ the \textbf{real log canonical threshold} (RLCT) \citep{watanabe_algebraic_2009} of the (model, truth, prior) triplet. The RLCT is the central quantity of singular learning theory. 
%\label{def:RLCT}
%\end{definition}

By {\citet[Theorem 6.4]{watanabe_algebraic_2009}}, the RLCT is equal to $d/2$ in regular statistical models and bounded above by $d/2$ in strictly singular models if \textit{realizability} holds:
% \begin{definition}
we say $q(y|x)$ is \textbf{realizable} by the model class $\{p(y|x,w)\}$ if $W_0$ is non-empty.
%\end{definition}
The condition of realizability is critical to standard results in singular learning theory. Modifications to the theory are needed in the case that $q(y|x)$ is not realizable, see the more general condition called relatively finite variance in \citet{watanabe_mathematical_2018}.

% The work of Sumio Watanabe on singular learning theory \cite{watanabe_algebraic_2009} suggests that the \textit{real log canonical threshold} (RLCT) of a DNN is suitable for measuring its effective number of parameters (see Section ???). 
\textbf{RLCT plays an important role in model selection.}
One of the most accessible results in singular learning theory is the work related to the widely-applicable Bayesian information criterion (WBIC) \citet{watanabe_widely_2013}, which we briefly review here for completeness.
Let $\mathcal D_n =  \{(x_i,y_i)\}_{i=1}^n$ be a dataset of input-output pairs.  
Let $L_n(w)$ be the negative log likelihood
\begin{equation}
L_n(w) = -\frac{1}{n} \sum_{i=1}^n \log p(y_i |x_i, w)
\label{eq:nll}
\end{equation}
and $p(\mathcal D_n | w) = \exp( n L_n(w)).$
The marginal likelihood of a model $\{p(y|x,w): w \in W\}$ is given by
$
p(\mathcal D_n) = \int_W p(\mathcal D_n|w) \varphi(w) \,dw
$
and can be loosely interpreted as the evidence for the model. Between two models, we should prefer the one with higher model evidence.
However, since the marginal likelihood is an intractable integral over the parameter space of the model, one needs to consider some approximation.

Recall that the well-known Bayesian Information Criterion (BIC) derives from an asymptotic approximation of the negative log marginal likelihood, $-\log p(\mathcal D_n)$, using the Laplace approximation, leading to
$
\operatorname{BIC} = nL_n( w_{\operatorname{MLE}}) + \frac{d}{2} \log n.
$
Since we want the marginal likelihood of the data for some given model to be \textit{high}, we want the negative log marginal likelihood to be \textit{low}. Thus when $d$ is extremely large as is the case in DNNs, one should almost never adopt a deep network according to the BIC. 
%(aka the log marginal likelihood, aka the free energy). 
% Given two statistical models of a data source, one should choose (all else being equal) the one with higher model evidence. 

However, this argument contains a serious mathematical error: the Laplace approximation used to derive BIC only applies to \emph{regular} statistical models, and DNNs are not regular. Deep neural networks are \textit{strictly singular} as we illustrated in Appendix \ref{appendix:nn_singular}. 
If we have realizability (along with a few more technical conditions), the correct criterion for both regular and strictly singular models was shown in \citet{watanabe_widely_2013} to be 
%\begin{equation}
$
nL_n(w_0) + \lambda \log n,
$
%\label{logmarginal_rlct}
%\end{equation}
where $w_0 \in W_0$ and $\lambda$ is the RLCT. 
Since more complex singularities lead to smaller values of $\lambda$, which we illustrate in Section \ref{section:simple_func}, and deep learning models are highly singular, it is possible for DNNs to have high marginal likelihood -- consistent with their empirical success. 


\section{Volume dimension, effective degrees of freedom, and flatness}
\label{section:no_flat_minima}

\textbf{Volume dimension}. The easiest way to understand the RLCT is as a volume dimension \citep[Theorem 7.1]{watanabe_algebraic_2009}.  Suppose that $W \subseteq \mathbb{R}^d$ and $W_0$ is nonempty, i.e., the true distribution is realizable. We consider a special case in which the Kullback-Leibler distance in a neighborhood of every point $w_0 \in W_0$ has an expression in local coordinates of the form
\begin{equation}\label{eq:local_Kw}
K(w) = \sum_{i=1}^{d'} c_i w_i^2,
\end{equation} %This corresponds to a reduced rank regression model, or a neural network with two linear layers and the identity function: $f_w(x) = BAx$ where $B$ and $A$ are matrices of the appropriate size. 
where the coefficients $c_1,\ldots,c_{d'} > 0$ may depend on $w_0$ and $d'$ may be strictly less than $d$. If the model is regular then this is true with $d = d'$ and if it holds for $d' < d$ then we say that the pair $(p,q)$ is \emph{minimally singular}. It follows that the set $W_0 \subseteq W$ of true parameters is a regular submanifold of codimension $d'$ (that is, $W_0$ is a manifold of dimension $d - d'$ where $W$ has dimension $d$). Under this hypothesis there are, near each true parameter $w_0 \in W_0$, exactly $d - d'$ directions in which $w_0$ can be varied without changing the model $p(y|x,w)$ and $d'$ directions in which varying the parameters does change the model. In this sense, there are $d'$ \emph{effective parameters} near $w_0$. 

This effective dimension near $w_0$ can be computed by an integral. To see this, define the volume of the set of ``almost true'' parameters
%This is not true in general, and it is not true of DNNs, but it is true of regular models with $d' = 0$ (the set of true parameters is a set of isolated points) and for singular models such as reduced rank regression with $d' > 0$.
%\begin{equation}\label{eq:true_param}
$
V(t) = \int_{K(w) < t} \varphi(w) dw
$
%\end{equation}
where the integral is restricted to some sufficiently small open neighborhood of $w_0$. As long as the prior $\varphi(w)$ is non-zero on $W_0$ it does not affect the relevant features of the volume, so we may assume $\varphi$ is constant on the region of integration in the first $d'$ directions and normal in the remaining directions, so up to a constant depending only on $d'$ we have
\begin{equation}\label{eq:volume_singular}
V(t) \propto  t^{d'/2} (\Pi_i c_i)^{-1/2} 
\end{equation}
and we can extract the exponent of $t$ in this volume in the limit
\be\label{eq:limit_volumedim}
d' = 2 \lim_{t \to 0} \frac{\log\big\{V(at)/V(t)\big\}}{\log(a)}
\ee
for any $a > 0$, $a \neq 1$. The right hand side of (\ref{eq:limit_volumedim}) is the \emph{volume dimension} associated to $K$. 

The function $K(w)$ has the special form (\ref{eq:local_Kw}) locally with $d' = d$ if the statistical model is regular (and realizable) and with $d' < d$ in some singular models such as reduced rank regression (Appendix \ref{appendix:reducedrank}). While such a local form does not exist for a general singular model (in particular for neural networks) nonetheless under natural conditions Watanabe shows in \citet[Theorem 7.1]{watanabe_algebraic_2009} that
$
V(t) = c t^\lambda + o( t^\lambda)
$
where $\lambda$ is the RLCT and $c$ is a constant.\footnote{We are assuming that in a sufficiently small neighborhood of $w_0$ the RLCT at $w_0$ is less than or equal to the RLCT at every point in the neighborhood so that the multiplicity $m = 1$, see Section 7.6 of \citep{watanabe_algebraic_2009}.} It follows that the limit on the right hand side of  (\ref{eq:volume_singular}) exists and is equal to $\lambda$. In particular $\lambda = d/2$ in the regular case and $d'/2$ in the more general case of (\ref{eq:local_Kw}) above. In general it is appropriate to think of $2 \lambda$ as a volume dimension, or as the effective number of parameters near $w_0$.\footnote{In general $\lambda$ is a rational number, so it is not straightforward to give a geometric interpretation of why the volume grows as $t^\lambda$. Such an analysis requires an understanding of the resolution of singularities.}

%It can be easily shown in this special case that the RLCT $\lambda = d'/2$. In other words, $2\lambda$ is the effective number of parameters in the model.

\textbf{RLCT and likelihood vs temperature}. Let us assume that our statistical model is of the exponential form  (\ref{eq:gaussian_model_in_w}) and let $\mathcal D_n = \{ (x_i,y_i) \}_{i=1}^n$ be sampled from the true distribution. Suppose the task is regression and the negative log likelihood is
%\begin{equation}
$
L_n(w) = \frac{M}{2} \log(2\pi) + \frac{1}{n} \sum_{i=1}^n \frac{1}{2} \| y_i - f(x_i,w) \|^2\,.
$
%\end{equation}
Given a temperature $T$ consider the quantity
$$
E(T) = \mathbb{E}^{1/T}_w\big[nL_n(w) \big] = \mathbb{E}_w^{1/T}\Big[ \tfrac{1}{2} \sum_{i=1}^n \| y_i - f(x_i, w) \|^2 \Big] + \frac{nM}{2} \log(2\pi),
$$
where $p^{1/T}(w|D_n)$ is the Bayesian posterior at inverse temperature $1/T$ of \ref{general_posterior}.

Note that when $n$ is large $L_n(w_0) \approx \frac{M}{2} \log(2\pi)$ for any $w_0 \in W_0$ so for $T \approx 0$ the posterior concentrates around the set $W_0$ of true parameters and $E(T) \approx \frac{nM}{2} \log(2\pi)$. Consider the increase $\Delta E = E(T + \Delta T) - E(T)$ corresponding to an increase in temperature $\Delta T$. It can be shown that 
$
\Delta E \approx \lambda \Delta T
$
where the reader should see \citep[Corollary 3]{watanabe_widely_2013} for a precise statement. As the temperature increases, samples taken from the tempered posterior are more likely to come from further away from $W_0$ and $E$ will increase. If $\lambda$ is smaller then for a given increase in temperature the quantity $E$ increases less: this is one way to understand why a model with smaller RLCT generalizes better from the dataset $D_n$ to the true distribution.\footnote{The relation $\Delta E \approx \lambda \Delta T$ holds for the global RLCT. If the posterior is defined with respect to a prior $\varphi(w)$ which is concentrated around a point $w_0$ with lower point RLCT than any other point in its neighborhood, as above, then the same relation holds for the point RLCT $\lambda$.}

%Hence if $d'_0 < d'_1$ then the model we obtain by integrating near $w_0$ will have an error which increases less quickly with temperature. From the point of view of statistical learning two true parameters with different volume dimensions are not equivalent.

%\textbf{Not all true parameters are created equal} More generally, we can consider a model in which $K(w)$ has the form (\ref{eq:local_Kw})  near each point of $W_0$ but where the codimension $d'$ (and hence the volume dimension) varies from point to point (for instance $W_0$ might be a disjoint union of two regular submanifolds). What are we to make of such a situation? Since two true parameters $w_0, w_1$ with different volume dimensions $d_0' < d_1'$ still determine the same model, the significance of this situation requires explanation.


%We take the point of view that in deep learning it is not meaningful to talk about \emph{points} $w \in W$ as this would suggest an inference procedure of infinite precision. The only ``physically'' meaningful quantities are distributions $\psi(w)$ over $W$ and the associated predictive distributions
%\[
%p^*_\psi(y|x) = \int p(y|x,w) \psi(w) dw\,.
%\]
%While two parameters $w_0, w_1$ with volume dimensions $d'_0, d'_1$ determine the same model, if the predictive distributions $p^*_{\psi_1}(y|x)$ and $p^*_{\psi_2}(y|x)$ are essentially different for $\psi_i$ arbitrarily concentrated around $w_i$ then the two parameters are, in a deeper sense, not equivalent. For example, suppose the statistical model is of exponential form, as with the models defined in Section \ref{??} associated to two-layer ReLU networks. 

%\begin{remark}
%The explanation requires examining local neighborhoods around $w_0$ and $w_1$ with respect to the %\textit{posterior distribution}:
%\begin{equation}
%p(w|\mathcal D_n) \propto \Pi_{i=1}^n p(y_i|x_i,w) \varphi(w)
%\end{equation}
%Namely, we must compare $E_w \displaystyle \1_\mathrm{w \in U(w_0)}$ and $E_w \displaystyle \1_\mathrm{w \in U(w_1)}$ where for random variable $R(w)$
%\begin{equation}
%{\E}_w [R(w)] := \int_W R(w) p(w|\mathcal D_n) \,dw
%\label{eq:E_w}
%\end{equation}
%For any $w^* \in W$, we can write
%$
%E_w \displaystyle \1_\mathrm{w \in U(w^*)} = p(w^*|\mathcal D_n) Z(w^*).
%$
%Watanabe pointed out that the more complicated singularity (smaller RLCT) has higher $Z(w^*)$. Since $d_0' < d_1'$, $w_0$ is the more complicated singularity and we have
%$$
%E_w \displaystyle \1_\mathrm{w \in U(w_0)} > E_w \displaystyle \1_\mathrm{w \in U(w_1)}
%$$
%This is to say that two true parameters may lead to the same model, but could have vastly different behavior if we look at \eqref{eq:E_w} restricted to local neighborhoods of the true parameters. 
%\end{remark}

%We take the point of view that in deep learning it is not meaningful to talk about \emph{points} $w \in W$ as this would suggest an inference procedure of infinite precision. The only ``physically'' meaningful quantities are distributions $\psi(w)$ over $W$ and the associated predictive distributions
%\[
%p^*_\psi(y|x) = \int p(y|x,w) \psi(w) dw\,.
%\]
%While two parameters $w_0, w_1$ with volume dimensions $d'_0, d'_1$ determine the same model, if the predictive distributions $p^*_{\psi_1}(y|x)$ and $p^*_{\psi_2}(y|x)$ are essentially different for $\psi_i$ arbitrarily concentrated around $w_i$ then the two parameters are, in a deeper sense, not equivalent. For example, suppose the statistical model is of exponential form, as with the models defined in Section \ref{??} associated to two-layer ReLU networks. 
%
%Given a fixed dataset $D_n = \{ (x_i,y_i) \}_{i=1}^n$ sampled from the true distribution we consider the expected squared error over the dataset
%\[
%E(T) = \mathbb{E}_w^T\Big[ \sum_{i=1}^n \| y_i - f(x_i, w) \|^2 \Big]
%\]
%where $w$ from the posterior at temperature $T$. This quantity is a random variable, which depends on the dataset $D_n$ the temperature $T$ and the prior $\varphi(w)$ that is placed on the parameters. The temperature controls how finely parameters are distinguished: at high temperature points with high posterior probability are barely distinguishable from points with low probability, and as $T \lto 0$ all the probability concentrates around true parameters. Consider the change $\Delta E = E(T + \Delta T) - E(T)$ resulting from an increase in the temperature. This is a measure of sensitivity of the performance of the predictive distribution to temperature. If our prior $\varphi = \psi_i$ is concentrated near a point $w_i \in W_0$ of volume dimension $d'_i$, then it can be shown that
%\[
%\Delta E \approx d'_i \Delta T
%\]
%Hence if $d'_0 < d'_1$ then the model we obtain by integrating near $w_0$ will have an error which increases less quickly with temperature. From the point of view of statistical learning two true parameters with different volume dimensions are not equivalent.

\textbf{Flatness}. It is folklore in the deep learning community that flatness of minima is related to generalization \citep{hinton_keeping_1993, hochreiter1997flat} and this claim has been revisited in recent years \citep{chaudhari2019entropy, smith2017bayesian, jastrzkebski2017three, Zhang:2018MolPh.116.3214Z}. In regular models this can be justified using the lower order terms of the asymptotic expansion of the Bayes free energy \citep[\S 3.1]{Balasubramanian:1996cond.mat..1030B} but the argument breaks down in singular models, since for example the Laplace approximation of \cite{Zhang:2018MolPh.116.3214Z} is invalid. The basic point can be understood by a close analysis of the version of the idea in \citep{hochreiter1997flat}. Their measure of entropy compares the volume of the set of parameters with tolerable error $t_0$ (our ``almost true'' parameters) to a standard volume defined without the Kullback-Leibler function
\begin{equation}\label{eq:entropy}
- \log\Big[\frac{V(t_0)}{t_0^{d/2}}\Big] = \frac{d-d'}{2} \log(t_0) + \tfrac{1}{2} \sum_{i=1}^{d} \log c_i\,.
\end{equation}
Hence in the case $d = d'$ the quantity $-\tfrac{1}{2} \sum_i \log(c_i)$ is a measure of the entropy of the set of true parameters near $w_0$, a point made for example in \cite{Zhang:2018MolPh.116.3214Z}. However when $d' < d$ this conception of entropy is inappropriate because of the $d - d'$ directions in which $K(w)$ is flat near $w_0$, which introduce the $t_0$ dependence in (\ref{eq:entropy}). There is no way around it: ideas from algebraic geometry such as the resolution of singularities are necessary to understand the geometric features of the loss surface that determine generalization error.

% Note that in comparing two regular models with parameter space $W \subseteq \mathbb{R}^d$, both of which have volume dimension $d/2$, the Hessian determinant is relevant to compare how the respective volumes of almost true parameters vary with $t$. But in comparing two minimally singular models with Hessian determinants of rank $d', d''$ the behavior of the volume depends much more strongly on the dimensions $d'/2, d''/2$ than it does on the eigenvalues.
% From a theoretical point of view the exponent $d/2$ appears as the leading $O(\log n)$ term in the asymptotic expansion of the Bayes free energy and the Hessian determinant appears in the next $O(1)$ term, where $n$ is the size of a dataset . The upshot of this is that if two regular models have parameter spaces of dimension $d_1 < d_2$ one should prefer the model class of dimension $d_1$ (assuming both can perfectly fit the data) but if $d_1 = d_2$ then it may be appropriate to compare models on the basis of their curvature at the true parameter.

% What kind of space is the set of true parameters of a singular statistical model? In modern mathematics we generally think of spaces as a pair consisting of a topological space $X$ and a sheaf of rings, which for every open subset $U \subseteq X$ picks out a certain class of continuous functions $U \lto \mathbb{R}$. In the theory of smooth or analytic manifolds, for instance, these would be the $C^\infty$ or $C^\omega$ functions, while in algebraic geometry these would be the regular functions. Historically statistical learning theory and machine learning has mostly employed concepts of space from differential geometry, where this sheaf can be safely left implicit: if $X \subseteq \mathbb{R}^D$ is a regular submanifold, the manifold structure on $X$ can be recovered from the \emph{set} $X$ and the manifold structure on $\mathbb{R}^D$. However in algebraic geometry a closed subspace does \emph{not} have an unambiguous structure as a space in its own right:

%\begin{example} The zero locus of $x$ and $x^2$ in $\mathbb{R}^2$ have the same underlying set, but as schemes the former is smooth and has coordinate ring $\mathbb{R}[x,y]/x \cong \mathbb{R}[y]$ while the latter is singular and has coordinate ring $\mathbb{R}[x,y]/x^2$. Singularity vs regularity is \emph{not a property of the set} but the function whose zero locus the set is.
%\end{example}

\section{Generalization}\label{section:gen_error}
The generalization puzzle \citep{DBLP:journals/corr/abs-1801-00173} is one of the central mysteries of deep learning. Theoretical investigations into the matter is an active area of research \cite{neyshabur_exploring_2017}. Many of the recent proposals of capacity measures for neural networks are based on the eigenspectrum of the (degenerate) Hessian, e.g., \citet{thomas_information_2019, maddox_rethinking_2020}. But this is not appropriate for singular models, and hence for deep neural networks.
%??? \cite{gao_degrees_2016}, \cite{sun_lightlike_2020}.???


Precise statements regarding the generalization behavior in singular models can be made using singular learning theory.
%The RLCT is a birational invariant \citep{kollar_birational_1998} that measures the complexity of singularities in the model. Since the complexity of singularities determine most of the interesting properties of a model, the RLCT is a very appealing choice for understanding, say, the generalization behavior in singular models, particularly DNNs. Furthermore the RLCT as a capacity measure is theoretically rigorous which most existing capacity measures for DNNs cannot claim. 
Let the network weights be denoted $\theta$ rather than $w$ for reasons that will become clear. Recall in the Bayesian paradigm, prediction proceeds via the so-called Bayes predictive distribution,
%\begin{equation}
$
p(y|x, \mathcal D_n) = \int p(y|x,\theta) p(\theta|\mathcal D_n) \,d\theta.
$
%\label{eq:bayes_pred_dist}
%\end{equation}
More commonly encountered in deep learning practice are the MAP and MLE point estimators.
While in a regular statistical model, the three estimators 1) Bayes predictive distribution, 2) MAP, and 3) MLE have the \textit{same} leading term in their asymptotic generalization behavior, the same is not true in singular models.
% Let $\varphi(\theta)$ be the prior distribution of the weights of the network. We call $p(\mathcal D_n |\theta) = \Pi_{i=1}^n p(y_i | x_i, \theta)$ the likelihood of the data $\mathcal D_n = \{(x_i,y_i)\}_{i=1}^n$. Then we have the posterior distribution of $\theta$: $$
% p(\theta | \mathcal D_n) \propto p(\mathcal D_n | \theta) \varphi(\theta).
% $$
% In general the normalizing constant in the posterior distribution is an intractable integral. When $f_\theta$ is a DNN, this is especially true. Though many approximate Bayesian techniques exist for sampling the posterior, e.g., MC, variational inference, doing this effectively for a DNN is still an open challenge. 
% Leaving aside the fact that the posterior is hard to sample, let's see why we should care about the posterior distribution in the first place. 
More precisely, let $\hat q_n(y|x)$ be some estimate of the true unknown conditional density $q(y|x)$ based on the dataset $\mathcal D_n$. The generalization error of the predictor $\hat q_n(y|x)$ is defined as
\begin{equation}
G(n) := KL (q(y|x) || \hat q_n(y|x) ) = \int  \int q(y|x) \log \frac{q(y|x)}{\hat q_n(y|x)} q(x) \,dy  \,dx.
\label{eq:Gn}
\end{equation}
To account for sampling variability, we will work with the \textit{average generalization error}, ${\E}_n G(n)$, where ${\E}_n$ denotes expectation over the dataset $\mathcal D_n$.
By {\citet[Theorem 1.2 and Theorem 7.2]{watanabe_algebraic_2009}}, we have
\begin{equation}
{\E}_n G(n) = \lambda/n + o(1/n)  \text{ if $\hat q_n$ is the Bayes predictive distribution,}
\label{eq:Bayes_generalization}
\end{equation}
where $\lambda$ is the RLCT corresponding to the triplet $( p(y|x,\theta), q(y|x), \varphi(\theta) )$. In contrast, we should note that \citet{zhang_energyentropy_2018} and  \citet{le_bayesian_2018} rely on the Laplace approximation to explain the generalization of the Bayes predictive distribution though both works acknowledge the Laplace approximate is inappropriate. For completeness, a quick sketch of the derivation of \eqref{eq:Bayes_generalization} is provided in Appendix \ref{appendix:generalization_theory}.
Now by {\cite[Theorem 6.4]{watanabe_algebraic_2009}} we have
\begin{equation}
{\E}_n G(n) = C/n + o(1/n)   \text{ if $\hat q_n$ is the MAP or MLE,}
\label{eq:map_generalization}
\end{equation}
where $C$ (different for MAP and MLE) is the maximum of some Gaussian process. For regular models, the MAP, MLE, and the Bayes predictive distribution have the same leading term for ${\E}_n G(n)$ since $\lambda = C = d/2$. However in singular models, $C$ is generally greater than $\lambda$. 

This means, since neural networks are singular, we should employ the Bayes predictive distribution rather than MAP or MLE in deep learning. This is seldom done in practice. It is not for an ignorance of singular learning theory but for the mere fact that the Bayes predictive distribution is intractable as it is based on the intractable posterior distribution. 
We set out to investigate whether certain very simple approximations of the Bayes predictive distribution can already demonstrate superiority over point estimators. 

%To approximate the Bayes predictive distribution, we consider marginalization over the final layers of the neural network with earlier layers frozen at the MAP point estimate. 
Suppose the input-target relationship is modeled as
\begin{equation}
p(y|x,\theta) \propto \exp\{-|| y - f_\theta(x) ||^2/2\},
\label{eq:genexp_model}
\end{equation}
where $f_\theta$ is a multilayer neural network.
We set $q(x) = N(0,I_3)$. 
For now consider the realizable case, $q(y|x) = p(y|x,\theta_0)$ where $\theta_0$ is drawn randomly according to the default initialization in PyTorch when model \ref{eq:genexp_model} is instantiated. We calculate $\E_n G(n)$ using multiple datasets $\mathcal D_n$ and a large testing set, see Appendix \ref{appendix:generalizaton} for more details. 

\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.35]{taskid8.png}
		\includegraphics[scale=0.35]{taskid9.png}
		\includegraphics[scale=0.35]{taskid10.png}
		\includegraphics[scale=0.35]{taskid11.png}
	\end{center}
	\caption{\textit{Realizable and full batch gradient descent for MAP.} Average generalization errors $\E_n G(n)$ are displayed for various approximations of the Bayes predictive distribution. The results of the Laplace approximations are reported in the Appendix and not displayed here because they are higher than other approximation schemes by at least an order of magnitude. Each subplot shows a different combination of hidden layers in $g$ (1 or 5) and activation function in $h$ (ReLU or identity). Note that the y-axis is not shared.
	}
%		Figure \ref{fig:avg_gen_err_minibatch_realizable} is analogous to this figure except minibatch gradient descent was employed to find MAP.}
	\label{fig:avg_gen_err_fullbatch_realizable}
\end{figure}




 Since $f$ is a hierarchical model, let's write it as $f_\theta(\cdot) = h(g(\cdot;v);w)$ with the dimension of $w$ being relatively small. Let $\theta_{map} = (v_{map}, w_{\operatorname{MAP}})$ be the MAP estimate for $\theta$ in model \ref{eq:genexp_model} using batch gradient descent. The idea of our simple approximate Bayesian scheme is to freeze the network weights at the MAP estimate for early layers and perform approximate Bayesian inference for the final layers\footnote{This is similar in spirit to \citet{kristiadi_being_2020} who claim that even ``being Bayesian a little bit" fixes overconfidence. They approach this via the Laplace approximation for the final layer of a ReLU network. It is also worth noting that \citet{kristiadi_being_2020} do not attempt to formalize what it means to "fix overconfidence"; the precise statement should be in terms of $G(n)$.}. e.g., freeze the parameters of $g$ at $v_{map}$ and perform MCMC over $w$. 
Throughout the experiments, $g: \mathbb R^3 \to \mathbb R^3$ is a feedforward ReLU block with each hidden layer having 5 hidden units and $h: \mathbb R^3 \to \mathbb R^3$ is either $BAx$ or $B \operatorname{ReLU}(Ax)$ where $A \in \mathbb R^{3 \times r}, B \in \mathbb R^{r \times 3}$. We set $r=3$. We shall consider 1 or 5 hidden layers for $g$. 

\begin{table}[h!]%
	\caption{Companion to Figure \ref{fig:avg_gen_err_fullbatch_realizable}. The learning coefficient is the slope of the linear fit $1/n$ versus $\E_n G(n)$ (no intercept since realizable). The $R^2$ value gives a sense of the goodness-of-fit.}
	\label{table:avg_gen_err_fullbatch_realizable}
	\centering
	\begin{tiny}
		\begin{subtable}[t]{2.5in}
			\caption{1 hidden layer(s) in $g$, identity activation in $h$}			\input{graphics/taskid8.tex}
		\end{subtable}
		\quad
		\begin{subtable}[t]{2.5in}
			\caption{5 hidden layer(s) in $g$, identity activation in $h$}
			\input{graphics/taskid9.tex}
		\end{subtable}
		\quad
		\begin{subtable}[t]{2.5in}
			\caption{1 hidden layer(s) in $g$, ReLU activation in $h$}
			\input{graphics/taskid10.tex}
		\end{subtable}
		\quad
		\begin{subtable}[t]{2.5in}
			\caption{5 hidden layer(s) in $g$, ReLU activation in $h$}			\input{graphics/taskid11.tex}
		\end{subtable}
	\end{tiny}
\end{table}

%Since we consider $h$ to be a composition of two layers, let's write $h(\cdot;w)=h_2( h_1(\cdot; A); B)$ where $w=(A,B)$.
To approximate the Bayes predictive distribution, we perform either the Laplace approximation or the NUTS variant of HMC \citep{hoffman2014no} in the last two layers, i.e., performing inference over $A,B$ in
%\begin{equation}
$h(g(\cdot;v_{map});A,B).$
%\label{eq:last_two_layers}
%\end{equation}
Note that MCMC is operating in a space of 18 dimensions in this case, which is small enough for us to expect MCMC to perform well.
We also implemented the Laplace approximation and NUTS in the last layer only, i.e. performing inference over $B$ in
%\begin{equation}
$h_2(h_1(g(\cdot;v_{map});A_{map}); B).$
%\label{eq:last_layer}
%\end{equation}
Further implementation details of these approximate Bayesian schemes are found in Appendix \ref{appendix:generalizaton}.


From the outset, we expect the Laplace approximation over $w = (A, B)$ to be invalid since the model is singular. We do however expect the last-layer-only Laplace approximation over $B$ to be sound. Next, we expect the MCMC approximation in either the last layer or last two layers to be  superior to the Laplace approximations and to the MAP. We further expect the last-two-layers MCMC to have better generalization than the last-layer-only MCMC since the former is closer to the Bayes predictive distribution. In summary, we anticipate the following performance order for these five approximate Bayesian schemes (from worst to best): last-two-layers Laplace, last-layer-only Laplace, MAP, last-two-layers MCMC, last-layer-only MCMC.


To understand the effect of the network architecture, we varied the following factors:  1) either 1 or 5 layers in $g$, and 2) ReLU or identity activation in $h$.
The results displayed in Figures \ref{fig:avg_gen_err_fullbatch_realizable} are in line with our stated expectations above, except for the surprise that the last-layer-only MCMC approximation is often superior to the last-two-layers MCMC approximation. This may arise from the fact that MCMC finds the singular setting in the last-two-layers more challenging. Table \ref{table:avg_gen_err_fullbatch_realizable} is a companion to Figure \ref{fig:avg_gen_err_fullbatch_realizable} and tabulates for each approximation scheme the slope of $1/n$ versus ${\E}_n G(n)$, also known as the learning coefficient. The $R^2$ corresponding to the linear fit is also provided. 


In Appendix \ref{appendix:generalizaton}, we also show the corresponding results when 1) the data-generating mechanism and the assumed model do not satisfy the condition of realizability and/or 2) the MAP estimate is obtained via minibatch stochastic gradient descent instead of batch gradient descent. 



\section{Simple functions and complex singularities}\label{section:simple_func}

In singular models the RLCT varies with the true distribution (in contrast to regular models) and in this section we examine this phenomenon in a simple example. As the true distribution becomes more complicated relative to the supposed model, the singularities of the analytic variety of true parameters should become simpler and hence the RLCT should increase \citep[\S 7.6]{watanabe_algebraic_2009}

Consider the model $p(y|x,w)$ in (\ref{eq:gaussian_model_in_w}) where
$
f(x,w) = c + \sum_{i=1}^H q_i \operatorname{ReLU}( \langle w_i, x \rangle + b_i )
$
is a two-layer ReLU network with weight vector $w = (\{w_i\}_{i=1}^H, \{b_i\}_{i=1}^H, \{q_i\}_{i=1}^H, c) \in \mathbb{R}^{4H+1}$ and $w_i \in \mathbb{R}^2, b_i \in \mathbb{R}, q_i \in \mathbb{R}$ for $1 \le i \le H$. We let $W$ be some compact neighborhood of the origin.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.35]{truedist3.png}
\includegraphics[scale=0.35]{truedist4.png}
\includegraphics[scale=0.35]{truedist5.png}
\end{center}
\caption{Increasingly complicated true distributions $q_m(x,y)$ on $[-1,1]^2 \times \mathbb{R}$.}
\label{fig:simp_func_complex}
\end{figure}

\begin{table}[h]
	\centering
	\caption{\footnotesize RLCT estimates for ReLU and SiLU networks. We observe the RLCT increasing as $m$ increases, i.e., the true distribution becomes more ``complicated'' relative to the supposed model.}
	\label{table:hyper}
    \begin{tiny}
    \begin{tabular}
    {r l l l l}
    \toprule
      \textbf{m}  & \textbf{Nonlinearity}  & \textbf{RLCT} & \textbf{Std} & \textbf{R squared}\\ 
    \midrule
    3 & ReLU & 0.526301 & 0.027181 & 0.983850\\
    3 & SiLU & 0.522393 & 0.026342 & 0.978770\\
    \hline
    4 & ReLU & 0.539590 & 0.024774 & 0.991241\\
    4 & SiLU & 0.539387 & 0.020769 & 0.988495\\
    \hline
    5 & ReLU & 0.555303 & 0.002344 & 0.993092\\
    5 & SiLU & 0.555630 & 0.021184 & 0.990971\\
   \bottomrule
   \end{tabular}
	\end{tiny}
\end{table}


%\begin{table}[h]
%    \begin{center}
%    \begin{tabular}
%    {r l l l l}
%    \toprule
%      \textbf{m}  & \textbf{Nonlinearity}  & \textbf{Mean} & \textbf{Std}\\ 
%    \midrule
%    3 & ReLU & 0.526301 & 0.027181\\
%    3 & SiLU & 0.522393 & 0.026342\\
%    4 & ReLU & 0.539590 & 0.024774\\
%    4 & SiLU & 0.539387 & 0.020769\\
%    5 & ReLU & 0.555303 & 0.002344\\
%    5 & SiLU & 0.555630 & 0.021184\\
%   \bottomrule
%   \end{tabular}
%    \end{center}
%    \caption{\footnotesize RLCT estimates for ReLU and SiLU.}
%    \label{table:hyper}
%\end{table}
%

%\begin{figure}[h]
%\begin{center}
%\includegraphics[scale=0.6]{RLCTplot.png}
%\end{center}
%\caption{RLCT estimates for ReLU and SiLU. Shaded region shows one standard deviation.}
%\label{fig:simp_func_complex2}
%\end{figure}

Given an integer $3 \le m \le H$ we define a network $s_m \in W$ and $q_m(y|x) := p(y|x, s_m)$. Let $g \in SO(2)$ stand for rotation by $\frac{2\pi}{m}$, set $w_1 = g^{\tfrac{1}{2}} e_1$ where $e_i$ denote unit vectors. The components of $s_m$ are the vectors $w_i = g^{i-1} w_1$ for $1 \le i \le m$ and $w_i = 0$ for $i > m$, $b_i = - \tfrac{1}{3}$ and $q_i = 1$ for $1 \le i \le m$ and $b_i = q_i = 0$ for $i > m$, and finally $c = 0$ (the factor of $\tfrac{1}{3}$ ensures the relevant parts of the decision boundaries lie within $X = [-1,1]^2$). We let $q(x)$ be the uniform distribution on $X$ and define $q_m(x,y) = q_m(y|x) q(x)$. The functions $f(x,s_m)$ are graphed in Figure \ref{fig:simp_func_complex}. It is intuitively clear that the complexity of these true distributions increases with $m$.

We let $\varphi$ be a normal distribution $\mathcal{N}(0,50^2)$ centered on $W$ and estimate the RLCTs of the triples $(p, q_m, \varphi)$. We conducted the experiments with $H = 5$, $n = 1000$. For each $m \in \{3,4,5\}$, Table \ref{table:hyper} shows the estimated RLCT. Algorithm \ref{alg:thm4} in Appendix \ref{appendix:RLCT_estimation} details the estimation procedure for the RLCT which we base on \citep[Theorem 4]{watanabe_widely_2013}. As predicted the RLCT increases with $m$ verifying that in this case, the simpler true distributions give rise to more complex singularities.

Note that in all cases the number of parameters in $W$ is $d = 21$ and so if the model were regular the RLCT would be $10.5$. It can be shown that when $m = H$ the set of true parameters $W_0 \subseteq W$ is a regular submanifold of dimension $m$ \cite{???}. If such a model were minimally singular its RLCT would be $\tfrac{1}{2}( (4m + 1) - m ) = \tfrac{1}{2}( 3m + 1 )$. In the case $m = 5$ we observe an RLCT more than an order of magnitude less than the value $8$ predicted by this formula. So the function $K$ is not quadratic locally near $W_0$.

Strictly speaking it is incorrect to speak of the RLCT of a ReLU network because the function $K(w)$ is not necessarily analytic (Example \ref{example:not_analytic}). However we observe empirically that the predicted linear relationship between $E^\beta_w[nL_n(w)]$ and $1/\beta$ holds in our small ReLU networks, and that the RLCT estimates are close to those for the two-layer SiLU network, which is analytic (the SiLU or sigmoid weighted linear unit is $\sigma(x) = x (1 + e^{-\tau x})^{-1}$ which approaches the ReLU as $\tau \to \infty$. We use $\tau = 100.0$ in our experiments). The competitive performance of SiLU on standard benchmarks \cite{???} shows that the non-analyticity of ReLU is not fundamental.

\begin{remark} The comments of \citet[\S 7.6]{watanabe_algebraic_2009} are based on the results of \citet[\S 7.2]{watanabe_algebraic_2009} which are summaries of \cite{??,??}. In \cite{??} the nonlinearity is the identity (reduced rank regression) and in \cite{??} the true distribution is always the zero function and the nonlinearity is $\operatorname{tanh}(x)$.
\end{remark}

% The RLCT estimates for the two-layer SiLU network (\ref{??}) provide evidence that this model is not minimally singular, when combined with the symmetric true distribution $q^{SiLU}_m(x,y)$. Starting from the weight vector of this true distribution, varying $c$ or any of the $b_i$ independently takes the model off the set of true parameters, so that the normal bundle has dimension $> H + 1$. Hence the RLCT in the minimally singular case would be bounded below by $3$ in the case $H = 5$, but our estimates for the RLCT are $< 0.6$.

\section{Future directions}

Deep neural networks are singular statistical models, and that's good: the presence of singularities is \emph{necessary} for neural networks with large numbers of parameters to have low generalization error. Singular learning theory clarifies how mathematical tools like the Laplace approximation are not just inappropriate in deep learning on narrow technical grounds: the failure of this approximation and the existence of interesting phenomena like the generalisation puzzle have a common cause, namely the existence of degenerate critical points of the Kullback-Leibler function $K(w)$. 

Singular learning theory is a promising foundation for a mathematical theory of deep learning. However, much remains to be done. The important open problems include:

\textbf{Real-world distributions are unrealizable}
The consequences of nonrealizability are not well understood in singular learning theory. Among the most profound experimental results in deep learning are the existence of power laws in language model training \cite{??} and this is an indication of interesting new phenomena in singular learning theory in the case where the true distribution is unrealizable.

\textbf{SGD vs the posterior.} A growing number of works \citep{Simsekli17,mandt_stochastic_2018,smith_stochastic_2018} suggest that some version of mini-batch SGD may be governed by SDEs that have the posterior distribution as its stationary distribution. This suggests approximating the Bayes predictive distribution using SGD to sample from the posterior. Our preliminary work on this indicates having SGD samples match a target posterior distribution is challenging.

\textbf{RLCT estimation for large networks.} 
Theoretical RLCTs have been cataloged for very few singular models at significant effort\footnote{Hironaka's resolution of singularities guarantees existence. However it is difficult to do the required blowup transformations in high dimensions to obtain the standard form.}, namely single-hidden-layer $\tanh$ network \citep{aoyagi_resolution_2006} and reduced rank regression \citep{aoyagi_stochastic_2005}. We believe RLCT estimation in these small networks should be standard benchmarks for any method that purports to approximate the Bayesian posterior of a neural network.

No theoretical RLCTs are known for modern DNNs.
Further, no method is known for estimating RLCTs of large neural networks. In theory MCMC provides the gold standard but this approach does not scale to large networks. We made a (unsuccessful) foray into variational inference for sampling from the Bayesian posterior but this did not prove sufficient for accurate RLCT estimation. 
The intractability of RLCT estimation for DNNs is not a significant deterrent to reaping the insights offered by singular learning theory. For instance, used in the context of model selection, the exact value of the RLCT is not as important as model selection consistency. We also demonstrated the utility of singular learning results such as \eqref{eq:Bayes_generalization} and \eqref{eq:map_generalization} which can be exploited even without knowledge of the exact value of the RLCT.
%Despite this, we have illustrated that it is still possible to reap the insights offered by singular learning theory. In particular we showed that being Bayesian in the final layers, though a pale substitute for the full Bayes predictive distribution, nonetheless demonstrates humble improvements over point estimators. 

%As it stands, singular learning theory does not yet provide a complete mathematical theory of deep learning, as it is practiced. Much of this is technical in nature. For instance, theory allowing for neural networks with the ReLU activation function is lacking. Also, the consequences of nonrealizability is still not well understood. 

%\subsubsection*{Author Contributions}
%If you'd like to, you may include  a section for author contributions as is done
%in many journals. This is optional and at the discretion of the authors.
%
%\subsubsection*{Acknowledgments}
%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{iclr2021_conference}
\bibliographystyle{iclr2021_conference}

\appendix
\section{Appendix}

\subsection{Neural networks are strictly singular}
\label{appendix:nn_singular}
Many-layered neural networks are strictly singular. The degeneracy of the Hessian in deep learning has certainly been acknowledged in e.g., \citet{DBLP:journals/corr/SagunBL16} which recognizes the eigenspectrum is concentrated around zero and in \citet{pennington_spectrum_2018} which deliberately studies the Fisher information matrix of a \textit{single}-hidden-layer, rather than multilayer, neural network. 

We first explain how to think about a neural network in the context of singular learning theory. A feedforward network of depth $c$ parametrizes a function $f: \mathbb{R}^N \lto \mathbb{R}^M$ of the form
\[
f = A_c \circ \sigma_{c-1} \circ A_{c-1} \cdots \sigma_1 \circ A_1
\]
where the $A_l: \mathbb{R}^{d_{l-1}} \lto \mathbb{R}^{d_{l}}$ are affine functions and $\sigma_l: \mathbb{R}^{d_{l}} \lto \mathbb{R}^{d_{l}}$ is coordinate-wise some fixed nonlinearity $\sigma: \mathbb{R} \lto \mathbb{R}$. Let $W$ be a compact subspace of $\mathbb{R}^d$ containing the origin, where $\mathbb{R}^d$ is the space of sequences affine functions $(A_l)_{l=1}^c$ with coordinates denoted $w_1,\ldots,w_d$ so that $f$ may be viewed as a function $f: \mathbb{R}^N \times W \lto \mathbb{R}^M$. We define
\begin{equation}
p(y|x,w) = \frac{1}{(2 \pi)^{M/2}} \exp\Big(-\tfrac{1}{2} \| y - f(x,w) \|^2 \Big)\,.
\label{eq:gaussian_model_in_w}
\end{equation}
We assume the true distribution is realizable, $q(y|x) = p(y|x,w_0)$ and that a distribution $q(x)$ on $\mathbb{R}^N$ is fixed with respect to which $p(x,y) = p(y|x)q(x)$ and $q(x,y) = q(y|x)q(x)$. Given some prior $\varphi(w)$ on $W$ we may apply singular learning theory to the triplet $(p,q,\varphi)$.

By straightforward calculations we obtain
\begin{align}
K(w) &= \tfrac{1}{2} \int \| f(x,w) - f(x,w_0) \|^2 q(x) dx \label{eq:K_nn}\\
\tfrac{\partial^2}{\partial w_i \partial w_j} K(w) &= \int \Big\langle \tfrac{\partial}{\partial w_i} f(x,w), \tfrac{\partial}{\partial w_j} f(x,w) \Big\rangle q(x) dx \nonumber \\
&\quad + \int \Big\langle f(x,w) - f(x,w_0), \tfrac{\partial^2}{\partial w_i \partial w_j} f(x,w) \Big\rangle q(x) dx \label{eq:Hessian}\\
I(w)_{ij} &= \frac{1}{2^{(M-3)/2} \pi^{(M-2)/2}} \int \Big\langle \tfrac{\partial}{\partial w_i} f(x,w), \tfrac{\partial}{\partial w_j} f(x,w) \Big\rangle q(x) dx\label{eq:fisher_relu}
\end{align}
where $\langle -, - \rangle$ is the dot product. We assume $q(x)$ is such that these integrals exist.

It will be convenient below to introduce another set of coordinates for $W$. Let $w_{jk}^{l}$ denote the weight from the $k$th neuron in the $(l-1)$th layer to the $j$th neuron in the $l$th layer and let $b_j^{l}$ denote the bias of the $j$th neuron in the $l$th layer. Here $1 \le l \le c$ and the input is layer zero. Let $u_{j}^{l}$ and $a_{j}^{l}$ denote the value of the $j$th neuron in the $l$th layer before and after activation, respectively. Let $u^{l}$ and $a^{l}$ denote the vectors with values $u_{j}^{l}$ and $a_{j}^{l}$, respectively. Let $d_{l}$ denote the number of neurons in the $l$th layer. Then
\begin{align*}
	u_{j}^{l} &=\sum_{k=1}^{d_{l-1}}w_{jk}^{l}a_{k}^{l-1}+b_{j}^{l}, &1 \le l \le c, 1 \le j \le d_l\\
	a_{j}^{l} &=\sigma(u_{j}^{l}) & 1 \le l < c, 1 \le j \le d_l
\end{align*}
with the convention that $a^0 = x$ is the input and $u^c = y$ is the output.

In the case where $\sigma = \operatorname{ReLU}$ the partial derivatives $\frac{\partial}{\partial w_j} f$ do not exist on all of $\mathbb{R}^N$. However given $w \in W$ we let $\mathcal{D}(w)$ denote the complement in $\mathbb{R}^N$ of the union over all hidden nodes of the associated decision boundary, that is
\[
\mathbb{R}^N \setminus \mathcal{D}(w) = \bigcup_{1 \le l < c} \bigcup_{1 \le j \le d_l} \{ x \in \mathbb{R}^N \,|\, u^l_j(x) = 0 \}\,.
\]
The partial derivative $\frac{\partial}{\partial w_j} f$ exists on the open subset $\{ (x,w) | x \in \mathcal{D}(w) \}$ of $\mathbb{R}^N \times W$. 

\begin{lemma}\label{lemma:reln}
	Suppose $\sigma = \operatorname{ReLU}$ and there are $c > 1$ layers. For any hidden neuron $1 \le j \le d_l$ in layer $l$ with $1 \le l < c$ there is a differential equation
	\begin{align*}
		\Big\{ \sum_{k=1}^{d_{l-1}}w_{jk}^{l}\frac{\partial}{\partial w_{jk}^{l}} + b_{j}^{l}\frac{\partial}{\partial b_{j}^{l}}-\sum_{i=1}^{d_{l+1}}w_{ij}^{l+1}\frac{\partial}{\partial w_{ij}^{l+1}} \Big\} f =0
	\end{align*}
\end{lemma}
which holds on $\cat{D}(w)$ for any fixed $w \in W$.
\begin{proof}
Without loss of generality assume $M = 1$, to simplify the notation. Let $e_{i}\in \mathbb{R}^{d_{l+1}}$ denote a unit vector and let $H(x)=\frac{d}{dx}\operatorname{ReLU}(x)$. Writing $\frac{\partial f}{\partial u^{l+1}}$ for a gradient vector
\begin{gather*}
	\frac{\partial f}{\partial w_{ij}^{l+1}} = \Big\langle \frac{\partial f}{\partial u^{l+1}}, \frac{\partial u^{l+1}}{\partial w_{ij}^{l+1}} \Big\rangle = \Big\langle \frac{\partial f}{\partial u^{l+1}}, a_{j}^{l}e_{i} \Big\rangle =\frac{\partial f}{\partial u^{l+1}_i}u_{j}^{l}H(u_{j}^{l})\\
	\frac{\partial f}{\partial w_{jk}^{l}} =\Big\langle \frac{\partial f}{\partial u^{l+1}}, \frac{\partial u^{l+1}}{\partial w_{jk}^{l}} \Big\rangle = \Big\langle \frac{\partial f}{\partial u^{l+1}}, \sum_{i=1}^{d_{l+1}} w_{ij}^{l+1}a_{k}^{l-1}H(u_{j}^{l})e_{i} \Big\rangle = \sum_{i=1}^{d_{l+1}} \frac{\partial f}{\partial u^{l+1}_i}w_{ij}^{l+1}a_{k}^{l-1}H(u_{j}^{l}) \\
	\frac{\partial f}{\partial b_{j}^{l}} = \Big\langle \frac{\partial f}{\partial u^{l+1}}, \frac{\partial u^{l+1}}{\partial b_{j}^{l}} \Big\rangle =\Big\langle \frac{\partial f}{\partial u^{l+1}}, \sum_{i=1}^{d_{l+1}}w_{ij}^{l+1}H(u_{j}^{l})e_{i} \Big\rangle = \sum_{i=1}^{d_{l+1}} \frac{\partial f}{\partial u^{l+1}_i}w_{ij}^{l+1}H(u_{j}^{l}).
\end{gather*}
The claim immediately follows.
\end{proof}

\begin{lemma}\label{lemma:all_degen} Suppose $\sigma = \operatorname{ReLU}, c > 1$ and that $w \in W$ has at least one weight or bias at a hidden node nonzero. Then the matrix $I(w)$ is degenerate and if $w \in W_0$ then the Hessian of $K$ at $w$ is also degenerate.
\end{lemma}
\begin{proof}
	Let $w \in W$ be given, and choose a hidden node where at least one of the incident weights (or bias) is nonzero. Then Lemma \ref{lemma:reln} gives a nontrivial linear dependence relation $\sum_i \lambda_i \frac{\partial}{\partial w_i} f = 0$ as functions on $\mathcal{D}(w)$. The rows of $I(w)$ satisfy the same linear dependence relation. At a true parameter the second summand in (\ref{eq:Hessian}) vanishes so by the same argument the Hessian is degenerate.
\end{proof}

Lemma \ref{lemma:all_degen} implies that every true parameter for a nontrivial ReLU network is a degenerate critical point of $K$. Hence in the study of nontrivial ReLU networks it is never appropriate to divide by the determinant of the Hessian of $K$ at a true parameter, and in particular Laplace or saddle-point approximations at a true parameter are invalid.

The well-known positive scale invariance of ReLU networks is responsible for the linear dependence of Lemma \ref{lemma:reln}, but this is only one source of degeneracy or singularity in ReLU networks. The degeneracy, as measured by the RLCT, is much lower than one would expect on the basis of this symmetry alone (see Section \ref{section:simple_func}).

\begin{example}\label{example:not_analytic} In general the Kullback-Leibler function $K(w)$ for ReLU networks is not analytic. For the minimal counterexample, let $q(x)$ be uniform on $[-N, N]$ and zero outside and consider
\[
K(b) = \int q(x) ( \operatorname{ReLU}(x - b) - \operatorname{ReLU}(x) )^2 dx\,.
\]
It is easy to check that up to a scalar factor
\[
K(b) = \begin{cases} -\tfrac{2}{3} b^3 + b^2 N & 0 \le b \le N \\
-\tfrac{1}{3} b^3 + b^2 N & -N \le b \le 0
\end{cases}
\]
so that $K$ is $C^2$ but not $C^3$ let alone analytic.
\end{example}

\subsection{Reduced rank regression} \label{appendix:reducedrank}
For reduced rank regression, the model is
$$
p( y \rvert x, w) = \frac{1}{(2\pi \sigma^2)^{N/2}} \exp\left( -
\frac{1}{2 \sigma^2} | y - BA x|^2\right),
$$
where $x \in \mathbb{R}^M, y \in \mathbb{R}^N$, $A$ an $M \times H$
matrix and $B$ an $H \times N$ matrix; the parameter $w$ denotes the
entries of $A$ and $B$, i.e. $w = (A, B)$, and $\sigma > 0$ is a
parameter which for the moment is irrelevant.

If the true distribution is realizable then there is $w_0 = (A_0,
B_0)$ such that $q(y|x) = p(y \rvert x, w_0)$.  Without loss of generality assume $q(x)$ is the uniform density. In this case the KL
divergence from $p(y \rvert x, w)$ to $q(y|x)$ is
$$
K(w) = \int q(y|x) \log \frac{q(y|x)}{p(y|x, w)} dxdy = \| BA -
B_0A_0 \|^2 \left( 1 + E(w) \right)
$$
where the error $E$ is smooth and $E(w) = O(\| BA -
B_0A_0 \|^2)$ in any region where $\| BA -
B_0A_0 \| < C$, so $K(w)$ is equivalent to $\|BA - B_0 A_0\|^2$.  We
write $K(w) = \|BA - B_0 A_0\|^2$ for simplicity below.

Now assume that $B_0A_0$ is symmetric and that $B_0$ is square,
i.e.\ $N = H$.  Then the zero locus of $K(w)$ is explicitly given as
follows
$$
K(w) = \{ (A, B) : \det B \neq 0 \mbox{ and } A = B^{-1}B_0A_0 \}.
$$
It follows that $W_0$ is globally a graph over $GL(H;
\mathbb{R})$.  Indeed, the set $(B^{-1}B_0 A_0, B)$ with $B \in GL(H;
\mathbb{R})$ is exactly $W_0$.  Thus $\{ K = 0\}$ is a smooth
$H^2$-dimensional submanifold of $\mathbb{R}^{H^2} \times
\mathbb{R}^{H \times M}$.  

To prove that $W_0$ is minimally singular it suffices to show that
%\begin{equation}
%\label{eq:1}
$$
\mathrm{rank} ( D^2_{A,B}K) \ge HM,
$$
%\end{equation}
but as it is no more difficult to so, we find explicit local
coordinates $(u, v)$ near an arbitrary point $(\overline{A},
\overline{B}) \in W_0$ for which $\{ v = 0 \} = W_0$
and $K(u, v) = a(u,v)|u|^2$ in this neighborhood, where $a$ is a
$C^\infty$ function with $a \ge c > 0$ for some $c$.  Write
$$
A(v) = (\overline{B} + v)^{-1}B_0 A_0.
$$
Then $u, v \mapsto (A(v) + u, \overline{B} + v)$ gives local
coordinates on $\mathbb{R}^{H^2} \times
\mathbb{R}^{H \times M}$ near $(\overline{A}, \overline{B})$, and
\begin{equation*}
\begin{split}
K(u, v) &= | (\overline{B} + v)( (\overline{B} + v)^{-1}B_0 A_0 + u) -
B_0 A_0 | \\
&= | B_0 A_0 +  (\overline{B} + v) u -
B_0 A_0 |^2 \\
&= | (\overline{B} + v) u |^2,
\end{split}
\end{equation*}
so for $v$ sufficiently small (and hence $\overline{B} + v$
invertible) we can take $a(u,v) = | (\overline{B} + v) u |^2 /
|u|^2$.  


\subsection{RLCT estimation} \label{appendix:RLCT_estimation}

%From derivations in \citep{watanabe_widely_2013}, we can glean several useful asymptotic characterizations of the RLCT. To set this up, we begin with some definitions. 

Let $L_n(w)$ be the negative log likelihood as in \eqref{eq:nll}. Define the data likelihood at inverse temperature $\beta >0$ to be $p^\beta(\mathcal D_n | w) = \Pi_{i=1}^n p(y_i |x_i, w)^\beta$ which can also be written 
\begin{equation}
p^\beta(\mathcal D_n | w) = \exp(-\beta n L_n(w)).
\label{general_likelihood}
\end{equation}
The posterior distribution, at inverse temperature $\beta$, is defined as 
\begin{equation}
p^\beta(w|\mathcal D_n) = \frac{\Pi_{i=1}^n p(y_i|x_i,w)^\beta \varphi(w)}{\int_W \Pi_{i=1}^n p(y_i|x_i,w)^\beta \varphi(w)} = \frac{p^\beta(\mathcal D_n|w) \varphi(w)}{p^\beta(\mathcal D_n)}
\label{general_posterior}
\end{equation}
where $\varphi$ is the prior distribution on the network weights $w$ and
\begin{equation}
p^\beta(\mathcal D_n) = \int_W p^\beta(\mathcal D_n|w) \varphi(w) \,dw
\label{general_marginal_likelihood}
\end{equation}
is the marginal likelihood of the data at inverse temperature $\beta$. 
Finally, denote the expectation of a random variable $R(w)$ with respect to the tempered posterior $p^\beta(w|\mathcal D_n)$ as
\begin{equation}
{\E}_w^\beta [R(w)] = \int_W R(w) p^\beta(w|\mathcal D_n) \,dw
\label{general_expectation_posterior}
\end{equation}
% There are two senses to the estimation that is required of the real log canonical threshold. In the first sense, assuming the true distribution $q$ is known, we may be able to only approximate $\lambda(q)$. In the second sense, we must grapple with the fact that $q$ is not known and the plug-in procedure $\lambda(\hat q)$ is not sound. Works addressing the former vein largely come from researchers well-versed in algebraic geometry \cite{lin_ideal-theoretic_2017,imai_estimating_2019} while statisticians tend to treat the second estimation aspect \cite{drton_bayesian_2017}.

% There are two senses to the estimation that is required of the real log canonical threshold. In the first sense, assuming the true distribution $q$ is known, we may be able to only approximate $\lambda(q)$. In the second sense, we smut grapple with the fact that $q$ is not known and the plug-in procedure $\lambda(\hat q)$ is not sound. Works addressing the former vein largely come from researchers well-versed in algebraic geometry \cite{lin_ideal-theoretic_2017,imai_estimating_2019} while statisticians tend to treat the second estimation aspect \cite{drton_bayesian_2017}.
In the main text, we drop the superscript in the quantities \ref{general_likelihood}, \ref{general_posterior}, \ref{general_marginal_likelihood}, \ref{general_expectation_posterior} when $\beta = 1$, e.g., $p(\mathcal D_n)$ rather than $p^1(\mathcal D_n)$.


Assuming the conditions of Theorem 4 in \cite{watanabe_widely_2013} hold, we have
\begin{equation}
{\E}_w^\beta [nL_n(w)] = nL_n(w_0) + \frac{\lambda }{\beta} + U_n \sqrt{\frac{\lambda}{2 \beta}} + O_p(1)
\label{eq:Theorem4_WBIC}
\end{equation}
where $\beta_0$ is a positive constant and $U_n$ is a sequence of random variables satisfying ${\E}_n U_n = 0$. %Assuming $q(y|x)$ is realizable (which we do), $U_n$ behaves nicely ???insert some of those nice properties???.
In Algorithm \ref{alg:thm4}, we describe an estimation procedure for the RLCT based on the asymptotic result in \eqref{eq:Theorem4_WBIC}.
%To approximate the integral $E_w^\beta [n L_n(w)]$ on the left-hand-side of \eqref{eq:Theorem4_WBIC} we use NUTS. 
%However, the $p^\beta(w|\mathcal D_n)$ is intractable for any $\beta>0$, rendering computation of $E_w^\beta$ challenging. For regular models, the Laplace approximation to ${\E}_w^\beta [R(w)]$ would be reasonable (as guaranteed for instance by the Berstein-von Mises theorem.) Recall the Laplace approximation in this case would replace ${\E}_w^\beta$ with an expectation with respect to a normal random variable with mean $w_0$, which is a mode of $L_n$, and covariance which is the Hessian $H_{ij} =\frac{\partial^2 L_n}{\partial w_i \partial w_j}$. But as we discussed earlier, for strictly singular models, the Laplace approximation does not hold.

The \emph{a posteriori} distribution was approximated using the NUTS variant of Hamiltonian Monte Carlo \citep{hoffman2014no} where the first 1000 steps were omitted and $20,000$ samples were collected.  Each $\hat \lambda(\mathcal D_n)$ estimate in Algorithm \ref{alg:thm4} was performed by linear regression on the pairs $\{ (1/\beta_i, \mathbb{E}^{\beta_i}_w[ nL_n(w) ] ) \}_{i=1}^5$ where the five inverse temperatures $\beta_i$ are centered on high temperature $1/\log(20000)$.

\begin{algorithm}[tb]
	\caption{RLCT via Theorem 4  in \cite{watanabe_widely_2013}}
	\label{alg:thm4}
	\begin{algorithmic}
		\STATE {\bfseries Input:} range of $\beta$'s, set of training sets $\mathcal T$ each of size $n$, approximate samples $\{w_1,\ldots,w_R\}$ from $p^\beta(w|\mathcal D_n)$ for each training set $\mathcal D_n$ and each $\beta$
		\FOR{training set $\mathcal D_n \in \mathcal T$}
		\FOR{$\beta$ in range of $\beta$'s}
		\STATE Approximate ${\E}_w^\beta [nL_n(w)]$ with $\frac{1}{R} \sum_{i=1}^R nL_n(w_r)$ where $w_1,\ldots,w_R$ are approximate samples from $p^\beta(w|\mathcal D_n)$
		\ENDFOR
		\STATE Perform generalized least squares to fit $\lambda$ in \eqref{eq:Theorem4_WBIC}, call result $\hat \lambda(\mathcal D_n)$
		\ENDFOR
		\STATE {\bfseries Output:} $\frac{1}{|\mathcal T|} \sum_{\mathcal D_n \in \mathcal T} \hat \lambda(\mathcal D_n)$
	\end{algorithmic}
\end{algorithm}


\subsection{Connection between RLCT and generalization} \label{appendix:generalization_theory}
For completeness, we sketch the derivation of \eqref{eq:Bayes_generalization} which gives the asymptotic expansion of the average generalization error ${\E}_n G(n)$ of the Bayes prediction distribution  in singular models. The exposition is an amalgamation of various works published by Sumio Watanabe, but is mostly based on the textbook \cite{watanabe_algebraic_2009}. 

To understand the connection between the RLCT and $G(n)$, we first define the so-called \textbf{Bayes free energy} as 
\[
F(n) = -\log p(\mathcal D_n)
\]
whose expectation admits the following asymptotic expansion \cite{watanabe_algebraic_2009}:
\[
{\E}_n F(n) =  {\E}_n n S_n + \lambda \log n + o(\log n)
\]
where $S_n = -\frac{1}{n} \sum_{i=1}^n \log q(y_i|x_i)$ is the entropy. 
%This deceptively simple result is actually based on a set of sophisticated tools, in particular Hironaka's resolution theorem from algebraic geometry.
The expected Bayesian generalization error is related to the Bayes free energy as follows
\[
{\E}_n G(n) = \E F(n+1) - \E F(n)
\]
Then for the average generalization error, we have
\begin{equation}
{\E}_n G(n) = \lambda/n + o(1/n).
\label{eq:bayesgenerr}
\end{equation}
Since models with more complex singularities have smaller RLCTs, this would suggest that the more singular a model is, the better its generalization (assuming one uses the Bayesian predictive distribution for prediction). In this connection it is interesting to note that simpler (relative to the model) true distributions lead to more singular models (Section \ref{section:simple_func}).

That the RLCT has such a simple relationship to the Bayesian generalization error is remarkable. On the other hand, the practical implications of (\ref{eq:bayesgenerr}) are limited. This is because the Bayes predictive distribution in the case of a DNN is itself intractable. While we believe that approximations to the Bayesian predictive distribution, say via variational inference, might inherit a similar relationship between generalization and the (variational) RLCT, serious theoretical developments will be required to rigorously establish this. The challenge comes from the fact that for approximate Bayesian predictive distributions, the free energy and generalization error have different learning coefficients $\lambda$. This was well documented in the case of a neural network with one hidden layer \citep{nakajima_variational_2007}. 

\subsection{Details for generalization error experiments}
\label{appendix:generalizaton}

\textbf{Simulated data}
The distribution of $x \in \mathbb R^3$ is set to $q(x)=N(0,I_3)$. 
In the realizable case, $y \in \mathbb R^3$ is drawn according to $q(y|x) = p(y|x,\theta_0)$ as in \eqref{eq:genexp_model}. In the nonrealizable setting, we set $q(y|x) \propto \exp\{-|| y - h_{w_0}(x) ||^2/2\},$ where $w_0 = (A_0,B_0)$ is drawn according to the PyTorch model initialization of $h$.
%???mention $x_test_std$.???

%\textbf{Network architecture} In our experiments, $f = h \circ g$ where $g$ is a sequence of 
%\[
%\text{linear} \circ \text{ReLU} \circ \ldots \text{linear}
%\]
%and $h$ is either
%\begin{equation}
%    \text{linear} \circ \text{ReLU} \circ \text{linear} 
%    \label{eq:rr_with_relu}
%\end{equation}
%or
%\begin{equation}
%    \text{linear}  \circ \text{linear}. 
%    label{eq:rr_no_relu}
%\end{equation}
%We fix the number of hidden units in $g$, the feedforward ReLU block, to 5 and the number of hidden units in $h$, the last two years, to 3. We varied the number of layers in $g$ between $1$ and $5$.

\textbf{MAP training}
The MAP estimator is found via gradient descent using the mean-squared-error loss with either the full data set or minibatch set to 32. Training was set to 5000 epochs. No form of early stopping was employed.

\textbf{Calculating the generalization error}
Using a held-out-test set $T_{n'} = \{(x_i',y_i')\}_{i=1}^{n'}$, we calculate the average generalization error as
\begin{equation}
\frac{1}{n'} \sum_{i=1}^{n'} \log q(y_i'|x_i') - {\E}_n \frac{1}{n'} \sum_{i=1}^{n'} \log \hat q_n(y_i'|x_i')
\label{eq:computed_avgGn}
\end{equation}
Assume the held-out test set is large enough so that the difference between ${\E}_n G(n)$ and \eqref{eq:computed_avgGn} is negligible. We will refer to them interchangeably as the average generalization error. In our experiments we use $n' = 10,000$ and $30$ draws of the dataset $\mathcal{D}_n$ to estimate ${\E}_n$.

\textbf{Last layer(s) inference}
Without loss of generality, we discuss performing inference in the $w$ parameters of $h$ while freezing the parameters of $g$ at the MAP estimate. The steps easily extend to performing inference over the final layer only of $f = h \circ g$. Let $\tilde x_i = g_{v_{map}}(x_i)$. Define a new transformed dataset $\tilde{\mathcal D}_n = \{(\tilde x_i, y_i) \}_{i=1}^n$. We take the prior on $w$ to be standard Gaussian. 
Define the posterior over $w$ given $\tilde{\mathcal D}_n$ as:
\begin{equation}
p(w | \tilde{\mathcal D}_n) \propto p(\tilde{\mathcal D}_n | w) \varphi(w) = \Pi_{i=1}^n \exp\{-|| y_i - h_w(\tilde x_i) ||^2/2\} \varphi(w)
\label{eq:last_layer_posterior}
\end{equation}
Define the following approximation to the Bayesian predictive distribution
$$
\tilde p(y|x, \mathcal D_n) = \int p(y|x,(v_{map},w)) p(w|\tilde{\mathcal D}_n) \,dw.
$$
Let $w_1,\ldots,w_R$ be some approximate samples from $p(w | \tilde{\mathcal D}_n)$. Then we approximate $\tilde p(y|x, \mathcal D_n)$ with
\[
\frac{1}{R} \sum_{r=1}^R p(y|x,(v_{map},w_r))
\]
where $R$ is a large number, set to 1000 in our experiments. We consider the Laplace approximation and the NUTS variant of HMC for drawing samples from $p(w | \tilde{\mathcal D}_n)$:

\begin{itemize}
	\item \textbf{Laplace in the last layer(s)}
	Recall $\theta_{map} = (v_{map}, w_{\operatorname{MAP}})$ is the MAP estimate for $f_\theta$ trained with the data $\mathcal D_n$. With the Laplace approximation, we draw $w_1,\ldots w_R$ from the Gaussian
	\[
	N(w_{\operatorname{MAP}}, \Sigma)
	\]
	where $\Sigma = (- \nabla^2 \log p(w| \tilde{\mathcal D}_n) |_{w_{\operatorname{MAP}}})^{-1}$ is the inverse Hessian\footnote{Following \citet{kristiadi_being_2020}, the code for the exact Hessian calculation is borrowed from \url{https://github.com/f-dangel/hbp}} of the negative log posterior evaluated at the MAP estimate of the mode.
	\item \textbf{MCMC in the last layer(s)}
	We used the NUTS variant of HMC to draw samples from \eqref{eq:last_layer_posterior} with the first 1000 samples  discarded.. Our implementation used the \texttt{pyro} package in \texttt{PyTorch}.
	
\end{itemize}

\newpage
\begin{figure}[t!]
	\begin{center}
		\includegraphics[scale=0.35]{taskid12.png}
		\includegraphics[scale=0.35]{taskid13.png}
		\includegraphics[scale=0.35]{taskid14.png}
		\includegraphics[scale=0.35]{taskid15.png}
	\end{center}
	\caption{\textit{Realizable and minibatch gradient descent for MAP training.}}
	\label{fig:avg_gen_err_minibatch_realizable}
\end{figure}


\begin{table}[h!]%
	\centering

	\caption{Companion to Figure \ref{fig:avg_gen_err_minibatch_realizable}.}%
	\label{table::avg_gen_err_minibatch_realizable}%
	\begin{tiny}
	\begin{subtable}[t]{2.5in}

		\caption{1 hidden layer(s) in $g$, identity activation in $h$}
		\input{graphics/taskid12.tex}
	\end{subtable}
	\quad
	\begin{subtable}[t]{2.5in}
		\caption{5 hidden layer(s) in $g$, identity activation in $h$}
		\input{graphics/taskid13.tex}
	\end{subtable}
	\quad 
	\begin{subtable}[t]{2.5in}
		\caption{1 hidden layer(s) in $g$, ReLU activation in $h$}
		\input{graphics/taskid14.tex}
	\end{subtable}
	\quad
	\begin{subtable}[t]{2.5in}
		\caption{5 hidden layer(s) in $g$, ReLU activation in $h$}
		\input{graphics/taskid15.tex}
	\end{subtable}
	\end{tiny}
\end{table}

\newpage

\begin{figure}[t!]
	\begin{center}
		\includegraphics[scale=0.35]{taskid0.png}
		\includegraphics[scale=0.35]{taskid1.png}
		\includegraphics[scale=0.35]{taskid2.png}
		\includegraphics[scale=0.35]{taskid3.png}
	\end{center}
	\caption{\textit{Nonrealizable and full batch gradient descent for MAP training.}}
	\label{fig:avg_gen_err_fullbatch_nonrealizable}
\end{figure}


\begin{table}[h!]%
	\centering
	\caption{Companion to Figure \ref{fig:avg_gen_err_fullbatch_nonrealizable}. The learning coefficient is the slope of the linear fit $1/n$ versus $\E_n G(n)$ (with intercept since nonrealizable).}%
	\label{table::avg_gen_err_fullbatch_nonrealizable}%
	\begin{tiny}
	\begin{subtable}[t]{2.5in}
		\caption{1 hidden layer(s) in $g$, identity activation in $h$}
		\input{graphics/taskid0.tex}
	\end{subtable}
	\quad
	\begin{subtable}[t]{2.5in}
		\caption{5 hidden layer(s) in $g$, identity activation in $h$}		\input{graphics/taskid1.tex}
	\end{subtable}
	\quad 
	\begin{subtable}[t]{2.5in}
		\caption{1 hidden layer(s) in $g$, ReLU activation in $h$}
		\input{graphics/taskid2.tex}
	\end{subtable}
	\quad 
	\begin{subtable}[t]{2.5in}
		\caption{5 hidden layer(s) in $g$, ReLU activation in $h$}		\input{graphics/taskid3.tex}
	\end{subtable}
	\end{tiny}
\end{table}

\newpage
\begin{figure}[t!]
	\begin{center}
		\includegraphics[scale=0.35]{taskid4.png}
		\includegraphics[scale=0.35]{taskid5.png}
		\includegraphics[scale=0.35]{taskid6.png}
		\includegraphics[scale=0.35]{taskid7.png}
	\end{center}
	\caption{\textit{Nonrealizable and minibatch gradient descent for MAP training.} Missing points on the MAP learning curve are due to estimated probabilities too close to 0.}
	\label{fig:avg_gen_err_minibatch_nonrealizable}
\end{figure}


\begin{table}[h!]%
	\centering
	\caption{Companion to Figure \ref{fig:avg_gen_err_minibatch_nonrealizable}. The learning coefficient is the slope of the linear fit $1/n$ versus $\E_n G(n)$ (with intercept since nonrealizable).}%
	\label{table::avg_gen_err_minibatch_nonrealizable}%
	\begin{tiny}
	\begin{subtable}[t]{2.5in}
		\caption{1 hidden layer(s) in $g$, identity activation in $h$}		\input{graphics/taskid0.tex}
	\end{subtable}
	\quad
	\begin{subtable}[t]{2.5in}
		\caption{5 hidden layer(s) in $g$, identity activation in $h$}		\input{graphics/taskid1.tex}
	\end{subtable}
	\quad
	\begin{subtable}[t]{2.5in}
		\caption{1 hidden layer(s) in $g$, ReLU activation in $h$}		\input{graphics/taskid2.tex}
	\end{subtable}
	\quad
	\begin{subtable}[t]{2.5in}
		\caption{5 hidden layer(s) in $g$, ReLU activation in $h$}		\input{graphics/taskid3.tex}
	\end{subtable}
	\end{tiny}
\end{table}

%We have to be careful when we speak of the generalization error of the approximate predictive distribution above. For a proper comparison to ${\E}_n G_{map}(n)$ or ${\E}_n G_{mle}(n)$, we have to look at 
%\begin{equation}
%{\E}_n G_{mcmcrr}(n) =  KL (q(y|x) || p_{mcmcrr}(y|x, \mathcal D_n) )
%\label{G_LLB}
%\end{equation}
%where ${\E}_n$ averages out the randomness of both $D_n$ and  $v_{map}$. 
%
%An alternative is to condition on $v_{map}$, using $g_{v_{map}}$ as a feature extractor in a preprocessing step. Then, assuming realizability $q(y|x) = p(y|x,(v_0,w_0))$ we may examine the generalizaton error
%\begin{equation}
%E_{\mathcal D_n | v_{map}} KL( p(y|x,(v_{map},w_0)) || p_{mcmcrr}(y|x, \mathcal D_n) )
%\label{G_LLB_vfixed}
%\end{equation}
%The average generalization error in \eqref{G_LLB_vfixed} is distinctly different from the one in \eqref{G_LLB}. The nice thing about \eqref{G_LLB_vfixed} is that we know its asymptotic expansion is $\lambda/n$ where $\lambda$ corresponds to the triplet $( p(y|x,(v_{map},w_0)), p(y| x, (v_{map},w)), \varphi(w))$. For certain functions $h_w$ where the true $\lambda$ is known, we can verify this in the experiments. (Not implemented yet).

\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.35]{laplace_taskid8.png}
		\includegraphics[scale=0.35]{laplace_taskid9.png}
		\includegraphics[scale=0.35]{laplace_taskid10.png}
		\includegraphics[scale=0.35]{laplace_taskid11.png}
	\end{center}
	\caption{\textit{Realizable and full batch gradient descent for MAP}: average generalization errors of Laplace approximations of the predictive distribution. The last-two-layers Laplace approximation results in numerical instabilities due to degenerate Hessian. Any missing points are due to estimated probabilities too close to 0. 
	}
	%		Figure \ref{fig:avg_gen_err_minibatch_realizable} is analogous to this figure except minibatch gradient descent was employed to find MAP.}
	\label{fig:avg_gen_err_fullbatch_realizable_laplace}
\end{figure}

\begin{figure}[t!]
	\begin{center}
		\includegraphics[scale=0.35]{laplace_taskid12.png}
		\includegraphics[scale=0.35]{laplace_taskid13.png}
		\includegraphics[scale=0.35]{laplace_taskid14.png}
		\includegraphics[scale=0.35]{laplace_taskid15.png}
	\end{center}
	\caption{\textit{Realizable and minibatch gradient descent for MAP training}. Details are same as for Figure \ref{fig:avg_gen_err_fullbatch_realizable_laplace}}
	\label{fig:avg_gen_err_minibatch_realizable_laplace}
\end{figure}


\begin{figure}[t!]
	\begin{center}
		\includegraphics[scale=0.35]{laplace_taskid0.png}
		\includegraphics[scale=0.35]{laplace_taskid1.png}
		\includegraphics[scale=0.35]{laplace_taskid2.png}
		\includegraphics[scale=0.35]{laplace_taskid3.png}
	\end{center}
	\caption{\textit{Nonrealizable and full batch gradient descent for MAP training.} Details are same as for Figure \ref{fig:avg_gen_err_fullbatch_realizable_laplace}
	}
	\label{fig:avg_gen_err_fullbatch_nonrealizable_laplace}
\end{figure}

\begin{figure}[t!]
	\begin{center}
		\includegraphics[scale=0.35]{laplace_taskid4.png}
		\includegraphics[scale=0.35]{laplace_taskid5.png}
		\includegraphics[scale=0.35]{laplace_taskid6.png}
		\includegraphics[scale=0.35]{laplace_taskid7.png}
	\end{center}
	\caption{\textit{Nonrealizable and minibatch gradient descent for MAP training.} Details are same as for Figure \ref{fig:avg_gen_err_fullbatch_realizable_laplace}
	}
	\label{fig:avg_gen_err_minibatch_nonrealizable_laplace}
\end{figure}

\end{document}
